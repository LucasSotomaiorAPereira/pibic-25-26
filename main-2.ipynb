{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931bd0c8c313863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.285447Z",
     "start_time": "2025-09-16T13:51:50.364805Z"
    }
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    'NF-UNSW-NB15-v3',\n",
    "    'NF-BoT-IoT-v3',\n",
    "    'NF-CICIDS2018-v3',\n",
    "    'NF-ToN-IoT-v3'\n",
    "]\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "print(\"Iniciando o carregamento dos arquivos...\")\n",
    "for filename in filenames:\n",
    "    path = f'db/{filename}.csv'\n",
    "    print(f\"  -> Carregando: {path}\")\n",
    "    try:\n",
    "        temp_df = pd.read_csv(path)\n",
    "        list_of_dfs.append(temp_df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  -> AVISO: O arquivo {path} não foi encontrado e será ignorado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b9109-26ea-4e79-a859-107a5d9a2a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Concatenando os dataframes...\")\n",
    "df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "print(f\"Processo finalizado! O dataframe final contém {len(df)} linhas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b3db0-8049-4950-838b-9578a5d1c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_of_dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f89a4f5-5392-4db4-8507-6afb07e6e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'NF-BoT-IoT-v3'\n",
    "\n",
    "df = pd.read_csv(f'db/{filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b34d7b-0039-4e94-8645-04fb78d7116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['IPV4_SRC_ADDR',\n",
    "         'IPV4_DST_ADDR',\n",
    "         'L4_SRC_PORT',\n",
    "         'L4_DST_PORT',\n",
    "         'L7_PROTO',\n",
    "         'TCP_FLAGS',\n",
    "         'CLIENT_TCP_FLAGS',\n",
    "         'SERVER_TCP_FLAGS',\n",
    "         'MIN_TTL', \n",
    "         'MAX_TTL',\n",
    "         'SHORTEST_FLOW_PKT',\n",
    "         'MIN_IP_PKT_LEN', \n",
    "         'TCP_WIN_MAX_IN', \n",
    "         'TCP_WIN_MAX_OUT', \n",
    "         'DNS_QUERY_ID', \n",
    "         'DNS_TTL_ANSWER',\n",
    "         'FTP_COMMAND_RET_CODE',\n",
    "         'SRC_TO_DST_SECOND_BYTES',\n",
    "         'DST_TO_SRC_SECOND_BYTES',\n",
    "         'FLOW_START_MILLISECONDS',\n",
    "         'FLOW_END_MILLISECONDS',], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b562f79-627a-4ac9-b36e-648d28fc7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_sets_by_attack_type = {}\n",
    "attack_types = df['Attack'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:52:06.300809Z",
     "start_time": "2025-09-16T13:52:02.159503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 25994 | Teste: 12997 | Validação: 12998\n",
      "Processando a categoria: 'DDoS'\n",
      "  -> Treino: 3575441 | Teste: 1787720 | Validação: 1787721\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 4017095 | Teste: 2008547 | Validação: 2008548\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 847566 | Teste: 423783 | Validação: 423783\n",
      "Processando a categoria: 'Theft'\n",
      "  -> Treino: 807 | Teste: 404 | Validação: 404\n"
     ]
    }
   ],
   "source": [
    "for attack_type in attack_types:\n",
    "    print(f\"Processando a categoria: '{attack_type}'\")\n",
    "    df_current_attack = df[df['Attack'] == attack_type]\n",
    "\n",
    "    df_train_current_attack, df_aux_current_attack = train_test_split(df_current_attack, train_size=0.5, random_state=42)\n",
    "    df_test_current_attack, df_val_current_attack = train_test_split(df_aux_current_attack, train_size=0.5, random_state=42)\n",
    "\n",
    "    dictionary_sets_by_attack_type[attack_type] = {\n",
    "        'treino': df_train_current_attack,\n",
    "        'teste': df_test_current_attack,\n",
    "        'validacao': df_val_current_attack\n",
    "    }\n",
    "    print(f\"  -> Treino: {len(df_train_current_attack)} | Teste: {len(df_test_current_attack)} | Validação: {len(df_val_current_attack)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d9a5b9-f93f-4518-a6b0-6ac66f7a3c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorias para a base de TREINO FINAL: ['DoS', 'Theft', 'Benign']\n",
      "Categorias para a base de TESTE FINAL: ['DDoS', 'Benign']\n",
      "Categorias para a base de VALIDAÇÃO FINAL: ['Reconnaissance', 'Benign']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "attack_types = np.delete(attack_types, np.where(attack_types == 'Benign'))\n",
    "np.random.shuffle(attack_types)\n",
    "\n",
    "attack_types_for_train, attack_types_for_test, attack_types_for_val = np.split(\n",
    "    attack_types,\n",
    "    [\n",
    "        int(len(attack_types) * 0.5),\n",
    "        int(len(attack_types) * 0.75)\n",
    "    ]\n",
    ")\n",
    "\n",
    "attack_types_for_train = np.append(attack_types_for_train, 'Benign')\n",
    "attack_types_for_test = np.append(attack_types_for_test, 'Benign')\n",
    "attack_types_for_val = np.append(attack_types_for_val, 'Benign')\n",
    "\n",
    "print(f\"Categorias para a base de TREINO FINAL: {list(attack_types_for_train)}\")\n",
    "print(f\"Categorias para a base de TESTE FINAL: {list(attack_types_for_test)}\")\n",
    "print(f\"Categorias para a base de VALIDAÇÃO FINAL: {list(attack_types_for_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f13bae3-100b-4c3d-9050-0fc2a707c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = [dictionary_sets_by_attack_type[attack_type]['treino'] for attack_type in attack_types_for_train]\n",
    "df_train = pd.concat(list_train)\n",
    "\n",
    "list_test = [dictionary_sets_by_attack_type[attack_type]['teste'] for attack_type in attack_types_for_test]\n",
    "df_test = pd.concat(list_test)\n",
    "\n",
    "list_val = [dictionary_sets_by_attack_type[attack_type]['validacao'] for attack_type in attack_types_for_val]\n",
    "df_val = pd.concat(list_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2178ca15-bc21-4dc0-9d04-213137a39ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Base de Treino ---\n",
      "Tamanho: 4043896 linhas\n",
      "Categorias presentes: ['DoS' 'Theft' 'Benign']\n",
      "Attack\n",
      "DoS       4017095\n",
      "Benign      25994\n",
      "Theft         807\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 1800717 linhas\n",
      "Categorias presentes: ['DDoS' 'Benign']\n",
      "Attack\n",
      "DDoS      1787720\n",
      "Benign      12997\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 436781 linhas\n",
      "Categorias presentes: ['Reconnaissance' 'Benign']\n",
      "Attack\n",
      "Reconnaissance    423783\n",
      "Benign             12998\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Base de Treino ---\")\n",
    "print(f\"Tamanho: {len(df_train)} linhas\")\n",
    "print(f\"Categorias presentes: {df_train['Attack'].unique()}\")\n",
    "print(df_train['Attack'].value_counts())\n",
    "print(\"-\" * 25)\n",
    "\n",
    "print(f\"\\n--- Base de Teste ---\")\n",
    "print(f\"Tamanho: {len(df_test)} linhas\")\n",
    "print(f\"Categorias presentes: {df_test['Attack'].unique()}\")\n",
    "print(df_test['Attack'].value_counts())\n",
    "print(\"-\" * 25)\n",
    "\n",
    "print(f\"\\n--- Base de Validação ---\")\n",
    "print(f\"Tamanho: {len(df_val)} linhas\")\n",
    "print(f\"Categorias presentes: {df_val['Attack'].unique()}\")\n",
    "print(df_val['Attack'].value_counts())\n",
    "print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4008a0f8-6ecd-427e-bf3c-12cd6bbf299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c36f73ac-bb76-40e1-bcbe-a623632f80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "973fb250-e077-479d-a875-d5d55c82ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_benign = df_train[df_train['Attack'] == 'Benign']\n",
    "df_train_attacks = df_train[df_train['Attack'] != 'Benign']\n",
    "\n",
    "max_attack_samples = df_train_attacks['Attack'].value_counts().max()\n",
    "\n",
    "if max_attack_samples > 1_000_000:\n",
    "    max_attack_samples = 1_000_000\n",
    "\n",
    "df_train_attacks_balanced = df_train_attacks.groupby('Attack').sample(n=max_attack_samples, replace=True, random_state=42)\n",
    "\n",
    "total_attack_samples = len(df_train_attacks_balanced)\n",
    "\n",
    "df_train_benign_sampled = df_train_benign.sample(n=total_attack_samples, replace=True, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_train_attacks_balanced, df_train_benign_sampled])\n",
    "df_train = shuffle(df_train, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_train = df_train.drop(['Label', 'Attack'], axis=1)\n",
    "y_train = df_train['Label'].to_numpy()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23cc17f8-9031-45e0-8d4e-495b4c40928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    2000000\n",
       "0    2000000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5523a4b-d907-4843-b7ac-75a2627147ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack\n",
       "Benign    2000000\n",
       "Theft     1000000\n",
       "DoS       1000000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a47480fa-d7a0-4f14-a40d-3bc70397a834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b97cb47-2526-4f70-b70e-195f46ac9ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4000000, 32])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7a86f64-4ab7-41e9-805f-9ea6a0ef46be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1], device='cuda:0'), tensor([2000000, 2000000], device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19ec99cd-a20f-46c3-b7dd-d290781444a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(1., device='cuda:0'),\n",
       " tensor(0.0229, device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min(), X_train.max(), X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93b59b35-519a-4f4f-b054-a2b8b08a1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['Label', 'Attack'], axis=1)\n",
    "y_test = df_test['Label'].to_numpy()\n",
    "\n",
    "rus = RandomUnderSampler(random_state = 42)\n",
    "X_test, y_test = rus.fit_resample(X_test, y_test)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "badbb600-f8f0-44f9-9812-37a726ec4ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "1    1787720\n",
       "0      12997\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04ecf3b0-d6cb-438e-b6e0-691852b62bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack\n",
       "DDoS      1787720\n",
       "Benign      12997\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4663f879-ab0a-4147-8d38-fbf6fd7fcbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25994"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe99195b-8721-4514-8866-7a04faca390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25994, 32])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4986367f-f62a-4650-b617-0bffc41c6240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1], device='cuda:0'), tensor([12997, 12997], device='cuda:0'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9498351f-e3e6-44bb-b37f-84f9eaf5a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(1.0091, device='cuda:0'),\n",
       " tensor(0.0245, device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.min(), X_test.max(), X_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "acb6961f-4261-477e-b6f1-42e5a1c2bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = df_val.drop(['Label', 'Attack'], axis=1)\n",
    "y_val = df_val['Label'].to_numpy()\n",
    "\n",
    "rus = RandomUnderSampler(random_state = 42)\n",
    "X_val, y_val = rus.fit_resample(X_val, y_val)\n",
    "\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c15bb96-cf2c-4af9-a780-1f66706a882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25996"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ef824f2-e93c-4691-a053-21730c9472d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1], device='cuda:0'), tensor([12998, 12998], device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9e1c680-2edb-409e-92bf-c0faa7039ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., device='cuda:0'),\n",
       " tensor(1.8930, device='cuda:0'),\n",
       " tensor(0.0114, device='cuda:0'))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.min(), X_val.max(), X_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abe3854a-90cf-4828-babe-d5e505420d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDSBranchyNet(\n",
      "  (main_part1): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=82, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (main_part2): Sequential(\n",
      "    (0): Linear(in_features=82, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      "  (branch): Sequential(\n",
      "    (0): Linear(in_features=82, out_features=41, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=41, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES = X_train.shape[1]\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        self.main_part1 = nn.Sequential(\n",
    "            nn.Linear(NUM_FEATURES, 82),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.main_part2 = nn.Sequential(\n",
    "            nn.Linear(82, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, NUM_CLASSES)\n",
    "        )\n",
    "        self.branch = nn.Sequential(\n",
    "            nn.Linear(82, 41),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(41, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, part=\"both\", intermediate_output=None):\n",
    "        if part == \"branch\":\n",
    "            intermediate_output = self.main_part1(x)\n",
    "            branch_output = self.branch(intermediate_output)\n",
    "            return branch_output, intermediate_output\n",
    "\n",
    "        if part == \"main\":\n",
    "            main_output = self.main_part2(intermediate_output)\n",
    "            return main_output\n",
    "        \n",
    "        if part == \"both\":\n",
    "            intermediate_output = self.main_part1(x)\n",
    "            branch_output = self.branch(intermediate_output)\n",
    "            main_output = self.main_part2(intermediate_output)\n",
    "            return [branch_output, main_output]\n",
    "\n",
    "model = IDSBranchyNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6b23ebb7-f861-47d4-a731-ed1a3201e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, weight_branch, weight_main, lr, device, patience=15):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs, \"both\")\n",
    "            loss_branch = criterion(outputs[0], labels)\n",
    "            loss_main = criterion(outputs[1], labels)\n",
    "\n",
    "            total_loss = (weight_branch * loss_branch) + (weight_main * loss_main)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += total_loss.item()\n",
    "        \n",
    "        epoch_train_loss = running_train_loss / len(train_loader)\n",
    "        train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs, \"both\")\n",
    "                loss_branch = criterion(outputs[0], labels)\n",
    "                loss_main = criterion(outputs[1], labels)\n",
    "                \n",
    "                total_loss = (weight_branch * loss_branch) + (weight_main * loss_main)\n",
    "                val_loss += total_loss.item()\n",
    "        \n",
    "        epoch_val_loss = val_loss/len(val_loader)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f'\\n--- EARLY STOPPING ATIVADO ---')\n",
    "            print(f'Parando o treino na época {epoch+1} pois a loss de validação não melhora há {patience} épocas.')\n",
    "            print(f'A melhor loss de validação foi: {best_val_loss:.4f}')\n",
    "            if best_model_state:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_loss_history) + 1), train_loss_history, label='Loss de Treinamento')\n",
    "    plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, label='Loss de Validação')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Curvas de Loss de Treinamento e Validação')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd9acd9be6862e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, confidence_threshold, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(loader.dataset)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    exited_early_count = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            branch_output, intermediate_output = model(samples, \"branch\")\n",
    "            branch_prob = F.softmax(branch_output, dim=1)\n",
    "            trusts, branch_preds = torch.max(branch_prob, 1)\n",
    "\n",
    "            batch_predictions = torch.zeros_like(labels)\n",
    "            \n",
    "            early_exit_mask = trusts > confidence_threshold\n",
    "            \n",
    "            if early_exit_mask.any():\n",
    "                batch_predictions[early_exit_mask] = branch_preds[early_exit_mask]\n",
    "                exited_early_count += early_exit_mask.sum().item()\n",
    "\n",
    "            main_branch_mask = ~early_exit_mask\n",
    "            if main_branch_mask.any():\n",
    "                intermediate_to_main = intermediate_output[main_branch_mask]\n",
    "                \n",
    "                main_output = model(None, \"main\", intermediate_to_main)\n",
    "                main_prob = F.softmax(main_output, dim=1)\n",
    "                _, main_preds = torch.max(main_prob, 1)\n",
    "                \n",
    "                batch_predictions[main_branch_mask] = main_preds\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions)\n",
    "    y_data = torch.cat(all_labels)\n",
    "\n",
    "    correct = (final_predictions == y_data).sum().item()\n",
    "    accuracy = 100 * correct / total_samples\n",
    "    exit_rate = 100 * exited_early_count / total_samples\n",
    "    avg_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Ataque'],\n",
    "                yticklabels=['Normal', 'Ataque'])\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão (Limiar de Confiança = {confidence_threshold})')\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    f1 = f1_score(y_data.numpy(), final_predictions.numpy())\n",
    "    \n",
    "    tpr = recall_score(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    tnr = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\\n\")\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positive Rate (TPR) / Recall: {tpr:.4f}\")\n",
    "    print(f\"True Negative Rate (TNR) / Specificity: {tnr:.4f}\")\n",
    "\n",
    "    return {\n",
    "            'accuracy': accuracy,\n",
    "            'exit_rate': exit_rate,\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'exited_early_count': exited_early_count,\n",
    "            'total_samples': total_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.1985, Validation Loss: 3.0635\n",
      "Epoch [2/50], Train Loss: 0.0350, Validation Loss: 3.0571\n",
      "Epoch [3/50], Train Loss: 0.0255, Validation Loss: 3.4568\n",
      "Epoch [4/50], Train Loss: 0.0211, Validation Loss: 4.2579\n",
      "Epoch [5/50], Train Loss: 0.0189, Validation Loss: 4.6265\n",
      "Epoch [6/50], Train Loss: 0.0173, Validation Loss: 4.5820\n",
      "Epoch [7/50], Train Loss: 0.0160, Validation Loss: 4.7465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m epochs = \u001b[32m50\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_val_loss, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_branch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_main\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, weight_branch, weight_main, lr, device, patience)\u001b[39m\n\u001b[32m     25\u001b[39m     optimizer.zero_grad()\n\u001b[32m     26\u001b[39m     total_loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     running_train_loss += total_loss.item()\n\u001b[32m     31\u001b[39m epoch_train_loss = running_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/PUC/PIBIC2026/code/.venv/lib/python3.12/site-packages/torch/optim/adam.py:673\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    666\u001b[39m             device_grads = torch._foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    667\u001b[39m                 device_grads, device_params, alpha=weight_decay\n\u001b[32m    668\u001b[39m             )\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    675\u001b[39m torch._foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[32m    677\u001b[39m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[32m    680\u001b[39m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "best_val_loss, train_losses, val_losses = train_model(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    epochs=epochs, \n",
    "    weight_branch=0.7,\n",
    "    weight_main=0.3,\n",
    "    lr=0.0001,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248ef82-7a24-4504-8cbc-ae2e4e3422fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Evaluation on Test Set using DataLoader ---\")\n",
    "\n",
    "thresholds_to_test = [0.8]\n",
    "\n",
    "for T in thresholds_to_test:\n",
    "    results = evaluate_model(model, test_loader, confidence_threshold=T, device=device)\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Threshold: {T}\")\n",
    "    print(f\"  Accuracy: {results['accuracy']:.2f}%\")\n",
    "    print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "    print(f\"  Early Exit Rate: {results['exit_rate']:.2f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1089e-5943-4eac-821d-c3df74361aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ee897-e98b-4857-a23c-9f3d92b122b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfbff9-201a-4c0f-900c-54d36066b8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
