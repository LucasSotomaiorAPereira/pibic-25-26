{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff7fbd-35cd-4bf4-9064-7542fbe66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_creator import CreatorDL\n",
    "creator = CreatorDL(seed=42, bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108772d1-330c-4ea7-b4fa-a96a9c5c06ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 1118865 | Teste: 559433 | Validação: 559433\n",
      "Processando a categoria: 'Fuzzers'\n",
      "  -> Treino: 16908 | Teste: 8454 | Validação: 8454\n",
      "Processando a categoria: 'Exploits'\n",
      "  -> Treino: 21374 | Teste: 10687 | Validação: 10687\n",
      "Processando a categoria: 'Backdoor'\n",
      "  -> Treino: 2329 | Teste: 1165 | Validação: 1165\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 8537 | Teste: 4268 | Validação: 4269\n",
      "Processando a categoria: 'Generic'\n",
      "  -> Treino: 9825 | Teste: 4913 | Validação: 4913\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 2990 | Teste: 1495 | Validação: 1495\n",
      "Processando a categoria: 'Shellcode'\n",
      "  -> Treino: 1190 | Teste: 595 | Validação: 596\n",
      "Processando a categoria: 'Analysis'\n",
      "  -> Treino: 613 | Teste: 306 | Validação: 307\n",
      "Processando a categoria: 'Worms'\n",
      "  -> Treino: 79 | Teste: 39 | Validação: 40\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 1182710 linhas\n",
      "Categorias presentes: ['Benign' 'Exploits' 'Reconnaissance' 'Fuzzers' 'DoS' 'Generic' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            1118865\n",
      "Exploits            21374\n",
      "Fuzzers             16908\n",
      "Generic              9825\n",
      "Reconnaissance       8537\n",
      "DoS                  2990\n",
      "Backdoor             2329\n",
      "Shellcode            1190\n",
      "Analysis              613\n",
      "Worms                  79\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 591355 linhas\n",
      "Categorias presentes: ['Benign' 'Generic' 'DoS' 'Reconnaissance' 'Exploits' 'Fuzzers' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4268\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            595\n",
      "Analysis             306\n",
      "Worms                 39\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 591359 linhas\n",
      "Categorias presentes: ['Benign' 'Fuzzers' 'Reconnaissance' 'Exploits' 'Generic' 'Analysis'\n",
      " 'Shellcode' 'Backdoor' 'DoS' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4269\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            596\n",
      "Analysis             307\n",
      "Worms                 40\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0516)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-1.4981e-07) tensor(4.5768) tensor(0.0510)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-2.9962e-07) tensor(3.5191) tensor(0.0518)\n"
     ]
    }
   ],
   "source": [
    "df_UNSW = creator.reader(\"NF-UNSW-NB15-v3\")\n",
    "\n",
    "df_train_UNSW, df_test_UNSW, df_val_UNSW = creator.splitter(df_UNSW)\n",
    "\n",
    "train_loader_UNSW, test_loader_UNSW, val_loader_UNSW = creator.balancer(df_train_UNSW, df_test_UNSW, df_val_UNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8fca6f-a878-4ab7-bbef-2d77092947de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 25994 | Teste: 12997 | Validação: 12998\n",
      "Processando a categoria: 'DDoS'\n",
      "  -> Treino: 3575441 | Teste: 1787720 | Validação: 1787721\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 4017095 | Teste: 2008547 | Validação: 2008548\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 847566 | Teste: 423783 | Validação: 423783\n",
      "Processando a categoria: 'Theft'\n",
      "  -> Treino: 807 | Teste: 404 | Validação: 404\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 8466903 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               4017095\n",
      "DDoS              3575441\n",
      "Reconnaissance     847566\n",
      "Benign              25994\n",
      "Theft                 807\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 4233451 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008547\n",
      "DDoS              1787720\n",
      "Reconnaissance     423783\n",
      "Benign              12997\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 4233454 linhas\n",
      "Categorias presentes: ['DoS' 'DDoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008548\n",
      "DDoS              1787721\n",
      "Reconnaissance     423783\n",
      "Benign              12998\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0232)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.1910e-07) tensor(1.4751) tensor(0.0235)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.7865e-07) tensor(5.3125) tensor(0.0232)\n"
     ]
    }
   ],
   "source": [
    "df_BOT= creator.reader(\"NF-BoT-IoT-v3\")\n",
    "\n",
    "df_train_BOT, df_test_BOT, df_val_BOT = creator.splitter(df_BOT)\n",
    "\n",
    "train_loader_BOT, test_loader_BOT, val_loader_BOT = creator.balancer(df_train_BOT, df_test_BOT, df_val_BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bc484e-aa09-40d0-967f-5ffe2b3c7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 8757313 | Teste: 4378656 | Validação: 4378657\n",
      "Processando a categoria: 'FTP-BruteForce'\n",
      "  -> Treino: 193360 | Teste: 96680 | Validação: 96680\n",
      "Processando a categoria: 'SSH-Bruteforce'\n",
      "  -> Treino: 94237 | Teste: 47118 | Validação: 47119\n",
      "Processando a categoria: 'DoS_attacks-GoldenEye'\n",
      "  -> Treino: 30650 | Teste: 15325 | Validação: 15325\n",
      "Processando a categoria: 'DoS_attacks-Slowloris'\n",
      "  -> Treino: 18020 | Teste: 9010 | Validação: 9010\n",
      "Processando a categoria: 'DoS_attacks-SlowHTTPTest'\n",
      "  -> Treino: 52775 | Teste: 26387 | Validação: 26388\n",
      "Processando a categoria: 'DoS_attacks-Hulk'\n",
      "  -> Treino: 50038 | Teste: 25019 | Validação: 25019\n",
      "Processando a categoria: 'DDoS_attacks-LOIC-HTTP'\n",
      "  -> Treino: 144294 | Teste: 72147 | Validação: 72148\n",
      "Processando a categoria: 'DDOS_attack-LOIC-UDP'\n",
      "  -> Treino: 1725 | Teste: 862 | Validação: 863\n",
      "Processando a categoria: 'DDOS_attack-HOIC'\n",
      "  -> Treino: 516155 | Teste: 258078 | Validação: 258078\n",
      "Processando a categoria: 'Brute_Force_-Web'\n",
      "  -> Treino: 809 | Teste: 404 | Validação: 405\n",
      "Processando a categoria: 'Brute_Force_-XSS'\n",
      "  -> Treino: 240 | Teste: 120 | Validação: 120\n",
      "Processando a categoria: 'SQL_Injection'\n",
      "  -> Treino: 220 | Teste: 110 | Validação: 110\n",
      "Processando a categoria: 'Infilteration'\n",
      "  -> Treino: 94076 | Teste: 47038 | Validação: 47038\n",
      "Processando a categoria: 'Bot'\n",
      "  -> Treino: 103851 | Teste: 51926 | Validação: 51926\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 10057763 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'FTP-BruteForce' 'DoS_attacks-Hulk' 'Bot' 'DoS_attacks-GoldenEye'\n",
      " 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Slowloris'\n",
      " 'Brute_Force_-Web' 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      8757313\n",
      "DDOS_attack-HOIC             516155\n",
      "FTP-BruteForce               193360\n",
      "DDoS_attacks-LOIC-HTTP       144294\n",
      "Bot                          103851\n",
      "SSH-Bruteforce                94237\n",
      "Infilteration                 94076\n",
      "DoS_attacks-SlowHTTPTest      52775\n",
      "DoS_attacks-Hulk              50038\n",
      "DoS_attacks-GoldenEye         30650\n",
      "DoS_attacks-Slowloris         18020\n",
      "DDOS_attack-LOIC-UDP           1725\n",
      "Brute_Force_-Web                809\n",
      "Brute_Force_-XSS                240\n",
      "SQL_Injection                   220\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 5028880 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDOS_attack-HOIC' 'FTP-BruteForce'\n",
      " 'SSH-Bruteforce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-LOIC-UDP' 'Bot'\n",
      " 'DoS_attacks-GoldenEye' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'DoS_attacks-Slowloris' 'Brute_Force_-Web' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378656\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72147\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47118\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26387\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            862\n",
      "Brute_Force_-Web                404\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 5028886 linhas\n",
      "Categorias presentes: ['Benign' 'FTP-BruteForce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'Bot' 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'Infilteration' 'DoS_attacks-GoldenEye' 'DoS_attacks-Slowloris'\n",
      " 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS' 'Brute_Force_-Web'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378657\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72148\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47119\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26388\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            863\n",
      "Brute_Force_-Web                405\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0473)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.4776) tensor(0.0477)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(2.7903) tensor(0.0478)\n"
     ]
    }
   ],
   "source": [
    "df_CIC= creator.reader(\"NF-CICIDS2018-v3\")\n",
    "\n",
    "df_train_CIC, df_test_CIC, df_val_CIC = creator.splitter(df_CIC)\n",
    "\n",
    "train_loader_CIC, test_loader_CIC, val_loader_CIC = creator.balancer(df_train_CIC, df_test_CIC, df_val_CIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9457e890-0e00-4842-a0eb-2b9a4bc57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "test_loaders = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "val_loaders = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, num_classes=2):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.exit1_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.exit2_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward_exit1(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit1_layers(features)\n",
    "\n",
    "    def forward_exit2(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit2_layers(features)\n",
    "\n",
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943446ef-8b5f-4e49-82a7-db448f6dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loaders, val_loaders, epochs, lr, device, current_threshold, patience=15):\n",
    "    print(f\"\\n[INIT] --- MODO DEBUG EXTREMO ATIVADO (CORRIGIDO) ---\")\n",
    "    print(f\"[INIT] Device: {device} | LR: {lr} | Threshold: {current_threshold}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.001, patience=7)\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    metrics = [\n",
    "        'loss1_a', 'loss1_b', 'loss1_c', 'loss_ex1_avg',\n",
    "        'loss2_a', 'loss2_b', 'loss2_c', 'loss_ex2_avg',\n",
    "        'l_joint', 'total_loss'\n",
    "    ]\n",
    "\n",
    "    history = {\n",
    "        'train': {k: [] for k in metrics},\n",
    "        'val': {k: [] for k in metrics}\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    max_train_batches = max(len(l) for l in train_loaders) \n",
    "    train_iter_loaders = [itertools.cycle(l) if len(l) < max_train_batches else l for l in train_loaders]\n",
    "    \n",
    "    max_val_batches = max(len(l) for l in val_loaders)\n",
    "    val_iter_loaders = [itertools.cycle(l) if len(l) < max_val_batches else l for l in val_loaders]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'#'*30} EPOCH {epoch+1}/{epochs} START {'#'*30}\")\n",
    "        model.train()\n",
    "        \n",
    "        running_metrics = {k: 0.0 for k in metrics}\n",
    "        total_steps = 0\n",
    "\n",
    "        loader_iterators = [iter(l) for l in train_iter_loaders]\n",
    "                \n",
    "        for batch_idx in range(max_train_batches):\n",
    "            print(f\"\\n>>> [TRAIN] BATCH {batch_idx} START <<<\")\n",
    "            \n",
    "            try:\n",
    "                batches = [next(it) for it in loader_iterators]\n",
    "            except StopIteration:\n",
    "                print(\"[DEBUG] StopIteration atingido.\")\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            (inputs_a, labels_a) = batches[0]\n",
    "            (inputs_b, labels_b) = batches[1]\n",
    "            (inputs_c, labels_c) = batches[2]\n",
    "            \n",
    "            inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "            inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "            inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "            \n",
    "            # --- DEBUG DE DADOS DE ENTRADA ---\n",
    "            print(f\"[DATA A] Shape: {inputs_a.shape} | Mean: {inputs_a.mean():.3f} | Std: {inputs_a.std():.3f} | Min: {inputs_a.min():.3f} | Max: {inputs_a.max():.3f}\")\n",
    "            \n",
    "            # --- FORWARD EXIT 1 ---\n",
    "            out1_a = model.forward_exit1(inputs_a)\n",
    "            out1_b = model.forward_exit1(inputs_b)\n",
    "            out1_c = model.forward_exit1(inputs_c)\n",
    "\n",
    "            # --- DEBUG LOGITS ---\n",
    "            print(f\"[LOGITS Ex1 A] Mean Abs: {out1_a.abs().mean():.3f} | Max: {out1_a.max():.3f}\")\n",
    "\n",
    "            probs_a = F.softmax(out1_a, dim=1)\n",
    "            conf_a, _ = torch.max(probs_a, dim=1)\n",
    "\n",
    "            probs_b = F.softmax(out1_b, dim=1)\n",
    "            conf_b, _ = torch.max(probs_b, dim=1)\n",
    "\n",
    "            probs_c = F.softmax(out1_c, dim=1)\n",
    "            conf_c, _ = torch.max(probs_c, dim=1)\n",
    "            \n",
    "            # --- DEBUG PROBABILIDADES DETALHADAS (CORRIGIDO) ---\n",
    "            # Determina o numero de classes dinamicamente para evitar erro se classes < 3\n",
    "            num_classes = probs_a.size(1)\n",
    "            k_val = min(3, num_classes) \n",
    "            \n",
    "            top_k_prob, top_k_idx = torch.topk(probs_a[0], k_val)\n",
    "            print(f\"[SAMPLE 0 PREDICTION A] Top{k_val} Probs: {top_k_prob.detach().cpu().numpy()} | Indices: {top_k_idx.detach().cpu().numpy()} | Label Real: {labels_a[0].item()}\")\n",
    "            print(f\"[CONFIDENCE A] Mean: {conf_a.mean().item():.3f} | Std: {conf_a.std().item():.3f}\")\n",
    "\n",
    "            mask_a_ex1 = conf_a > current_threshold\n",
    "            mask_b_ex1 = conf_b > current_threshold\n",
    "            mask_c_ex1 = conf_c > current_threshold\n",
    "\n",
    "            mask_a_ex2 = conf_a <= current_threshold\n",
    "            mask_b_ex2 = conf_b <= current_threshold\n",
    "            mask_c_ex2 = conf_c <= current_threshold\n",
    "\n",
    "            print(f\"[MASKS] A(Pass/Fail): {mask_a_ex1.sum()}/{mask_a_ex2.sum()} | B: {mask_b_ex1.sum()}/{mask_b_ex2.sum()} | C: {mask_c_ex1.sum()}/{mask_c_ex2.sum()}\")\n",
    "\n",
    "            # --- LOSS EXIT 1 ---\n",
    "            if mask_a_ex1.any():\n",
    "                loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "            else:\n",
    "                loss1_a = 0.0 * out1_a.sum()\n",
    "\n",
    "            if mask_b_ex1.any():\n",
    "                loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "            else:\n",
    "                loss1_b = 0.0 * out1_b.sum()\n",
    "\n",
    "            if mask_c_ex1.any():\n",
    "                loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "            else:\n",
    "                loss1_c = 0.0 * out1_c.sum()\n",
    "\n",
    "            loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "            print(f\"[LOSS Ex1] A: {loss1_a.item():.5f} | B: {loss1_b.item():.5f} | C: {loss1_c.item():.5f}\")\n",
    "\n",
    "            # --- FORWARD EXIT 2 ---\n",
    "            out2_a = model.forward_exit2(inputs_a)\n",
    "            out2_b = model.forward_exit2(inputs_b)\n",
    "            out2_c = model.forward_exit2(inputs_c)\n",
    "            \n",
    "            # Debug Logits Ex2\n",
    "            print(f\"[LOGITS Ex2 A] Mean Abs: {out2_a.abs().mean():.3f} | Max: {out2_a.max():.3f}\")\n",
    "            \n",
    "            if mask_a_ex2.any():\n",
    "                loss2_a = criterion(out2_a[mask_a_ex2], labels_a[mask_a_ex2])\n",
    "            else:\n",
    "                loss2_a = 0.0 * out2_a.sum() \n",
    "            \n",
    "            if mask_b_ex2.any():\n",
    "                loss2_b = criterion(out2_b[mask_b_ex2], labels_b[mask_b_ex2])\n",
    "            else:\n",
    "                loss2_b = 0.0 * out2_b.sum()\n",
    "\n",
    "            if mask_c_ex2.any():\n",
    "                loss2_c = criterion(out2_c[mask_c_ex2], labels_c[mask_c_ex2])\n",
    "            else:\n",
    "                loss2_c = 0.0 * out2_c.sum()\n",
    "\n",
    "            loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "            print(f\"[LOSS Ex2] A: {loss2_a.item():.5f} | B: {loss2_b.item():.5f} | C: {loss2_c.item():.5f}\")\n",
    "\n",
    "            l_joint = loss_ex1_avg + loss_ex2_avg\n",
    "            print(f\"** [JOINT LOSS] ** : {l_joint.item():.6f}\")\n",
    "\n",
    "            if torch.isnan(l_joint):\n",
    "                print(\"!!!!!!!!!! LOSS IS NAN !!!!!!!!!!\")\n",
    "                return current_threshold\n",
    "\n",
    "            l_joint.backward()\n",
    "            \n",
    "            # --- DEBUG DE GRADIENTES POR CAMADA ---\n",
    "            print(f\"[GRADIENTS CHECK]\")\n",
    "            has_grads = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_mean = param.grad.abs().mean().item()\n",
    "                    grad_max = param.grad.abs().max().item()\n",
    "                    print(f\"  -> Layer: {name} | Grad Mean: {grad_mean:.6f} | Grad Max: {grad_max:.6f}\")\n",
    "                    has_grads = True\n",
    "            \n",
    "            if not has_grads:\n",
    "                print(\"!!! NENHUM GRADIENTE ENCONTRADO EM TODO O MODELO !!!\")\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            print(f\"[GRADIENT NORM TOTAL] {total_norm:.4f}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_metrics['loss1_a'] += loss1_a.item()\n",
    "            running_metrics['loss1_b'] += loss1_b.item()\n",
    "            running_metrics['loss1_c'] += loss1_c.item()\n",
    "            running_metrics['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "            \n",
    "            running_metrics['loss2_a'] += loss2_a.item()\n",
    "            running_metrics['loss2_b'] += loss2_b.item()\n",
    "            running_metrics['loss2_c'] += loss2_c.item()\n",
    "            running_metrics['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "            \n",
    "            running_metrics['l_joint'] += l_joint.item()\n",
    "            \n",
    "            total_steps += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['train'][key].append(running_metrics[key] / total_steps)\n",
    "        \n",
    "        epoch_train_loss = history['train']['l_joint'][-1]\n",
    "        print(f\"\\n[EPOCH SUMMARY] Train Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        print(f\"\\n[VALIDATION] Starting...\")\n",
    "        model.eval()\n",
    "        running_metrics_val = {k: 0.0 for k in metrics}\n",
    "        total_steps_val = 0\n",
    "        \n",
    "        val_loader_iterators = [iter(l) for l in val_iter_loaders]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_val_idx in range(max_val_batches):\n",
    "                try:\n",
    "                    batches = [next(it) for it in val_loader_iterators]\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                \n",
    "                if batch_val_idx == 0:\n",
    "                    print(\"[VAL] Processando primeiro batch de validação...\")\n",
    "\n",
    "                (inputs_a, labels_a) = batches[0]\n",
    "                (inputs_b, labels_b) = batches[1]\n",
    "                (inputs_c, labels_c) = batches[2]\n",
    "                    \n",
    "                inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "                inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "                inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "    \n",
    "                out1_a = model.forward_exit1(inputs_a)\n",
    "                out1_b = model.forward_exit1(inputs_b)\n",
    "                out1_c = model.forward_exit1(inputs_c)\n",
    "    \n",
    "                probs_a = F.softmax(out1_a, dim=1)\n",
    "                conf_a, _ = torch.max(probs_a, dim=1)\n",
    "    \n",
    "                probs_b = F.softmax(out1_b, dim=1)\n",
    "                conf_b, _ = torch.max(probs_b, dim=1)\n",
    "    \n",
    "                probs_c = F.softmax(out1_c, dim=1)\n",
    "                conf_c, _ = torch.max(probs_c, dim=1)\n",
    "    \n",
    "                mask_a_ex1 = conf_a > current_threshold\n",
    "                mask_b_ex1 = conf_b > current_threshold\n",
    "                mask_c_ex1 = conf_c > current_threshold\n",
    "    \n",
    "                mask_a_ex2 = conf_a <= current_threshold\n",
    "                mask_b_ex2 = conf_b <= current_threshold\n",
    "                mask_c_ex2 = conf_c <= current_threshold\n",
    "    \n",
    "                if mask_a_ex1.any():\n",
    "                    loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "                else:\n",
    "                    loss1_a = 0.0 * out1_a.sum()\n",
    "    \n",
    "                if mask_b_ex1.any():\n",
    "                    loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "                else:\n",
    "                    loss1_b = 0.0 * out1_b.sum()\n",
    "    \n",
    "                if mask_c_ex1.any():\n",
    "                    loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "                else:\n",
    "                    loss1_c = 0.0 * out1_c.sum()\n",
    "    \n",
    "                loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "                \n",
    "                out2_a = model.forward_exit2(inputs_a)\n",
    "                out2_b = model.forward_exit2(inputs_b)\n",
    "                out2_c = model.forward_exit2(inputs_c)\n",
    "                    \n",
    "                if mask_a_ex2.any():\n",
    "                    loss2_a = criterion(out2_a[mask_a_ex2], labels_a[mask_a_ex2])\n",
    "                else:\n",
    "                    loss2_a = 0.0 * out2_a.sum() \n",
    "                \n",
    "                if mask_b_ex2.any():\n",
    "                    loss2_b = criterion(out2_b[mask_b_ex2], labels_b[mask_b_ex2])\n",
    "                else:\n",
    "                    loss2_b = 0.0 * out2_b.sum()\n",
    "    \n",
    "                if mask_c_ex2.any():\n",
    "                    loss2_c = criterion(out2_c[mask_c_ex2], labels_c[mask_c_ex2])\n",
    "                else:\n",
    "                    loss2_c = 0.0 * out2_c.sum()\n",
    "    \n",
    "                loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "    \n",
    "                l_joint = loss_ex1_avg + loss_ex2_avg\n",
    "                                        \n",
    "                running_metrics_val['loss1_a'] += loss1_a.item()\n",
    "                running_metrics_val['loss1_b'] += loss1_b.item()\n",
    "                running_metrics_val['loss1_c'] += loss1_c.item()\n",
    "                running_metrics_val['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "                \n",
    "                running_metrics_val['loss2_a'] += loss2_a.item()\n",
    "                running_metrics_val['loss2_b'] += loss2_b.item()\n",
    "                running_metrics_val['loss2_c'] += loss2_c.item()\n",
    "                running_metrics_val['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "                \n",
    "                running_metrics_val['l_joint'] += l_joint.item()\n",
    "                \n",
    "                total_steps_val += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['val'][key].append(running_metrics_val[key] / total_steps_val)\n",
    "\n",
    "        epoch_val_loss = history['val']['l_joint'][-1]\n",
    "        \n",
    "        thresh_print = current_threshold.item() if isinstance(current_threshold, torch.Tensor) else current_threshold\n",
    "        print(f'[EPOCH END] Val Loss: {epoch_val_loss:.4f} | Alpha: {thresh_print:.4f}')\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"!!! BEST MODEL SAVED !!! (Old: {best_val_loss:.4f} -> New: {epoch_val_loss:.4f})\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improve count: {epochs_no_improve}/{patience}\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"EARLY STOPPING TRIGGERED\")\n",
    "                if best_model_state: model.load_state_dict(best_model_state)\n",
    "                break\n",
    "                \n",
    "        scheduler.step(epoch_val_loss)\n",
    "                \n",
    "    epochs_range = range(1, len(history['train']['l_joint']) + 1)\n",
    "    \n",
    "    # Plotting (mantido igual)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Exit 1\")\n",
    "    ax.plot(epochs_range, history['train']['loss1_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex1_avg'], label='Tr Avg', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss1_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex1_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Exit 2\")\n",
    "    ax.plot(epochs_range, history['train']['loss2_a'], label='Tr A', color='blue', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_b'], label='Tr B', color='green', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_c'], label='Tr C', color='red', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex2_avg'], label='Tr Avg', color='black', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss2_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex2_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Global Optimization\")\n",
    "    ax.plot(epochs_range, history['train']['l_joint'], label='Tr Joint (Ex1 + Ex2)', color='purple')\n",
    "    ax.plot(epochs_range, history['val']['l_joint'], label='Val Joint', color='purple', linestyle='--')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return current_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd9acd9be6862e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, confidence_threshold, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(loader.dataset)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    exited_early_count = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            branch_output = model.forward_exit1(samples)\n",
    "            \n",
    "            branch_prob = F.softmax(branch_output, dim=1)\n",
    "            trusts, branch_preds = torch.max(branch_prob, 1)\n",
    "\n",
    "            batch_predictions = torch.zeros_like(labels)\n",
    "            \n",
    "            early_exit_mask = trusts > confidence_threshold\n",
    "            \n",
    "            if early_exit_mask.any():\n",
    "                batch_predictions[early_exit_mask] = branch_preds[early_exit_mask]\n",
    "                exited_early_count += early_exit_mask.sum().item()\n",
    "\n",
    "            main_branch_mask = ~early_exit_mask\n",
    "            if main_branch_mask.any():\n",
    "                \n",
    "                samples_to_main = samples[main_branch_mask]\n",
    "                \n",
    "                main_output = model.forward_exit2(samples_to_main)\n",
    "                \n",
    "                main_prob = F.softmax(main_output, dim=1)\n",
    "                _, main_preds = torch.max(main_prob, 1)\n",
    "                \n",
    "                batch_predictions[main_branch_mask] = main_preds\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions)\n",
    "    y_data = torch.cat(all_labels)\n",
    "\n",
    "    correct = (final_predictions == y_data).sum().item()\n",
    "    accuracy = 100 * correct / total_samples\n",
    "    exit_rate = 100 * exited_early_count / total_samples\n",
    "    avg_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Ataque'],\n",
    "                yticklabels=['Normal', 'Ataque'])\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão (Limiar de Confiança = {confidence_threshold})')\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    f1 = f1_score(y_data.numpy(), final_predictions.numpy())\n",
    "    \n",
    "    tpr = recall_score(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\\n\")\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positive Rate (TPR) / Recall: {tpr:.4f}\")\n",
    "    print(f\"True Negative Rate (TNR) / Specificity: {tnr:.4f}\")\n",
    "\n",
    "    return {\n",
    "            'accuracy': accuracy,\n",
    "            'exit_rate': exit_rate,\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'exited_early_count': exited_early_count,\n",
    "            'total_samples': total_samples,\n",
    "            'f1': f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bfbf4b5-dcc6-4bd5-96e5-b96988ebd70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teste_ljoint9'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'teste_ljoint9'\n",
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INIT] --- MODO DEBUG EXTREMO ATIVADO (CORRIGIDO) ---\n",
      "[INIT] Device: cuda | LR: 0.0001 | Threshold: 0.55\n",
      "\n",
      "############################## EPOCH 1/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.087 | Max: 0.053\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.54157937 0.45842063] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.540 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 4/2044 | B: 111/1937 | C: 87/1961\n",
      "[LOSS Ex1] A: 0.80643 | B: 0.58085 | C: 0.82449\n",
      "[LOGITS Ex2 A] Mean Abs: 0.025 | Max: -0.007\n",
      "[LOSS Ex2] A: 0.69312 | B: 0.69286 | C: 0.69319\n",
      "** [JOINT LOSS] ** : 1.430313\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000902 | Grad Max: 0.029290\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010389 | Grad Max: 0.052381\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016483 | Grad Max: 0.068898\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.221262 | Grad Max: 0.221262\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000002 | Grad Max: 0.000043\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000015 | Grad Max: 0.000149\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000001 | Grad Max: 0.000054\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000019 | Grad Max: 0.000140\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000001 | Grad Max: 0.000055\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000038 | Grad Max: 0.000309\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000002 | Grad Max: 0.000064\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000094 | Grad Max: 0.000729\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000060 | Grad Max: 0.000533\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.006331 | Grad Max: 0.006331\n",
      "[GRADIENT NORM TOTAL] 0.4553\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.086 | Max: 0.085\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.54140913 0.45859092] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.539 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 5/2043 | B: 109/1939 | C: 105/1943\n",
      "[LOSS Ex1] A: 0.80593 | B: 0.58199 | C: 0.80719\n",
      "[LOGITS Ex2 A] Mean Abs: 0.025 | Max: -0.005\n",
      "[LOSS Ex2] A: 0.69206 | B: 0.69235 | C: 0.69287\n",
      "** [JOINT LOSS] ** : 1.424129\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001027 | Grad Max: 0.029102\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009119 | Grad Max: 0.047954\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014998 | Grad Max: 0.073260\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.202695 | Grad Max: 0.202695\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000002 | Grad Max: 0.000059\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000015 | Grad Max: 0.000170\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000001 | Grad Max: 0.000072\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000018 | Grad Max: 0.000173\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000001 | Grad Max: 0.000066\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000033 | Grad Max: 0.000252\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000001 | Grad Max: 0.000066\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000069 | Grad Max: 0.000688\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000056 | Grad Max: 0.000553\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003536 | Grad Max: 0.003536\n",
      "[GRADIENT NORM TOTAL] 0.4289\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.085 | Max: 0.066\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5409358 0.4590642] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.539 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 7/2041 | B: 122/1926 | C: 78/1970\n",
      "[LOSS Ex1] A: 0.77610 | B: 0.58441 | C: 0.81737\n",
      "[LOGITS Ex2 A] Mean Abs: 0.025 | Max: 0.006\n",
      "[LOSS Ex2] A: 0.69077 | B: 0.69241 | C: 0.69262\n",
      "** [JOINT LOSS] ** : 1.417894\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000827 | Grad Max: 0.026855\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.008403 | Grad Max: 0.043189\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013539 | Grad Max: 0.059040\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.173623 | Grad Max: 0.173623\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000003 | Grad Max: 0.000081\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000018 | Grad Max: 0.000264\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000002 | Grad Max: 0.000078\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000021 | Grad Max: 0.000200\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000001 | Grad Max: 0.000059\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000038 | Grad Max: 0.000354\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000002 | Grad Max: 0.000104\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000096 | Grad Max: 0.001177\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000096 | Grad Max: 0.000688\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008497 | Grad Max: 0.008497\n",
      "[GRADIENT NORM TOTAL] 0.3688\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.085 | Max: 0.053\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5382925  0.46170747] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.539 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 4/2044 | B: 105/1751 | C: 89/1959\n",
      "[LOSS Ex1] A: 0.80937 | B: 0.58487 | C: 0.81244\n",
      "[LOGITS Ex2 A] Mean Abs: 0.025 | Max: 0.024\n",
      "[LOSS Ex2] A: 0.68929 | B: 0.69205 | C: 0.69211\n",
      "** [JOINT LOSS] ** : 1.426713\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000928 | Grad Max: 0.034244\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010015 | Grad Max: 0.050289\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016396 | Grad Max: 0.066883\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.213033 | Grad Max: 0.213033\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000003 | Grad Max: 0.000101\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000024 | Grad Max: 0.000270\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000002 | Grad Max: 0.000093\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000025 | Grad Max: 0.000259\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000001 | Grad Max: 0.000083\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000044 | Grad Max: 0.000400\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000002 | Grad Max: 0.000114\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000111 | Grad Max: 0.000882\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000117 | Grad Max: 0.001016\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009555 | Grad Max: 0.009555\n",
      "[GRADIENT NORM TOTAL] 0.4465\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.085 | Max: 0.054\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53570426 0.46429572] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.538 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 6/2042 | B: 107/1941 | C: 80/1968\n",
      "[LOSS Ex1] A: 0.80474 | B: 0.58286 | C: 0.81036\n",
      "[LOGITS Ex2 A] Mean Abs: 0.031 | Max: 0.047\n",
      "[LOSS Ex2] A: 0.68885 | B: 0.69230 | C: 0.69138\n",
      "** [JOINT LOSS] ** : 1.423497\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000895 | Grad Max: 0.029997\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009867 | Grad Max: 0.049117\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015577 | Grad Max: 0.062595\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.206828 | Grad Max: 0.206828\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000003 | Grad Max: 0.000109\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000041 | Grad Max: 0.000395\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000002 | Grad Max: 0.000081\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000042 | Grad Max: 0.000349\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000002 | Grad Max: 0.000093\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000073 | Grad Max: 0.000612\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000002 | Grad Max: 0.000177\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000190 | Grad Max: 0.002159\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000130 | Grad Max: 0.000848\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.015800 | Grad Max: 0.015800\n",
      "[GRADIENT NORM TOTAL] 0.4300\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.084 | Max: 0.063\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.54151016 0.4584898 ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.538 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 9/2039 | B: 106/1942 | C: 100/1948\n",
      "[LOSS Ex1] A: 0.80732 | B: 0.58231 | C: 0.81862\n",
      "[LOGITS Ex2 A] Mean Abs: 0.035 | Max: 0.064\n",
      "[LOSS Ex2] A: 0.68856 | B: 0.69171 | C: 0.69087\n",
      "** [JOINT LOSS] ** : 1.426464\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000933 | Grad Max: 0.034604\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010206 | Grad Max: 0.052027\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016754 | Grad Max: 0.068448\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.217812 | Grad Max: 0.217812\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000004 | Grad Max: 0.000132\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000067 | Grad Max: 0.000542\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000003 | Grad Max: 0.000108\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000069 | Grad Max: 0.000495\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000002 | Grad Max: 0.000121\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000115 | Grad Max: 0.000899\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000003 | Grad Max: 0.000174\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000303 | Grad Max: 0.002361\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000188 | Grad Max: 0.001183\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025196 | Grad Max: 0.025196\n",
      "[GRADIENT NORM TOTAL] 0.4570\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.084 | Max: 0.062\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5320568 0.4679432] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.538 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 3/2045 | B: 119/1929 | C: 84/1964\n",
      "[LOSS Ex1] A: 0.81579 | B: 0.58650 | C: 0.81787\n",
      "[LOGITS Ex2 A] Mean Abs: 0.037 | Max: 0.073\n",
      "[LOSS Ex2] A: 0.68591 | B: 0.69057 | C: 0.68981\n",
      "** [JOINT LOSS] ** : 1.428815\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001030 | Grad Max: 0.043483\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010663 | Grad Max: 0.054560\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.018421 | Grad Max: 0.074851\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.229989 | Grad Max: 0.229989\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000005 | Grad Max: 0.000177\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000048 | Grad Max: 0.000508\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000002 | Grad Max: 0.000099\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000043 | Grad Max: 0.000364\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000002 | Grad Max: 0.000093\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000067 | Grad Max: 0.000564\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000003 | Grad Max: 0.000107\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000149 | Grad Max: 0.001240\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000162 | Grad Max: 0.001018\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008882 | Grad Max: 0.008882\n",
      "[GRADIENT NORM TOTAL] 0.4900\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.084 | Max: 0.071\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5388861  0.46111396] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.537 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 5/2043 | B: 96/1760 | C: 93/1955\n",
      "[LOSS Ex1] A: 0.76605 | B: 0.58205 | C: 0.80411\n",
      "[LOGITS Ex2 A] Mean Abs: 0.038 | Max: 0.081\n",
      "[LOSS Ex2] A: 0.68400 | B: 0.68975 | C: 0.68962\n",
      "** [JOINT LOSS] ** : 1.405190\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000950 | Grad Max: 0.028747\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.006688 | Grad Max: 0.031570\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011788 | Grad Max: 0.049205\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.132862 | Grad Max: 0.132862\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000006 | Grad Max: 0.000205\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000044 | Grad Max: 0.000684\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000003 | Grad Max: 0.000104\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000036 | Grad Max: 0.000352\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000002 | Grad Max: 0.000079\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000053 | Grad Max: 0.000622\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000003 | Grad Max: 0.000111\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000104 | Grad Max: 0.001242\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000191 | Grad Max: 0.001048\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.002714 | Grad Max: 0.002714\n",
      "[GRADIENT NORM TOTAL] 0.3089\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.083 | Max: 0.058\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5391728  0.46082723] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.537 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 6/1610 | B: 102/1946 | C: 89/1959\n",
      "[LOSS Ex1] A: 0.80766 | B: 0.58271 | C: 0.80599\n",
      "[LOGITS Ex2 A] Mean Abs: 0.040 | Max: 0.107\n",
      "[LOSS Ex2] A: 0.68287 | B: 0.68796 | C: 0.68901\n",
      "** [JOINT LOSS] ** : 1.418734\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000937 | Grad Max: 0.036276\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009821 | Grad Max: 0.049092\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016134 | Grad Max: 0.064247\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.205062 | Grad Max: 0.205062\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000007 | Grad Max: 0.000201\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000072 | Grad Max: 0.000774\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000003 | Grad Max: 0.000107\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000058 | Grad Max: 0.000423\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000002 | Grad Max: 0.000119\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000083 | Grad Max: 0.000846\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000003 | Grad Max: 0.000130\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000194 | Grad Max: 0.001380\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000242 | Grad Max: 0.001320\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014140 | Grad Max: 0.014140\n",
      "[GRADIENT NORM TOTAL] 0.4357\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.083 | Max: 0.046\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5388997  0.46110028] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.536 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 3/2045 | B: 100/1948 | C: 87/1961\n",
      "[LOSS Ex1] A: 0.80082 | B: 0.58399 | C: 0.80756\n",
      "[LOGITS Ex2 A] Mean Abs: 0.045 | Max: 0.125\n",
      "[LOSS Ex2] A: 0.67734 | B: 0.68642 | C: 0.68820\n",
      "** [JOINT LOSS] ** : 1.414777\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000919 | Grad Max: 0.029062\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009922 | Grad Max: 0.048567\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015624 | Grad Max: 0.065265\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.207160 | Grad Max: 0.207160\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000011 | Grad Max: 0.000307\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000073 | Grad Max: 0.001112\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000004 | Grad Max: 0.000200\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000056 | Grad Max: 0.000469\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000003 | Grad Max: 0.000133\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000077 | Grad Max: 0.000578\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000004 | Grad Max: 0.000141\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000133 | Grad Max: 0.001210\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000314 | Grad Max: 0.001804\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003356 | Grad Max: 0.003356\n",
      "[GRADIENT NORM TOTAL] 0.4319\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.083 | Max: 0.075\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5388734  0.46112663] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.536 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 113/1935 | C: 71/1977\n",
      "[LOSS Ex1] A: 0.81248 | B: 0.58654 | C: 0.79876\n",
      "[LOGITS Ex2 A] Mean Abs: 0.051 | Max: 0.155\n",
      "[LOSS Ex2] A: 0.67751 | B: 0.68597 | C: 0.68784\n",
      "** [JOINT LOSS] ** : 1.416365\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001409 | Grad Max: 0.050989\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009742 | Grad Max: 0.047264\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016301 | Grad Max: 0.083791\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.202164 | Grad Max: 0.202164\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000012 | Grad Max: 0.000373\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000106 | Grad Max: 0.001638\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000005 | Grad Max: 0.000245\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000076 | Grad Max: 0.000728\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000003 | Grad Max: 0.000158\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000094 | Grad Max: 0.000778\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000004 | Grad Max: 0.000156\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000154 | Grad Max: 0.001468\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000313 | Grad Max: 0.001629\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008263 | Grad Max: 0.008263\n",
      "[GRADIENT NORM TOTAL] 0.4885\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.082 | Max: 0.057\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5384072  0.46159282] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.536 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 5/2043 | B: 93/1763 | C: 87/1961\n",
      "[LOSS Ex1] A: 0.76203 | B: 0.58401 | C: 0.80905\n",
      "[LOGITS Ex2 A] Mean Abs: 0.063 | Max: 0.204\n",
      "[LOSS Ex2] A: 0.67073 | B: 0.68422 | C: 0.68432\n",
      "** [JOINT LOSS] ** : 1.398117\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.000932 | Grad Max: 0.030003\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.007638 | Grad Max: 0.037921\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012250 | Grad Max: 0.048490\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.144872 | Grad Max: 0.144872\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000014 | Grad Max: 0.000381\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000097 | Grad Max: 0.001465\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000005 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000065 | Grad Max: 0.000594\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000003 | Grad Max: 0.000142\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000076 | Grad Max: 0.000527\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000158\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000142 | Grad Max: 0.001046\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000432 | Grad Max: 0.002128\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.001810 | Grad Max: 0.001810\n",
      "[GRADIENT NORM TOTAL] 0.3296\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.082 | Max: 0.046\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5335658  0.46643418] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.535 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 4/2044 | B: 98/1950 | C: 76/1972\n",
      "[LOSS Ex1] A: 0.80195 | B: 0.58460 | C: 0.81292\n",
      "[LOGITS Ex2 A] Mean Abs: 0.082 | Max: 0.259\n",
      "[LOSS Ex2] A: 0.66865 | B: 0.68074 | C: 0.68321\n",
      "** [JOINT LOSS] ** : 1.410688\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001099 | Grad Max: 0.035032\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010958 | Grad Max: 0.053987\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017025 | Grad Max: 0.068610\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.217881 | Grad Max: 0.217881\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000014 | Grad Max: 0.000466\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000198 | Grad Max: 0.001569\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000006 | Grad Max: 0.000230\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000147 | Grad Max: 0.000927\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000004 | Grad Max: 0.000161\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000163 | Grad Max: 0.001020\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000203\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000295 | Grad Max: 0.001984\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000488 | Grad Max: 0.002471\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.019864 | Grad Max: 0.019864\n",
      "[GRADIENT NORM TOTAL] 0.4670\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.082 | Max: 0.046\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5326383 0.4673617] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.535 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 97/1951 | C: 49/1327\n",
      "[LOSS Ex1] A: 0.80365 | B: 0.58603 | C: 0.79153\n",
      "[LOGITS Ex2 A] Mean Abs: 0.097 | Max: 0.319\n",
      "[LOSS Ex2] A: 0.66938 | B: 0.67880 | C: 0.67767\n",
      "** [JOINT LOSS] ** : 1.402354\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001082 | Grad Max: 0.035627\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009612 | Grad Max: 0.046720\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015326 | Grad Max: 0.058287\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.187948 | Grad Max: 0.187948\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000016 | Grad Max: 0.000519\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000206 | Grad Max: 0.002373\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000007 | Grad Max: 0.000227\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000143 | Grad Max: 0.000948\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000004 | Grad Max: 0.000134\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000151 | Grad Max: 0.000953\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000199\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000263 | Grad Max: 0.001843\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000527 | Grad Max: 0.002666\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018358 | Grad Max: 0.018358\n",
      "[GRADIENT NORM TOTAL] 0.4145\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.4174\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.3558 | Alpha: 0.5500\n",
      "!!! BEST MODEL SAVED !!! (Old: inf -> New: 1.3558)\n",
      "\n",
      "############################## EPOCH 2/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.081 | Max: 0.055\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53785586 0.46214414] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.535 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 4/2044 | B: 108/1940 | C: 93/1955\n",
      "[LOSS Ex1] A: 0.80666 | B: 0.58827 | C: 0.80869\n",
      "[LOGITS Ex2 A] Mean Abs: 0.100 | Max: 0.353\n",
      "[LOSS Ex2] A: 0.66675 | B: 0.67686 | C: 0.67740\n",
      "** [JOINT LOSS] ** : 1.408206\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001199 | Grad Max: 0.040257\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011117 | Grad Max: 0.054611\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017952 | Grad Max: 0.070643\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.221633 | Grad Max: 0.221633\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000019 | Grad Max: 0.000600\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000178 | Grad Max: 0.002535\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000006 | Grad Max: 0.000195\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000101 | Grad Max: 0.000792\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000004 | Grad Max: 0.000123\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000102 | Grad Max: 0.000681\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000166\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000189 | Grad Max: 0.001335\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000552 | Grad Max: 0.002549\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011718 | Grad Max: 0.011718\n",
      "[GRADIENT NORM TOTAL] 0.4802\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.082 | Max: 0.055\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5278521 0.4721479] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.535 | Std: 0.003\n",
      "[MASKS] A(Pass/Fail): 3/2045 | B: 90/1766 | C: 66/1982\n",
      "[LOSS Ex1] A: 0.80785 | B: 0.58587 | C: 0.79857\n",
      "[LOGITS Ex2 A] Mean Abs: 0.111 | Max: 0.432\n",
      "[LOSS Ex2] A: 0.65750 | B: 0.67524 | C: 0.67734\n",
      "** [JOINT LOSS] ** : 1.400793\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001162 | Grad Max: 0.041634\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010168 | Grad Max: 0.049283\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016888 | Grad Max: 0.063067\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.202118 | Grad Max: 0.202118\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000023 | Grad Max: 0.000571\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000170 | Grad Max: 0.002775\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000007 | Grad Max: 0.000245\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000088 | Grad Max: 0.000675\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000004 | Grad Max: 0.000111\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000086 | Grad Max: 0.000485\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000157\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000160 | Grad Max: 0.001052\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000635 | Grad Max: 0.002285\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005667 | Grad Max: 0.005667\n",
      "[GRADIENT NORM TOTAL] 0.4458\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.081 | Max: 0.061\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53648937 0.46351066] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.534 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 3/2045 | B: 95/1953 | C: 81/1967\n",
      "[LOSS Ex1] A: 0.73866 | B: 0.58652 | C: 0.80280\n",
      "[LOGITS Ex2 A] Mean Abs: 0.122 | Max: 0.473\n",
      "[LOSS Ex2] A: 0.65383 | B: 0.66814 | C: 0.67195\n",
      "** [JOINT LOSS] ** : 1.373967\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001042 | Grad Max: 0.029957\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.005936 | Grad Max: 0.028378\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010563 | Grad Max: 0.041080\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.098358 | Grad Max: 0.098358\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000028 | Grad Max: 0.000845\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000250 | Grad Max: 0.003908\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000009 | Grad Max: 0.000327\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000140 | Grad Max: 0.001022\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000005 | Grad Max: 0.000165\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000125 | Grad Max: 0.000921\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000193\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000191 | Grad Max: 0.001336\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000763 | Grad Max: 0.002716\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.001100 | Grad Max: 0.001100\n",
      "[GRADIENT NORM TOTAL] 0.2751\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.081 | Max: 0.051\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5368648  0.46313518] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.534 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 4/1612 | B: 93/1955 | C: 64/1984\n",
      "[LOSS Ex1] A: 0.80332 | B: 0.58783 | C: 0.80677\n",
      "[LOGITS Ex2 A] Mean Abs: 0.135 | Max: 0.533\n",
      "[LOSS Ex2] A: 0.65419 | B: 0.66484 | C: 0.67076\n",
      "** [JOINT LOSS] ** : 1.395907\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001301 | Grad Max: 0.039878\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011169 | Grad Max: 0.052517\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017564 | Grad Max: 0.067294\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.215829 | Grad Max: 0.215829\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000032 | Grad Max: 0.000799\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000238 | Grad Max: 0.003761\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000009 | Grad Max: 0.000261\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000114 | Grad Max: 0.001099\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000005 | Grad Max: 0.000138\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000097 | Grad Max: 0.000653\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000196\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000175 | Grad Max: 0.001147\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000751 | Grad Max: 0.003065\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007086 | Grad Max: 0.007086\n",
      "[GRADIENT NORM TOTAL] 0.4739\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.081 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5365528 0.4634472] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.533 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 104/1944 | C: 74/1974\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58623 | C: 0.80284\n",
      "[LOGITS Ex2 A] Mean Abs: 0.166 | Max: 0.649\n",
      "[LOSS Ex2] A: 0.64295 | B: 0.66368 | C: 0.66334\n",
      "** [JOINT LOSS] ** : 1.119678\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001061 | Grad Max: 0.020243\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.004739 | Grad Max: 0.020429\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006782 | Grad Max: 0.031925\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.028031 | Grad Max: 0.028031\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000045 | Grad Max: 0.001177\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000545 | Grad Max: 0.005558\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000016 | Grad Max: 0.000611\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000324 | Grad Max: 0.001678\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000262\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000245 | Grad Max: 0.001443\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000241\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000256 | Grad Max: 0.001828\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000848 | Grad Max: 0.003551\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009716 | Grad Max: 0.009716\n",
      "[GRADIENT NORM TOTAL] 0.2014\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.080 | Max: 0.066\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5365013 0.4634987] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.533 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 88/1768 | C: 76/1972\n",
      "[LOSS Ex1] A: 0.80399 | B: 0.58771 | C: 0.79295\n",
      "[LOGITS Ex2 A] Mean Abs: 0.195 | Max: 0.755\n",
      "[LOSS Ex2] A: 0.65462 | B: 0.65569 | C: 0.65701\n",
      "** [JOINT LOSS] ** : 1.383990\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001747 | Grad Max: 0.052339\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011434 | Grad Max: 0.054771\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016142 | Grad Max: 0.083546\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.198909 | Grad Max: 0.198909\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000050 | Grad Max: 0.001460\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000578 | Grad Max: 0.005784\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000017 | Grad Max: 0.000508\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000354 | Grad Max: 0.001803\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000268\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000260 | Grad Max: 0.001577\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000291\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000323 | Grad Max: 0.002065\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000782 | Grad Max: 0.004643\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023021 | Grad Max: 0.023021\n",
      "[GRADIENT NORM TOTAL] 0.5021\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.080 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53606886 0.46393114] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.533 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 93/1955 | C: 74/1974\n",
      "[LOSS Ex1] A: 0.80551 | B: 0.58840 | C: 0.80372\n",
      "[LOGITS Ex2 A] Mean Abs: 0.213 | Max: 0.874\n",
      "[LOSS Ex2] A: 0.64063 | B: 0.64484 | C: 0.64810\n",
      "** [JOINT LOSS] ** : 1.377068\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001547 | Grad Max: 0.045126\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.013972 | Grad Max: 0.063317\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.018223 | Grad Max: 0.068439\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.216142 | Grad Max: 0.216142\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000059 | Grad Max: 0.001923\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000859 | Grad Max: 0.005826\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000024 | Grad Max: 0.000629\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000555 | Grad Max: 0.002550\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000351\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000402 | Grad Max: 0.001856\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000013 | Grad Max: 0.000354\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000469 | Grad Max: 0.002441\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001065 | Grad Max: 0.005596\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.032077 | Grad Max: 0.032077\n",
      "[GRADIENT NORM TOTAL] 0.5150\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.080 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52975136 0.47024864] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.533 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 89/1959 | C: 60/1988\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58954 | C: 0.80137\n",
      "[LOGITS Ex2 A] Mean Abs: 0.212 | Max: 0.920\n",
      "[LOSS Ex2] A: 0.63104 | B: 0.64531 | C: 0.64864\n",
      "** [JOINT LOSS] ** : 1.105299\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001186 | Grad Max: 0.020859\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.004470 | Grad Max: 0.018609\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006708 | Grad Max: 0.032983\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.030334 | Grad Max: 0.030334\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000049 | Grad Max: 0.001326\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000385 | Grad Max: 0.005228\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000012 | Grad Max: 0.000421\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000169 | Grad Max: 0.001497\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000006 | Grad Max: 0.000151\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000116 | Grad Max: 0.000687\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000189\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000146 | Grad Max: 0.001036\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001049 | Grad Max: 0.003227\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003682 | Grad Max: 0.003682\n",
      "[GRADIENT NORM TOTAL] 0.1996\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.080 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5305701  0.46942994] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.532 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 102/1946 | C: 71/1977\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58805 | C: 0.79812\n",
      "[LOGITS Ex2 A] Mean Abs: 0.223 | Max: 0.959\n",
      "[LOSS Ex2] A: 0.63480 | B: 0.64540 | C: 0.64949\n",
      "** [JOINT LOSS] ** : 1.105287\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001249 | Grad Max: 0.021342\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009158 | Grad Max: 0.042975\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006761 | Grad Max: 0.031646\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.026636 | Grad Max: 0.026636\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000092 | Grad Max: 0.002027\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001419 | Grad Max: 0.009562\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000035 | Grad Max: 0.000965\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000842 | Grad Max: 0.003713\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000338\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000541 | Grad Max: 0.002200\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000368\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000499 | Grad Max: 0.002225\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000941 | Grad Max: 0.004074\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022105 | Grad Max: 0.022105\n",
      "[GRADIENT NORM TOTAL] 0.2812\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53508544 0.46491456] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.532 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 86/1770 | C: 92/1956\n",
      "[LOSS Ex1] A: 0.80390 | B: 0.58953 | C: 0.79576\n",
      "[LOGITS Ex2 A] Mean Abs: 0.219 | Max: 1.170\n",
      "[LOSS Ex2] A: 0.62665 | B: 0.63640 | C: 0.63653\n",
      "** [JOINT LOSS] ** : 1.362922\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001603 | Grad Max: 0.042711\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011846 | Grad Max: 0.055683\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017484 | Grad Max: 0.063675\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.208885 | Grad Max: 0.208885\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000065 | Grad Max: 0.001911\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000434 | Grad Max: 0.006797\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000015 | Grad Max: 0.000491\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000157 | Grad Max: 0.000993\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000171\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000098 | Grad Max: 0.000658\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000183\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000127 | Grad Max: 0.001122\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001203 | Grad Max: 0.004323\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007335 | Grad Max: 0.007335\n",
      "[GRADIENT NORM TOTAL] 0.4802\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.080 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5244057  0.47559425] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.532 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 87/1961 | C: 59/1989\n",
      "[LOSS Ex1] A: 0.80460 | B: 0.58987 | C: 0.79393\n",
      "[LOGITS Ex2 A] Mean Abs: 0.237 | Max: 1.177\n",
      "[LOSS Ex2] A: 0.62638 | B: 0.62126 | C: 0.62199\n",
      "** [JOINT LOSS] ** : 1.352678\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001728 | Grad Max: 0.045178\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015196 | Grad Max: 0.064786\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.018016 | Grad Max: 0.066800\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.208269 | Grad Max: 0.208269\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000085 | Grad Max: 0.002983\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001206 | Grad Max: 0.008701\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000031 | Grad Max: 0.000797\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000708 | Grad Max: 0.002916\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000381\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000434 | Grad Max: 0.001707\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000380\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000431 | Grad Max: 0.001956\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001405 | Grad Max: 0.006746\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.027197 | Grad Max: 0.027197\n",
      "[GRADIENT NORM TOTAL] 0.5247\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.053\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5344121  0.46558785] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.532 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 80/1968 | C: 61/1987\n",
      "[LOSS Ex1] A: 0.80184 | B: 0.59074 | C: 0.79693\n",
      "[LOGITS Ex2 A] Mean Abs: 0.234 | Max: 1.310\n",
      "[LOSS Ex2] A: 0.62118 | B: 0.61865 | C: 0.61793\n",
      "** [JOINT LOSS] ** : 1.349089\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001577 | Grad Max: 0.042674\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011879 | Grad Max: 0.052322\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017861 | Grad Max: 0.066272\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.213354 | Grad Max: 0.213354\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000061 | Grad Max: 0.002070\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000451 | Grad Max: 0.006731\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000015 | Grad Max: 0.000716\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000183 | Grad Max: 0.001488\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000150\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000108 | Grad Max: 0.000689\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000191\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000109 | Grad Max: 0.000890\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001417 | Grad Max: 0.004679\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003578 | Grad Max: 0.003578\n",
      "[GRADIENT NORM TOTAL] 0.4886\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.044\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53498656 0.46501344] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.532 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/1615 | B: 97/1951 | C: 64/1984\n",
      "[LOSS Ex1] A: 0.80104 | B: 0.58954 | C: 0.79956\n",
      "[LOGITS Ex2 A] Mean Abs: 0.241 | Max: 1.345\n",
      "[LOSS Ex2] A: 0.61857 | B: 0.61408 | C: 0.60799\n",
      "** [JOINT LOSS] ** : 1.343593\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001686 | Grad Max: 0.041482\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.013786 | Grad Max: 0.053582\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.018033 | Grad Max: 0.067616\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.218737 | Grad Max: 0.218737\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000087 | Grad Max: 0.002302\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001117 | Grad Max: 0.011020\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000026 | Grad Max: 0.000815\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000566 | Grad Max: 0.002827\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000275\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000317 | Grad Max: 0.001421\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000284\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000250 | Grad Max: 0.001394\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001469 | Grad Max: 0.005257\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008762 | Grad Max: 0.008762\n",
      "[GRADIENT NORM TOTAL] 0.5162\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5345972  0.46540278] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.531 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 68/1788 | C: 12/1364\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58956 | C: 0.76474\n",
      "[LOGITS Ex2 A] Mean Abs: 0.274 | Max: 1.520\n",
      "[LOSS Ex2] A: 0.61313 | B: 0.60890 | C: 0.61348\n",
      "** [JOINT LOSS] ** : 1.063271\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001407 | Grad Max: 0.024655\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009286 | Grad Max: 0.063235\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004141 | Grad Max: 0.023147\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.020331 | Grad Max: 0.020331\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000093 | Grad Max: 0.003270\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001314 | Grad Max: 0.012778\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000029 | Grad Max: 0.000968\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000667 | Grad Max: 0.003529\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000012 | Grad Max: 0.000320\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000358 | Grad Max: 0.001536\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000334\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000309 | Grad Max: 0.001594\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001333 | Grad Max: 0.006252\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017961 | Grad Max: 0.017961\n",
      "[GRADIENT NORM TOTAL] 0.2581\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.2958\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.1294 | Alpha: 0.5500\n",
      "!!! BEST MODEL SAVED !!! (Old: 1.3558 -> New: 1.1294)\n",
      "\n",
      "############################## EPOCH 3/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.058\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5344573 0.4655427] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.531 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 68/1980 | C: 1/2047\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58989 | C: 0.79854\n",
      "[LOGITS Ex2 A] Mean Abs: 0.269 | Max: 1.416\n",
      "[LOSS Ex2] A: 0.61180 | B: 0.60094 | C: 0.58731\n",
      "** [JOINT LOSS] ** : 1.062827\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002109 | Grad Max: 0.035782\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.006893 | Grad Max: 0.027674\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012147 | Grad Max: 0.047202\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.034807 | Grad Max: 0.034807\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000072 | Grad Max: 0.002475\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000589 | Grad Max: 0.009971\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000016 | Grad Max: 0.000560\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000181 | Grad Max: 0.001185\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000220\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000090 | Grad Max: 0.000644\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000203\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000081 | Grad Max: 0.000672\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001517 | Grad Max: 0.004214\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003455 | Grad Max: 0.003455\n",
      "[GRADIENT NORM TOTAL] 0.3370\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53406453 0.46593553] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.531 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 58/1990 | C: 3/2045\n",
      "[LOSS Ex1] A: 0.79860 | B: 0.59033 | C: 0.58379\n",
      "[LOGITS Ex2 A] Mean Abs: 0.287 | Max: 1.556\n",
      "[LOSS Ex2] A: 0.60184 | B: 0.60066 | C: 0.59439\n",
      "** [JOINT LOSS] ** : 1.256537\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002374 | Grad Max: 0.045139\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.016180 | Grad Max: 0.056330\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013016 | Grad Max: 0.050449\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.112674 | Grad Max: 0.112674\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000182 | Grad Max: 0.004107\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002219 | Grad Max: 0.015537\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000059 | Grad Max: 0.001624\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001288 | Grad Max: 0.005245\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000459\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000658 | Grad Max: 0.002187\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000504\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000478 | Grad Max: 0.001883\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001705 | Grad Max: 0.006853\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021204 | Grad Max: 0.021204\n",
      "[GRADIENT NORM TOTAL] 0.4952\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52673185 0.47326812] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 76/1972 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58898 | C: 0.63056\n",
      "[LOGITS Ex2 A] Mean Abs: 0.336 | Max: 1.846\n",
      "[LOSS Ex2] A: 0.60279 | B: 0.58692 | C: 0.58473\n",
      "** [JOINT LOSS] ** : 0.997991\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002002 | Grad Max: 0.038927\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.026865 | Grad Max: 0.161656\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016839 | Grad Max: 0.075723\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.230213 | Grad Max: 0.230213\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000241 | Grad Max: 0.006067\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003977 | Grad Max: 0.025667\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000088 | Grad Max: 0.002170\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002156 | Grad Max: 0.008617\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000037 | Grad Max: 0.000699\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001059 | Grad Max: 0.003548\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000039 | Grad Max: 0.000664\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000785 | Grad Max: 0.002688\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002856 | Grad Max: 0.010119\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.039927 | Grad Max: 0.039927\n",
      "[GRADIENT NORM TOTAL] 0.7484\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52874655 0.47125348] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 61/1795 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58983 | C: 0.65734\n",
      "[LOGITS Ex2 A] Mean Abs: 0.333 | Max: 2.062\n",
      "[LOSS Ex2] A: 0.60074 | B: 0.58998 | C: 0.57595\n",
      "** [JOINT LOSS] ** : 1.004612\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001764 | Grad Max: 0.029208\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010303 | Grad Max: 0.042197\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013466 | Grad Max: 0.052365\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.185641 | Grad Max: 0.185641\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000091 | Grad Max: 0.003840\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.000779 | Grad Max: 0.010570\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000020 | Grad Max: 0.000902\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000253 | Grad Max: 0.002012\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000197\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000111 | Grad Max: 0.000784\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000230\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000077 | Grad Max: 0.000606\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001085 | Grad Max: 0.003232\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.001177 | Grad Max: 0.001177\n",
      "[GRADIENT NORM TOTAL] 0.4159\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5327482  0.46725178] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 64/1984 | C: 2/2046\n",
      "[LOSS Ex1] A: 0.79897 | B: 0.59014 | C: 0.69245\n",
      "[LOGITS Ex2 A] Mean Abs: 0.353 | Max: 1.895\n",
      "[LOSS Ex2] A: 0.58213 | B: 0.58682 | C: 0.59090\n",
      "** [JOINT LOSS] ** : 1.280473\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002023 | Grad Max: 0.031248\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.025701 | Grad Max: 0.126988\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.007917 | Grad Max: 0.024464\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.052758 | Grad Max: 0.052758\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000310 | Grad Max: 0.005944\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004424 | Grad Max: 0.026565\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000104 | Grad Max: 0.002752\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002433 | Grad Max: 0.009282\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000043 | Grad Max: 0.000787\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001147 | Grad Max: 0.003472\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000044 | Grad Max: 0.000781\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000781 | Grad Max: 0.002786\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003043 | Grad Max: 0.008507\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.035456 | Grad Max: 0.035456\n",
      "[GRADIENT NORM TOTAL] 0.6571\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.043\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5217575 0.4782425] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 58/1990 | C: 2/2046\n",
      "[LOSS Ex1] A: 0.79908 | B: 0.59061 | C: 0.69203\n",
      "[LOGITS Ex2 A] Mean Abs: 0.361 | Max: 2.333\n",
      "[LOSS Ex2] A: 0.58751 | B: 0.58834 | C: 0.58183\n",
      "** [JOINT LOSS] ** : 1.279801\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001996 | Grad Max: 0.041085\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.034461 | Grad Max: 0.210167\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008319 | Grad Max: 0.026996\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.052591 | Grad Max: 0.052591\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000305 | Grad Max: 0.008028\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005107 | Grad Max: 0.037798\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000105 | Grad Max: 0.002501\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002627 | Grad Max: 0.010219\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000042 | Grad Max: 0.000709\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001199 | Grad Max: 0.003485\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000043 | Grad Max: 0.000677\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000819 | Grad Max: 0.002641\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003165 | Grad Max: 0.009877\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.039020 | Grad Max: 0.039020\n",
      "[GRADIENT NORM TOTAL] 0.7575\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5330146 0.4669854] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 75/1973 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58883 | C: 0.66013\n",
      "[LOGITS Ex2 A] Mean Abs: 0.363 | Max: 2.307\n",
      "[LOSS Ex2] A: 0.59206 | B: 0.58446 | C: 0.56631\n",
      "** [JOINT LOSS] ** : 0.997263\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001921 | Grad Max: 0.026939\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011151 | Grad Max: 0.042179\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013956 | Grad Max: 0.057315\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.186013 | Grad Max: 0.186013\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000115 | Grad Max: 0.003838\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001094 | Grad Max: 0.014575\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000024 | Grad Max: 0.000740\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000408 | Grad Max: 0.002262\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000180 | Grad Max: 0.001007\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000266\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000116 | Grad Max: 0.000642\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000719 | Grad Max: 0.003225\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.004054 | Grad Max: 0.004054\n",
      "[GRADIENT NORM TOTAL] 0.4338\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53374404 0.46625596] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 61/1795 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58954 | C: 0.65752\n",
      "[LOGITS Ex2 A] Mean Abs: 0.387 | Max: 2.170\n",
      "[LOSS Ex2] A: 0.58149 | B: 0.57573 | C: 0.56903\n",
      "** [JOINT LOSS] ** : 0.991103\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001787 | Grad Max: 0.029880\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015923 | Grad Max: 0.064127\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013549 | Grad Max: 0.052373\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.185690 | Grad Max: 0.185690\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000152 | Grad Max: 0.003617\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002049 | Grad Max: 0.016227\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000044 | Grad Max: 0.001478\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001013 | Grad Max: 0.004704\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000017 | Grad Max: 0.000380\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000447 | Grad Max: 0.001574\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000436\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000287 | Grad Max: 0.001209\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001200 | Grad Max: 0.004258\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.012668 | Grad Max: 0.012668\n",
      "[GRADIENT NORM TOTAL] 0.4801\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.031\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53351986 0.46648014] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 64/1984 | C: 1/2047\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58968 | C: 0.58558\n",
      "[LOGITS Ex2 A] Mean Abs: 0.445 | Max: 3.003\n",
      "[LOSS Ex2] A: 0.57626 | B: 0.56715 | C: 0.55852\n",
      "** [JOINT LOSS] ** : 0.959062\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002266 | Grad Max: 0.040968\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015714 | Grad Max: 0.076174\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.022306 | Grad Max: 0.091468\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.296229 | Grad Max: 0.296229\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000120 | Grad Max: 0.005041\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001293 | Grad Max: 0.018628\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000028 | Grad Max: 0.001252\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000489 | Grad Max: 0.003128\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000319\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000190 | Grad Max: 0.001021\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000318\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000129 | Grad Max: 0.000912\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000744 | Grad Max: 0.003715\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.006443 | Grad Max: 0.006443\n",
      "[GRADIENT NORM TOTAL] 0.6445\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.056\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53366023 0.46633977] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 59/1989 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58998 | C: 0.80199\n",
      "[LOGITS Ex2 A] Mean Abs: 0.439 | Max: 2.599\n",
      "[LOSS Ex2] A: 0.58304 | B: 0.57529 | C: 0.57075\n",
      "** [JOINT LOSS] ** : 1.040347\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002499 | Grad Max: 0.052757\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.023575 | Grad Max: 0.152643\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011783 | Grad Max: 0.047023\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.035306 | Grad Max: 0.035306\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000238 | Grad Max: 0.005926\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003291 | Grad Max: 0.031604\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000070 | Grad Max: 0.002072\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001595 | Grad Max: 0.006072\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000549\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000679 | Grad Max: 0.002135\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000474\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000429 | Grad Max: 0.001506\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002002 | Grad Max: 0.005462\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.019917 | Grad Max: 0.019917\n",
      "[GRADIENT NORM TOTAL] 0.5822\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.533461   0.46653903] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 76/1972 | C: 1/2047\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58792 | C: 0.58548\n",
      "[LOGITS Ex2 A] Mean Abs: 0.467 | Max: 3.125\n",
      "[LOSS Ex2] A: 0.56823 | B: 0.58335 | C: 0.56710\n",
      "** [JOINT LOSS] ** : 0.964030\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003458 | Grad Max: 0.064933\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.051620 | Grad Max: 0.241989\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.023727 | Grad Max: 0.119564\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.295886 | Grad Max: 0.295886\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000467 | Grad Max: 0.009104\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007565 | Grad Max: 0.043541\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000152 | Grad Max: 0.003301\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003806 | Grad Max: 0.012402\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000059 | Grad Max: 0.000983\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001596 | Grad Max: 0.004531\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000057 | Grad Max: 0.000996\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000985 | Grad Max: 0.003423\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004069 | Grad Max: 0.009619\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.042855 | Grad Max: 0.042855\n",
      "[GRADIENT NORM TOTAL] 1.2227\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5253899 0.4746101] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 63/1793 | C: 1/2047\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58850 | C: 0.59395\n",
      "[LOGITS Ex2 A] Mean Abs: 0.494 | Max: 2.760\n",
      "[LOSS Ex2] A: 0.56863 | B: 0.57511 | C: 0.55886\n",
      "** [JOINT LOSS] ** : 0.961684\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003032 | Grad Max: 0.063169\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.020102 | Grad Max: 0.108816\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.021750 | Grad Max: 0.121543\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.297559 | Grad Max: 0.297559\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000208 | Grad Max: 0.006211\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002477 | Grad Max: 0.024022\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000058 | Grad Max: 0.001776\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001233 | Grad Max: 0.005201\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000021 | Grad Max: 0.000496\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000511 | Grad Max: 0.002028\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000413\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000316 | Grad Max: 0.001206\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001615 | Grad Max: 0.004876\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014473 | Grad Max: 0.014473\n",
      "[GRADIENT NORM TOTAL] 0.7716\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52826655 0.47173345] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 68/1980 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58863 | C: 0.67058\n",
      "[LOGITS Ex2 A] Mean Abs: 0.493 | Max: 2.787\n",
      "[LOSS Ex2] A: 0.58262 | B: 0.55461 | C: 0.54452\n",
      "** [JOINT LOSS] ** : 0.980320\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002589 | Grad Max: 0.069196\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.027681 | Grad Max: 0.182550\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011999 | Grad Max: 0.044950\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.163535 | Grad Max: 0.163535\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000325 | Grad Max: 0.007952\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004228 | Grad Max: 0.029417\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000096 | Grad Max: 0.002525\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002134 | Grad Max: 0.007386\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000036 | Grad Max: 0.000614\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000877 | Grad Max: 0.002539\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000035 | Grad Max: 0.000626\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000533 | Grad Max: 0.001876\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002655 | Grad Max: 0.006651\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023733 | Grad Max: 0.023733\n",
      "[GRADIENT NORM TOTAL] 0.7367\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5321175 0.4678825] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 68/1980 | C: 2/1374\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58938 | C: 0.69219\n",
      "[LOGITS Ex2 A] Mean Abs: 0.480 | Max: 2.645\n",
      "[LOSS Ex2] A: 0.55380 | B: 0.58095 | C: 0.56806\n",
      "** [JOINT LOSS] ** : 0.994793\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002176 | Grad Max: 0.036721\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.045913 | Grad Max: 0.255995\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010157 | Grad Max: 0.036930\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.130490 | Grad Max: 0.130490\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000451 | Grad Max: 0.008611\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007290 | Grad Max: 0.043702\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000145 | Grad Max: 0.003134\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003593 | Grad Max: 0.012059\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000054 | Grad Max: 0.000970\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001450 | Grad Max: 0.004543\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000052 | Grad Max: 0.000822\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000867 | Grad Max: 0.002659\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003766 | Grad Max: 0.009902\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.037432 | Grad Max: 0.037432\n",
      "[GRADIENT NORM TOTAL] 1.0042\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0551\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0013 | Alpha: 0.5500\n",
      "!!! BEST MODEL SAVED !!! (Old: 1.1294 -> New: 1.0013)\n",
      "\n",
      "############################## EPOCH 4/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5214254  0.47857457] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 88/1960 | C: 2/2046\n",
      "[LOSS Ex1] A: 0.79906 | B: 0.58740 | C: 0.58144\n",
      "[LOGITS Ex2 A] Mean Abs: 0.457 | Max: 3.092\n",
      "[LOSS Ex2] A: 0.55598 | B: 0.56055 | C: 0.57210\n",
      "** [JOINT LOSS] ** : 1.218840\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002327 | Grad Max: 0.046029\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009670 | Grad Max: 0.069582\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014505 | Grad Max: 0.058540\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.111619 | Grad Max: 0.111619\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000113 | Grad Max: 0.006068\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001201 | Grad Max: 0.026128\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000022 | Grad Max: 0.001053\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000299 | Grad Max: 0.002131\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000263\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000099 | Grad Max: 0.000777\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000200\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000060 | Grad Max: 0.000421\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000312 | Grad Max: 0.001670\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.001220 | Grad Max: 0.001220\n",
      "[GRADIENT NORM TOTAL] 0.4052\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.051\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5333442 0.4666558] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 79/1777 | C: 1/2047\n",
      "[LOSS Ex1] A: 0.79877 | B: 0.58838 | C: 0.79922\n",
      "[LOGITS Ex2 A] Mean Abs: 0.456 | Max: 3.101\n",
      "[LOSS Ex2] A: 0.58068 | B: 0.56020 | C: 0.56147\n",
      "** [JOINT LOSS] ** : 1.296239\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002708 | Grad Max: 0.048083\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.051316 | Grad Max: 0.293557\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.020604 | Grad Max: 0.086310\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.218563 | Grad Max: 0.218563\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000441 | Grad Max: 0.011333\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007397 | Grad Max: 0.061737\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000142 | Grad Max: 0.003776\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003541 | Grad Max: 0.014066\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000052 | Grad Max: 0.000770\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001415 | Grad Max: 0.004089\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000049 | Grad Max: 0.000766\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000832 | Grad Max: 0.002569\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003677 | Grad Max: 0.008662\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.036540 | Grad Max: 0.036540\n",
      "[GRADIENT NORM TOTAL] 1.1366\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5340876  0.46591243] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 89/1959 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58901 | C: 0.69234\n",
      "[LOGITS Ex2 A] Mean Abs: 0.459 | Max: 2.779\n",
      "[LOSS Ex2] A: 0.56022 | B: 0.55139 | C: 0.56128\n",
      "** [JOINT LOSS] ** : 0.984748\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001720 | Grad Max: 0.034786\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.008809 | Grad Max: 0.034749\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009363 | Grad Max: 0.040203\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.130276 | Grad Max: 0.130276\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000133 | Grad Max: 0.005432\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001294 | Grad Max: 0.023946\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000025 | Grad Max: 0.001083\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000323 | Grad Max: 0.003236\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000282\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000103 | Grad Max: 0.000715\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000207\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000059 | Grad Max: 0.000524\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000295 | Grad Max: 0.001752\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.000180 | Grad Max: 0.000180\n",
      "[GRADIENT NORM TOTAL] 0.3365\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53395194 0.46604803] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 84/1964 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58914 | C: 0.71924\n",
      "[LOGITS Ex2 A] Mean Abs: 0.509 | Max: 2.904\n",
      "[LOSS Ex2] A: 0.54125 | B: 0.56366 | C: 0.55287\n",
      "** [JOINT LOSS] ** : 0.988716\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002309 | Grad Max: 0.061985\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.032869 | Grad Max: 0.198311\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006310 | Grad Max: 0.034764\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.089234 | Grad Max: 0.089234\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000388 | Grad Max: 0.008588\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005377 | Grad Max: 0.032020\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000110 | Grad Max: 0.002499\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002581 | Grad Max: 0.009163\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000040 | Grad Max: 0.000663\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001014 | Grad Max: 0.003018\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000038 | Grad Max: 0.000670\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000589 | Grad Max: 0.001993\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002718 | Grad Max: 0.006526\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024808 | Grad Max: 0.024808\n",
      "[GRADIENT NORM TOTAL] 0.7674\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.059\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5342395  0.46576056] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 102/1946 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58691 | C: 0.74164\n",
      "[LOGITS Ex2 A] Mean Abs: 0.467 | Max: 2.988\n",
      "[LOSS Ex2] A: 0.55456 | B: 0.55488 | C: 0.55093\n",
      "** [JOINT LOSS] ** : 0.996307\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001800 | Grad Max: 0.030250\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.017990 | Grad Max: 0.099075\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005741 | Grad Max: 0.036211\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.055095 | Grad Max: 0.055095\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000198 | Grad Max: 0.005274\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002764 | Grad Max: 0.025355\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000049 | Grad Max: 0.001553\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001166 | Grad Max: 0.005035\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000017 | Grad Max: 0.000418\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000441 | Grad Max: 0.001658\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000373\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000254 | Grad Max: 0.001133\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001095 | Grad Max: 0.003171\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010565 | Grad Max: 0.010565\n",
      "[GRADIENT NORM TOTAL] 0.4096\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.043\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53408116 0.46591887] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 83/1773 | C: 21/2027\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58698 | C: 0.75899\n",
      "[LOGITS Ex2 A] Mean Abs: 0.520 | Max: 3.243\n",
      "[LOSS Ex2] A: 0.56967 | B: 0.56267 | C: 0.53235\n",
      "** [JOINT LOSS] ** : 1.003555\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002963 | Grad Max: 0.073617\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.080190 | Grad Max: 0.506793\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003589 | Grad Max: 0.017928\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.027646 | Grad Max: 0.027646\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000806 | Grad Max: 0.016033\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.013161 | Grad Max: 0.084551\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000254 | Grad Max: 0.006001\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006253 | Grad Max: 0.021613\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000091 | Grad Max: 0.001396\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002429 | Grad Max: 0.006581\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000085 | Grad Max: 0.001388\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001405 | Grad Max: 0.004330\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.006453 | Grad Max: 0.013296\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.061191 | Grad Max: 0.061191\n",
      "[GRADIENT NORM TOTAL] 1.7491\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5258353  0.47416475] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 89/1959 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58722 | C: 0.75605\n",
      "[LOGITS Ex2 A] Mean Abs: 0.525 | Max: 3.097\n",
      "[LOSS Ex2] A: 0.55465 | B: 0.54212 | C: 0.53507\n",
      "** [JOINT LOSS] ** : 0.991702\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002262 | Grad Max: 0.063552\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.036189 | Grad Max: 0.238154\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005076 | Grad Max: 0.023592\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.030754 | Grad Max: 0.030754\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000379 | Grad Max: 0.009459\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005815 | Grad Max: 0.049690\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000113 | Grad Max: 0.002800\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002716 | Grad Max: 0.009882\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000040 | Grad Max: 0.000691\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001070 | Grad Max: 0.003231\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000037 | Grad Max: 0.000676\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000620 | Grad Max: 0.001950\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002893 | Grad Max: 0.006726\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.027348 | Grad Max: 0.027348\n",
      "[GRADIENT NORM TOTAL] 0.8039\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52884644 0.47115353] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 86/1962 | C: 21/2027\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58766 | C: 0.76889\n",
      "[LOGITS Ex2 A] Mean Abs: 0.538 | Max: 3.146\n",
      "[LOSS Ex2] A: 0.56014 | B: 0.57032 | C: 0.54362\n",
      "** [JOINT LOSS] ** : 1.010209\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002894 | Grad Max: 0.073943\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.093036 | Grad Max: 0.539558\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004168 | Grad Max: 0.017277\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.011955 | Grad Max: 0.011955\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000915 | Grad Max: 0.016712\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.015113 | Grad Max: 0.084246\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000285 | Grad Max: 0.005204\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.007123 | Grad Max: 0.022018\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000102 | Grad Max: 0.001496\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002741 | Grad Max: 0.007398\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000095 | Grad Max: 0.001503\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001582 | Grad Max: 0.004612\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.007212 | Grad Max: 0.017388\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.068119 | Grad Max: 0.068119\n",
      "[GRADIENT NORM TOTAL] 1.9670\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.043\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53251404 0.46748596] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 102/1946 | C: 24/2024\n",
      "[LOSS Ex1] A: 0.79971 | B: 0.58530 | C: 0.79062\n",
      "[LOGITS Ex2 A] Mean Abs: 0.547 | Max: 3.160\n",
      "[LOSS Ex2] A: 0.54584 | B: 0.57730 | C: 0.57373\n",
      "** [JOINT LOSS] ** : 1.290830\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003356 | Grad Max: 0.079495\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.104916 | Grad Max: 0.591545\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016549 | Grad Max: 0.062805\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.205558 | Grad Max: 0.205558\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001017 | Grad Max: 0.017021\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.016754 | Grad Max: 0.091555\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000315 | Grad Max: 0.006178\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.007886 | Grad Max: 0.024893\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000112 | Grad Max: 0.001650\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.003030 | Grad Max: 0.008263\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000104 | Grad Max: 0.001625\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001742 | Grad Max: 0.005541\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.007844 | Grad Max: 0.020149\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.074877 | Grad Max: 0.074877\n",
      "[GRADIENT NORM TOTAL] 2.2216\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.044\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52194536 0.4780546 ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.531 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 83/1773 | C: 25/2023\n",
      "[LOSS Ex1] A: 0.80030 | B: 0.58551 | C: 0.78368\n",
      "[LOGITS Ex2 A] Mean Abs: 0.511 | Max: 3.302\n",
      "[LOSS Ex2] A: 0.53736 | B: 0.55627 | C: 0.53880\n",
      "** [JOINT LOSS] ** : 1.267305\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001919 | Grad Max: 0.044626\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.020050 | Grad Max: 0.116962\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015682 | Grad Max: 0.057955\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.192989 | Grad Max: 0.192989\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000164 | Grad Max: 0.004532\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002247 | Grad Max: 0.022239\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000044 | Grad Max: 0.001814\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000977 | Grad Max: 0.004554\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000015 | Grad Max: 0.000354\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000372 | Grad Max: 0.001528\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000341\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000215 | Grad Max: 0.000857\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001069 | Grad Max: 0.003634\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009581 | Grad Max: 0.009581\n",
      "[GRADIENT NORM TOTAL] 0.5471\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.054\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53383476 0.46616527] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 91/1957 | C: 25/2023\n",
      "[LOSS Ex1] A: 0.79973 | B: 0.58623 | C: 0.79160\n",
      "[LOGITS Ex2 A] Mean Abs: 0.517 | Max: 3.367\n",
      "[LOSS Ex2] A: 0.56877 | B: 0.54546 | C: 0.52259\n",
      "** [JOINT LOSS] ** : 1.271459\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003095 | Grad Max: 0.080641\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.070928 | Grad Max: 0.443962\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017062 | Grad Max: 0.064429\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.205838 | Grad Max: 0.205838\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000675 | Grad Max: 0.013685\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010852 | Grad Max: 0.070407\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000204 | Grad Max: 0.003888\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005024 | Grad Max: 0.015994\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000071 | Grad Max: 0.001017\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001914 | Grad Max: 0.005271\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000066 | Grad Max: 0.000965\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001099 | Grad Max: 0.003392\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005097 | Grad Max: 0.011816\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.047911 | Grad Max: 0.047911\n",
      "[GRADIENT NORM TOTAL] 1.5402\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5344334  0.46556658] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 86/1962 | C: 20/2028\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58668 | C: 0.79019\n",
      "[LOGITS Ex2 A] Mean Abs: 0.493 | Max: 3.457\n",
      "[LOSS Ex2] A: 0.56047 | B: 0.55563 | C: 0.55499\n",
      "** [JOINT LOSS] ** : 1.015986\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002107 | Grad Max: 0.060826\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.035096 | Grad Max: 0.228518\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004250 | Grad Max: 0.025136\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.019087 | Grad Max: 0.019087\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000347 | Grad Max: 0.008096\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005440 | Grad Max: 0.039351\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000101 | Grad Max: 0.003098\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002494 | Grad Max: 0.009311\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000645\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000955 | Grad Max: 0.002821\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000032 | Grad Max: 0.000603\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000549 | Grad Max: 0.001737\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002567 | Grad Max: 0.006581\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024164 | Grad Max: 0.024164\n",
      "[GRADIENT NORM TOTAL] 0.7604\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53422344 0.46577653] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 104/1944 | C: 27/2021\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58477 | C: 0.78368\n",
      "[LOGITS Ex2 A] Mean Abs: 0.569 | Max: 3.060\n",
      "[LOSS Ex2] A: 0.53394 | B: 0.56185 | C: 0.53639\n",
      "** [JOINT LOSS] ** : 1.000214\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003013 | Grad Max: 0.067370\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.083878 | Grad Max: 0.488740\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005186 | Grad Max: 0.024956\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.011438 | Grad Max: 0.011438\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000834 | Grad Max: 0.015029\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.013383 | Grad Max: 0.080302\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000249 | Grad Max: 0.004621\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006199 | Grad Max: 0.020035\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000086 | Grad Max: 0.001286\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002354 | Grad Max: 0.006551\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000079 | Grad Max: 0.001288\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001345 | Grad Max: 0.004122\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.006061 | Grad Max: 0.013701\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.058054 | Grad Max: 0.058054\n",
      "[GRADIENT NORM TOTAL] 1.7476\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.061\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.534485   0.46551505] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 84/1772 | C: 15/1361\n",
      "[LOSS Ex1] A: 0.80005 | B: 0.58504 | C: 0.77149\n",
      "[LOGITS Ex2 A] Mean Abs: 0.535 | Max: 2.954\n",
      "[LOSS Ex2] A: 0.54764 | B: 0.55600 | C: 0.54759\n",
      "** [JOINT LOSS] ** : 1.269272\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003280 | Grad Max: 0.061576\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.080267 | Grad Max: 0.458244\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013927 | Grad Max: 0.076562\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.175450 | Grad Max: 0.175450\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000771 | Grad Max: 0.013841\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.012470 | Grad Max: 0.072630\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000231 | Grad Max: 0.004629\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005781 | Grad Max: 0.018108\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000079 | Grad Max: 0.001121\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002191 | Grad Max: 0.005931\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000073 | Grad Max: 0.001142\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001253 | Grad Max: 0.003949\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005583 | Grad Max: 0.013942\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.054123 | Grad Max: 0.054123\n",
      "[GRADIENT NORM TOTAL] 1.6772\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.1147\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.1028 | Alpha: 0.5500\n",
      "No improve count: 1/15\n",
      "\n",
      "############################## EPOCH 5/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.044\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5342132  0.46578676] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 91/1957 | C: 13/2035\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58583 | C: 0.80069\n",
      "[LOGITS Ex2 A] Mean Abs: 0.514 | Max: 3.113\n",
      "[LOSS Ex2] A: 0.54636 | B: 0.54391 | C: 0.51180\n",
      "** [JOINT LOSS] ** : 0.996195\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001501 | Grad Max: 0.021439\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.022338 | Grad Max: 0.131727\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005325 | Grad Max: 0.030139\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.035884 | Grad Max: 0.035884\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000189 | Grad Max: 0.007041\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003108 | Grad Max: 0.032038\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000052 | Grad Max: 0.001591\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001284 | Grad Max: 0.005329\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000017 | Grad Max: 0.000386\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000482 | Grad Max: 0.001754\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000352\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000270 | Grad Max: 0.001040\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001080 | Grad Max: 0.003686\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011455 | Grad Max: 0.011455\n",
      "[GRADIENT NORM TOTAL] 0.4514\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5256315  0.47436845] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 86/1962 | C: 14/2034\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58642 | C: 0.73871\n",
      "[LOGITS Ex2 A] Mean Abs: 0.563 | Max: 3.181\n",
      "[LOSS Ex2] A: 0.56635 | B: 0.55422 | C: 0.53334\n",
      "** [JOINT LOSS] ** : 0.993016\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003261 | Grad Max: 0.107386\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.092443 | Grad Max: 0.577173\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004868 | Grad Max: 0.028087\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.059052 | Grad Max: 0.059052\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000924 | Grad Max: 0.018004\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.014729 | Grad Max: 0.087958\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000276 | Grad Max: 0.005176\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006760 | Grad Max: 0.021163\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000093 | Grad Max: 0.001422\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002547 | Grad Max: 0.006968\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000086 | Grad Max: 0.001544\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001461 | Grad Max: 0.004421\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.006718 | Grad Max: 0.014958\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.064113 | Grad Max: 0.064113\n",
      "[GRADIENT NORM TOTAL] 1.9728\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5288379 0.4711621] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 104/1944 | C: 17/2031\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58458 | C: 0.75072\n",
      "[LOGITS Ex2 A] Mean Abs: 0.530 | Max: 3.444\n",
      "[LOSS Ex2] A: 0.56212 | B: 0.54207 | C: 0.52589\n",
      "** [JOINT LOSS] ** : 0.988464\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002817 | Grad Max: 0.094655\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.043853 | Grad Max: 0.282991\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003680 | Grad Max: 0.022854\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.042095 | Grad Max: 0.042095\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000485 | Grad Max: 0.010912\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006900 | Grad Max: 0.043862\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000137 | Grad Max: 0.003568\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003219 | Grad Max: 0.010264\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000046 | Grad Max: 0.000770\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001220 | Grad Max: 0.003564\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000043 | Grad Max: 0.000736\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000701 | Grad Max: 0.002196\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003349 | Grad Max: 0.007196\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.030727 | Grad Max: 0.030727\n",
      "[GRADIENT NORM TOTAL] 0.9945\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5321661  0.46783385] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 84/1772 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.79885 | B: 0.58490 | C: 0.75166\n",
      "[LOGITS Ex2 A] Mean Abs: 0.560 | Max: 2.961\n",
      "[LOSS Ex2] A: 0.53878 | B: 0.57317 | C: 0.53285\n",
      "** [JOINT LOSS] ** : 1.260067\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003368 | Grad Max: 0.068081\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.083845 | Grad Max: 0.460347\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012358 | Grad Max: 0.041474\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.142816 | Grad Max: 0.142816\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000841 | Grad Max: 0.014345\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.013269 | Grad Max: 0.073991\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000246 | Grad Max: 0.005202\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006079 | Grad Max: 0.019460\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000083 | Grad Max: 0.001272\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002295 | Grad Max: 0.006312\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000076 | Grad Max: 0.001224\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001316 | Grad Max: 0.004044\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005715 | Grad Max: 0.013357\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.056347 | Grad Max: 0.056347\n",
      "[GRADIENT NORM TOTAL] 1.7438\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5214319  0.47856805] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 91/1957 | C: 20/2028\n",
      "[LOSS Ex1] A: 0.79921 | B: 0.58575 | C: 0.75643\n",
      "[LOGITS Ex2 A] Mean Abs: 0.550 | Max: 3.162\n",
      "[LOSS Ex2] A: 0.54077 | B: 0.56450 | C: 0.53880\n",
      "** [JOINT LOSS] ** : 1.261816\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.004106 | Grad Max: 0.121925\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.112896 | Grad Max: 0.652367\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012791 | Grad Max: 0.045768\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.152889 | Grad Max: 0.152889\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001147 | Grad Max: 0.019342\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.018033 | Grad Max: 0.101053\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000335 | Grad Max: 0.006523\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.008238 | Grad Max: 0.025944\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000112 | Grad Max: 0.001669\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.003096 | Grad Max: 0.008379\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000103 | Grad Max: 0.001604\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001779 | Grad Max: 0.005369\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.007870 | Grad Max: 0.017533\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.077104 | Grad Max: 0.077104\n",
      "[GRADIENT NORM TOTAL] 2.3687\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.054\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5336136 0.4663864] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.004\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 86/1962 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.79853 | B: 0.58635 | C: 0.75919\n",
      "[LOGITS Ex2 A] Mean Abs: 0.495 | Max: 3.013\n",
      "[LOSS Ex2] A: 0.54288 | B: 0.55178 | C: 0.52509\n",
      "** [JOINT LOSS] ** : 1.254605\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002527 | Grad Max: 0.054696\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.040034 | Grad Max: 0.209137\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012992 | Grad Max: 0.047648\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.156950 | Grad Max: 0.156950\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000450 | Grad Max: 0.008212\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006480 | Grad Max: 0.038425\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000127 | Grad Max: 0.002810\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003036 | Grad Max: 0.011664\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000042 | Grad Max: 0.000666\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001136 | Grad Max: 0.003280\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000039 | Grad Max: 0.000718\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000649 | Grad Max: 0.002289\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002894 | Grad Max: 0.006629\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.027748 | Grad Max: 0.027748\n",
      "[GRADIENT NORM TOTAL] 0.9215\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53408635 0.46591365] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 104/1944 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58454 | C: 0.72856\n",
      "[LOGITS Ex2 A] Mean Abs: 0.505 | Max: 3.714\n",
      "[LOSS Ex2] A: 0.56476 | B: 0.54049 | C: 0.53409\n",
      "** [JOINT LOSS] ** : 0.984144\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003644 | Grad Max: 0.108365\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.093985 | Grad Max: 0.578775\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005628 | Grad Max: 0.030345\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.074406 | Grad Max: 0.074406\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000913 | Grad Max: 0.018151\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.014670 | Grad Max: 0.088491\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000271 | Grad Max: 0.005887\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006708 | Grad Max: 0.021162\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000090 | Grad Max: 0.001447\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002504 | Grad Max: 0.007146\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000082 | Grad Max: 0.001343\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001446 | Grad Max: 0.004132\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.006482 | Grad Max: 0.014219\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.064254 | Grad Max: 0.064254\n",
      "[GRADIENT NORM TOTAL] 1.9580\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5338868  0.46611315] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 84/1772 | C: 7/2041\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58488 | C: 0.77082\n",
      "[LOGITS Ex2 A] Mean Abs: 0.587 | Max: 3.485\n",
      "[LOSS Ex2] A: 0.57161 | B: 0.56751 | C: 0.52755\n",
      "** [JOINT LOSS] ** : 1.007455\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.004275 | Grad Max: 0.161688\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.142574 | Grad Max: 0.860699\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003949 | Grad Max: 0.017986\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.011544 | Grad Max: 0.011544\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001361 | Grad Max: 0.025583\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.022226 | Grad Max: 0.129208\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000403 | Grad Max: 0.007875\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.010097 | Grad Max: 0.030520\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000133 | Grad Max: 0.001941\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.003760 | Grad Max: 0.010496\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000120 | Grad Max: 0.001986\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.002154 | Grad Max: 0.006589\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.009283 | Grad Max: 0.021047\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.094031 | Grad Max: 0.094031\n",
      "[GRADIENT NORM TOTAL] 2.9420\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.060\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53416216 0.4658378 ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 91/1957 | C: 6/2042\n",
      "[LOSS Ex1] A: 0.79907 | B: 0.58579 | C: 0.61779\n",
      "[LOGITS Ex2 A] Mean Abs: 0.522 | Max: 3.297\n",
      "[LOSS Ex2] A: 0.55445 | B: 0.53846 | C: 0.51573\n",
      "** [JOINT LOSS] ** : 1.203761\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003392 | Grad Max: 0.110468\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.088878 | Grad Max: 0.538996\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.007232 | Grad Max: 0.037329\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.056017 | Grad Max: 0.056017\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000873 | Grad Max: 0.017300\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.013897 | Grad Max: 0.083779\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000253 | Grad Max: 0.004935\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006312 | Grad Max: 0.019969\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000083 | Grad Max: 0.001258\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002358 | Grad Max: 0.006505\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000075 | Grad Max: 0.001089\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001355 | Grad Max: 0.004030\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005839 | Grad Max: 0.013787\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.059428 | Grad Max: 0.059428\n",
      "[GRADIENT NORM TOTAL] 1.8419\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.043\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5338712  0.46612883] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 86/1962 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58640 | C: 0.72788\n",
      "[LOGITS Ex2 A] Mean Abs: 0.511 | Max: 3.057\n",
      "[LOSS Ex2] A: 0.53431 | B: 0.55140 | C: 0.50096\n",
      "** [JOINT LOSS] ** : 0.966985\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002184 | Grad Max: 0.046267\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033337 | Grad Max: 0.191811\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008315 | Grad Max: 0.028207\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.074672 | Grad Max: 0.074672\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000380 | Grad Max: 0.007181\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005416 | Grad Max: 0.035030\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000105 | Grad Max: 0.002321\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002494 | Grad Max: 0.008278\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000034 | Grad Max: 0.000563\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000934 | Grad Max: 0.002806\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000031 | Grad Max: 0.000578\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000533 | Grad Max: 0.001805\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002242 | Grad Max: 0.005983\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022250 | Grad Max: 0.022250\n",
      "[GRADIENT NORM TOTAL] 0.7463\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.031\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5248296 0.4751704] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 104/1944 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58451 | C: 0.65604\n",
      "[LOGITS Ex2 A] Mean Abs: 0.552 | Max: 3.121\n",
      "[LOSS Ex2] A: 0.53101 | B: 0.56197 | C: 0.54594\n",
      "** [JOINT LOSS] ** : 0.959821\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002913 | Grad Max: 0.077561\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.091285 | Grad Max: 0.542165\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013849 | Grad Max: 0.054478\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.184641 | Grad Max: 0.184641\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000865 | Grad Max: 0.014910\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.014052 | Grad Max: 0.081313\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000250 | Grad Max: 0.004917\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006327 | Grad Max: 0.020920\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000081 | Grad Max: 0.001168\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002356 | Grad Max: 0.006404\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000073 | Grad Max: 0.001131\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001357 | Grad Max: 0.004108\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005610 | Grad Max: 0.013346\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.058716 | Grad Max: 0.058716\n",
      "[GRADIENT NORM TOTAL] 1.8831\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5284631 0.4715369] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 84/1772 | C: 4/2044\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58474 | C: 0.69620\n",
      "[LOGITS Ex2 A] Mean Abs: 0.529 | Max: 3.285\n",
      "[LOSS Ex2] A: 0.54511 | B: 0.55431 | C: 0.52261\n",
      "** [JOINT LOSS] ** : 0.967657\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002622 | Grad Max: 0.041377\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.060207 | Grad Max: 0.340314\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009488 | Grad Max: 0.039963\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.129992 | Grad Max: 0.129992\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000600 | Grad Max: 0.012655\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009549 | Grad Max: 0.068425\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000172 | Grad Max: 0.003688\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004307 | Grad Max: 0.013455\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000056 | Grad Max: 0.000845\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001600 | Grad Max: 0.004519\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000050 | Grad Max: 0.000755\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000922 | Grad Max: 0.002664\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003810 | Grad Max: 0.008152\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.039765 | Grad Max: 0.039765\n",
      "[GRADIENT NORM TOTAL] 1.2524\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53157485 0.46842512] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 91/1957 | C: 2/2046\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58554 | C: 0.79894\n",
      "[LOGITS Ex2 A] Mean Abs: 0.521 | Max: 3.284\n",
      "[LOSS Ex2] A: 0.54731 | B: 0.53885 | C: 0.53006\n",
      "** [JOINT LOSS] ** : 1.000235\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002872 | Grad Max: 0.080549\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.062338 | Grad Max: 0.379050\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005590 | Grad Max: 0.032682\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.035678 | Grad Max: 0.035678\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000609 | Grad Max: 0.012072\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009620 | Grad Max: 0.058816\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000174 | Grad Max: 0.003605\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004329 | Grad Max: 0.014099\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000056 | Grad Max: 0.000965\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001621 | Grad Max: 0.005247\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000050 | Grad Max: 0.000857\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000935 | Grad Max: 0.002932\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003938 | Grad Max: 0.008805\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.041087 | Grad Max: 0.041087\n",
      "[GRADIENT NORM TOTAL] 1.2989\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5207561  0.47924387] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 86/1962 | C: 1/1375\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58607 | C: 0.79894\n",
      "[LOGITS Ex2 A] Mean Abs: 0.514 | Max: 3.196\n",
      "[LOSS Ex2] A: 0.53895 | B: 0.55168 | C: 0.51386\n",
      "** [JOINT LOSS] ** : 0.996499\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003032 | Grad Max: 0.091137\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.090980 | Grad Max: 0.552945\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005062 | Grad Max: 0.025049\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.035576 | Grad Max: 0.035576\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000850 | Grad Max: 0.017454\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.014021 | Grad Max: 0.092344\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000247 | Grad Max: 0.005154\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006234 | Grad Max: 0.020439\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000079 | Grad Max: 0.001160\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002324 | Grad Max: 0.006750\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000071 | Grad Max: 0.001169\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001340 | Grad Max: 0.004163\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005561 | Grad Max: 0.012421\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.059077 | Grad Max: 0.059077\n",
      "[GRADIENT NORM TOTAL] 1.8575\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0601\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0475 | Alpha: 0.5500\n",
      "No improve count: 2/15\n",
      "\n",
      "############################## EPOCH 6/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.054\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5334178  0.46658212] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 104/1944 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58412 | C: 0.65730\n",
      "[LOGITS Ex2 A] Mean Abs: 0.500 | Max: 3.152\n",
      "[LOSS Ex2] A: 0.53497 | B: 0.53267 | C: 0.50360\n",
      "** [JOINT LOSS] ** : 0.937555\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001836 | Grad Max: 0.032636\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009516 | Grad Max: 0.049633\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013893 | Grad Max: 0.053553\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.184326 | Grad Max: 0.184326\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000129 | Grad Max: 0.004389\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001232 | Grad Max: 0.026840\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000025 | Grad Max: 0.001376\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000340 | Grad Max: 0.003392\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000281\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000116 | Grad Max: 0.001038\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000176\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000068 | Grad Max: 0.000474\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000673 | Grad Max: 0.003406\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003091 | Grad Max: 0.003091\n",
      "[GRADIENT NORM TOTAL] 0.4348\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5338781 0.4661219] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 85/1771 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58443 | C: 0.65119\n",
      "[LOGITS Ex2 A] Mean Abs: 0.502 | Max: 3.087\n",
      "[LOSS Ex2] A: 0.54436 | B: 0.55008 | C: 0.51900\n",
      "** [JOINT LOSS] ** : 0.949684\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002130 | Grad Max: 0.039442\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.043965 | Grad Max: 0.245183\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014021 | Grad Max: 0.059150\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.196104 | Grad Max: 0.196104\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000413 | Grad Max: 0.008552\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006505 | Grad Max: 0.049943\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000115 | Grad Max: 0.002822\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002877 | Grad Max: 0.009819\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000037 | Grad Max: 0.000618\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001060 | Grad Max: 0.003233\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000032 | Grad Max: 0.000627\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000604 | Grad Max: 0.002113\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002350 | Grad Max: 0.005531\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025434 | Grad Max: 0.025434\n",
      "[GRADIENT NORM TOTAL] 0.9369\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5338048  0.46619523] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 92/1956 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58510 | C: 0.71341\n",
      "[LOGITS Ex2 A] Mean Abs: 0.539 | Max: 3.092\n",
      "[LOSS Ex2] A: 0.52432 | B: 0.53873 | C: 0.50726\n",
      "** [JOINT LOSS] ** : 0.956271\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001657 | Grad Max: 0.026737\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.019717 | Grad Max: 0.098016\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008442 | Grad Max: 0.032613\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.095730 | Grad Max: 0.095730\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000207 | Grad Max: 0.004915\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003009 | Grad Max: 0.026924\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000053 | Grad Max: 0.001590\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001304 | Grad Max: 0.005624\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000017 | Grad Max: 0.000399\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000483 | Grad Max: 0.001888\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000263\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000277 | Grad Max: 0.001035\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001086 | Grad Max: 0.003443\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011708 | Grad Max: 0.011708\n",
      "[GRADIENT NORM TOTAL] 0.4518\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.061\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53423715 0.46576282] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 86/1962 | C: 4/2044\n",
      "[LOSS Ex1] A: 0.79961 | B: 0.58531 | C: 0.64084\n",
      "[LOGITS Ex2 A] Mean Abs: 0.502 | Max: 2.968\n",
      "[LOSS Ex2] A: 0.53742 | B: 0.53687 | C: 0.49366\n",
      "** [JOINT LOSS] ** : 1.197900\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001842 | Grad Max: 0.036289\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.026039 | Grad Max: 0.168469\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.007453 | Grad Max: 0.033828\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.029508 | Grad Max: 0.029508\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000245 | Grad Max: 0.006376\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003875 | Grad Max: 0.034191\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000065 | Grad Max: 0.001819\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001601 | Grad Max: 0.006884\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000020 | Grad Max: 0.000475\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000586 | Grad Max: 0.001993\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000311\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000336 | Grad Max: 0.001134\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001306 | Grad Max: 0.004813\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014654 | Grad Max: 0.014654\n",
      "[GRADIENT NORM TOTAL] 0.5537\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.044\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5340314  0.46596864] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 105/1943 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58329 | C: 0.71385\n",
      "[LOGITS Ex2 A] Mean Abs: 0.516 | Max: 2.964\n",
      "[LOSS Ex2] A: 0.53072 | B: 0.53391 | C: 0.49981\n",
      "** [JOINT LOSS] ** : 0.953858\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001580 | Grad Max: 0.022563\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.019582 | Grad Max: 0.122537\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006881 | Grad Max: 0.044394\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.096386 | Grad Max: 0.096386\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000170 | Grad Max: 0.004631\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002791 | Grad Max: 0.025977\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000046 | Grad Max: 0.001624\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001152 | Grad Max: 0.005347\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000275\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000426 | Grad Max: 0.001480\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000012 | Grad Max: 0.000327\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000252 | Grad Max: 0.001068\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001062 | Grad Max: 0.004442\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011866 | Grad Max: 0.011866\n",
      "[GRADIENT NORM TOTAL] 0.4340\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52467054 0.47532943] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 85/1771 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58331 | C: 0.72808\n",
      "[LOGITS Ex2 A] Mean Abs: 0.535 | Max: 3.160\n",
      "[LOSS Ex2] A: 0.52677 | B: 0.54318 | C: 0.49831\n",
      "** [JOINT LOSS] ** : 0.959883\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001381 | Grad Max: 0.020898\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009225 | Grad Max: 0.035513\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005601 | Grad Max: 0.030923\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.074398 | Grad Max: 0.074398\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000109 | Grad Max: 0.004640\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001336 | Grad Max: 0.026950\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000023 | Grad Max: 0.001054\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000447 | Grad Max: 0.002675\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000253\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000149 | Grad Max: 0.001000\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000197\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000080 | Grad Max: 0.000544\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000565 | Grad Max: 0.002714\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.002014 | Grad Max: 0.002014\n",
      "[GRADIENT NORM TOTAL] 0.2526\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5288193 0.4711807] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 92/1956 | C: 12/2036\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58393 | C: 0.78196\n",
      "[LOGITS Ex2 A] Mean Abs: 0.526 | Max: 3.090\n",
      "[LOSS Ex2] A: 0.53729 | B: 0.54394 | C: 0.51135\n",
      "** [JOINT LOSS] ** : 0.986161\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001481 | Grad Max: 0.020034\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.026035 | Grad Max: 0.146631\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004042 | Grad Max: 0.021720\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.008513 | Grad Max: 0.008513\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000240 | Grad Max: 0.006500\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003814 | Grad Max: 0.033885\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.001709\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001598 | Grad Max: 0.006755\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000019 | Grad Max: 0.000346\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000578 | Grad Max: 0.001848\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000380\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000331 | Grad Max: 0.001348\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001260 | Grad Max: 0.003507\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014072 | Grad Max: 0.014072\n",
      "[GRADIENT NORM TOTAL] 0.5068\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53172725 0.46827278] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 89/1959 | C: 10/2038\n",
      "[LOSS Ex1] A: 0.79858 | B: 0.58461 | C: 0.75610\n",
      "[LOGITS Ex2 A] Mean Abs: 0.525 | Max: 3.049\n",
      "[LOSS Ex2] A: 0.52158 | B: 0.54050 | C: 0.52058\n",
      "** [JOINT LOSS] ** : 1.240646\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001705 | Grad Max: 0.039888\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.022681 | Grad Max: 0.115113\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013231 | Grad Max: 0.046592\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.153095 | Grad Max: 0.153095\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000177 | Grad Max: 0.005361\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002768 | Grad Max: 0.028554\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000048 | Grad Max: 0.001380\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001172 | Grad Max: 0.005088\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000015 | Grad Max: 0.000356\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000430 | Grad Max: 0.001662\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000013 | Grad Max: 0.000372\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000252 | Grad Max: 0.001002\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001118 | Grad Max: 0.003824\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.012006 | Grad Max: 0.012006\n",
      "[GRADIENT NORM TOTAL] 0.5225\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5209515  0.47904852] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 106/1942 | C: 11/2037\n",
      "[LOSS Ex1] A: 0.79917 | B: 0.58231 | C: 0.76116\n",
      "[LOGITS Ex2 A] Mean Abs: 0.515 | Max: 3.260\n",
      "[LOSS Ex2] A: 0.51607 | B: 0.53005 | C: 0.49679\n",
      "** [JOINT LOSS] ** : 1.228515\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001666 | Grad Max: 0.039608\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.011678 | Grad Max: 0.055056\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013327 | Grad Max: 0.048597\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.159612 | Grad Max: 0.159612\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000112 | Grad Max: 0.007884\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001222 | Grad Max: 0.046018\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000022 | Grad Max: 0.000850\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000341 | Grad Max: 0.002376\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000007 | Grad Max: 0.000204\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000105 | Grad Max: 0.000864\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000182\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000060 | Grad Max: 0.000459\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000733 | Grad Max: 0.002278\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.000298 | Grad Max: 0.000298\n",
      "[GRADIENT NORM TOTAL] 0.4047\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.056\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53381854 0.46618152] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 87/1769 | C: 15/2033\n",
      "[LOSS Ex1] A: 0.79857 | B: 0.58268 | C: 0.78606\n",
      "[LOGITS Ex2 A] Mean Abs: 0.513 | Max: 3.278\n",
      "[LOSS Ex2] A: 0.51651 | B: 0.54284 | C: 0.49946\n",
      "** [JOINT LOSS] ** : 1.242039\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001796 | Grad Max: 0.046466\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.024805 | Grad Max: 0.109289\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016152 | Grad Max: 0.060565\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.197502 | Grad Max: 0.197502\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000216 | Grad Max: 0.007709\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003138 | Grad Max: 0.044828\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000054 | Grad Max: 0.001515\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001319 | Grad Max: 0.005956\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000367\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000469 | Grad Max: 0.001754\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000317\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000263 | Grad Max: 0.000924\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001020 | Grad Max: 0.003908\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011092 | Grad Max: 0.011092\n",
      "[GRADIENT NORM TOTAL] 0.6047\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5341852  0.46581486] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 94/1954 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58354 | C: 0.77887\n",
      "[LOGITS Ex2 A] Mean Abs: 0.504 | Max: 3.310\n",
      "[LOSS Ex2] A: 0.53227 | B: 0.52562 | C: 0.49230\n",
      "** [JOINT LOSS] ** : 0.970865\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001869 | Grad Max: 0.043292\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.021994 | Grad Max: 0.130756\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003849 | Grad Max: 0.020126\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.006160 | Grad Max: 0.006160\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000219 | Grad Max: 0.007838\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003408 | Grad Max: 0.032704\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000059 | Grad Max: 0.001676\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001457 | Grad Max: 0.005847\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000018 | Grad Max: 0.000402\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000537 | Grad Max: 0.001884\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000388\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000308 | Grad Max: 0.001253\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001276 | Grad Max: 0.004412\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014371 | Grad Max: 0.014371\n",
      "[GRADIENT NORM TOTAL] 0.4777\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53404063 0.46595937] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 90/1958 | C: 13/2035\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58429 | C: 0.73454\n",
      "[LOGITS Ex2 A] Mean Abs: 0.560 | Max: 3.338\n",
      "[LOSS Ex2] A: 0.51197 | B: 0.53323 | C: 0.48621\n",
      "** [JOINT LOSS] ** : 0.950081\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001581 | Grad Max: 0.034723\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.031791 | Grad Max: 0.194626\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005020 | Grad Max: 0.026215\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.066055 | Grad Max: 0.066055\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000289 | Grad Max: 0.008574\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004756 | Grad Max: 0.053458\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000078 | Grad Max: 0.001913\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001971 | Grad Max: 0.007557\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000023 | Grad Max: 0.000473\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000709 | Grad Max: 0.002406\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000392\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000402 | Grad Max: 0.001322\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001572 | Grad Max: 0.005409\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017843 | Grad Max: 0.017843\n",
      "[GRADIENT NORM TOTAL] 0.6528\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.062\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.534418   0.46558207] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 106/1942 | C: 10/2038\n",
      "[LOSS Ex1] A: 0.80059 | B: 0.58194 | C: 0.71438\n",
      "[LOGITS Ex2 A] Mean Abs: 0.547 | Max: 3.281\n",
      "[LOSS Ex2] A: 0.51988 | B: 0.53134 | C: 0.50941\n",
      "** [JOINT LOSS] ** : 1.219177\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002134 | Grad Max: 0.032202\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.042733 | Grad Max: 0.238938\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008688 | Grad Max: 0.048955\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.087624 | Grad Max: 0.087624\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000411 | Grad Max: 0.007696\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006492 | Grad Max: 0.040063\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000110 | Grad Max: 0.002507\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002778 | Grad Max: 0.009720\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000033 | Grad Max: 0.000557\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000994 | Grad Max: 0.002815\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000028 | Grad Max: 0.000528\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000554 | Grad Max: 0.001967\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001975 | Grad Max: 0.005692\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022568 | Grad Max: 0.022568\n",
      "[GRADIENT NORM TOTAL] 0.8599\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.045\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.534116   0.46588397] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 87/1769 | C: 3/1373\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58240 | C: 0.80029\n",
      "[LOGITS Ex2 A] Mean Abs: 0.555 | Max: 3.372\n",
      "[LOSS Ex2] A: 0.50998 | B: 0.54255 | C: 0.53361\n",
      "** [JOINT LOSS] ** : 0.989610\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002059 | Grad Max: 0.039805\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.016558 | Grad Max: 0.088164\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006864 | Grad Max: 0.034391\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.036465 | Grad Max: 0.036465\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000226 | Grad Max: 0.007580\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002710 | Grad Max: 0.026669\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000053 | Grad Max: 0.001884\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001144 | Grad Max: 0.005779\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000370\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000397 | Grad Max: 0.001699\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000013 | Grad Max: 0.000303\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000223 | Grad Max: 0.000864\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000922 | Grad Max: 0.002926\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008820 | Grad Max: 0.008820\n",
      "[GRADIENT NORM TOTAL] 0.4318\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0559\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0529 | Alpha: 0.5500\n",
      "No improve count: 3/15\n",
      "\n",
      "############################## EPOCH 7/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.031\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52426344 0.47573656] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 94/1954 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58339 | C: 0.66049\n",
      "[LOGITS Ex2 A] Mean Abs: 0.573 | Max: 3.296\n",
      "[LOSS Ex2] A: 0.52086 | B: 0.52558 | C: 0.50031\n",
      "** [JOINT LOSS] ** : 0.930209\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002499 | Grad Max: 0.057560\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.037404 | Grad Max: 0.210553\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013600 | Grad Max: 0.053790\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.184529 | Grad Max: 0.184529\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000368 | Grad Max: 0.008112\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005430 | Grad Max: 0.037099\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000097 | Grad Max: 0.002302\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002345 | Grad Max: 0.008590\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000029 | Grad Max: 0.000540\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000833 | Grad Max: 0.002685\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000024 | Grad Max: 0.000409\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000473 | Grad Max: 0.001635\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001957 | Grad Max: 0.005049\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021175 | Grad Max: 0.021175\n",
      "[GRADIENT NORM TOTAL] 0.8255\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52884877 0.47115123] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 89/1959 | C: 9/2039\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58395 | C: 0.63587\n",
      "[LOGITS Ex2 A] Mean Abs: 0.571 | Max: 3.468\n",
      "[LOSS Ex2] A: 0.52865 | B: 0.53070 | C: 0.51245\n",
      "** [JOINT LOSS] ** : 0.930541\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002408 | Grad Max: 0.048009\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.026172 | Grad Max: 0.139000\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015849 | Grad Max: 0.069065\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.221806 | Grad Max: 0.221806\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000252 | Grad Max: 0.007261\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003443 | Grad Max: 0.035271\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.002214\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001440 | Grad Max: 0.006235\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000018 | Grad Max: 0.000367\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000517 | Grad Max: 0.001818\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000329\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000292 | Grad Max: 0.001066\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001284 | Grad Max: 0.004041\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013480 | Grad Max: 0.013480\n",
      "[GRADIENT NORM TOTAL] 0.6733\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5313622  0.46863788] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 106/1942 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58159 | C: 0.76376\n",
      "[LOGITS Ex2 A] Mean Abs: 0.574 | Max: 3.339\n",
      "[LOSS Ex2] A: 0.51290 | B: 0.53472 | C: 0.48090\n",
      "** [JOINT LOSS] ** : 0.957956\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001605 | Grad Max: 0.026657\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.047100 | Grad Max: 0.266340\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003984 | Grad Max: 0.014739\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.018629 | Grad Max: 0.018629\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000431 | Grad Max: 0.008838\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007158 | Grad Max: 0.048739\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000117 | Grad Max: 0.002357\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003016 | Grad Max: 0.009456\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000034 | Grad Max: 0.000524\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001048 | Grad Max: 0.002938\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000028 | Grad Max: 0.000470\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000575 | Grad Max: 0.001804\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001954 | Grad Max: 0.005619\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023504 | Grad Max: 0.023504\n",
      "[GRADIENT NORM TOTAL] 0.9161\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5204509 0.4795491] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 87/1769 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58186 | C: 0.79880\n",
      "[LOGITS Ex2 A] Mean Abs: 0.573 | Max: 3.408\n",
      "[LOSS Ex2] A: 0.50753 | B: 0.54165 | C: 0.49670\n",
      "** [JOINT LOSS] ** : 0.975513\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001610 | Grad Max: 0.023327\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.010765 | Grad Max: 0.035075\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004706 | Grad Max: 0.025614\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.036345 | Grad Max: 0.036345\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000139 | Grad Max: 0.004166\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001627 | Grad Max: 0.020371\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000030 | Grad Max: 0.001195\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000610 | Grad Max: 0.004422\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000200 | Grad Max: 0.001033\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000195\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000106 | Grad Max: 0.000593\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000470 | Grad Max: 0.002301\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003630 | Grad Max: 0.003630\n",
      "[GRADIENT NORM TOTAL] 0.2664\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.056\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5337329  0.46626714] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 94/1954 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58270 | C: 0.66518\n",
      "[LOGITS Ex2 A] Mean Abs: 0.551 | Max: 3.654\n",
      "[LOSS Ex2] A: 0.51698 | B: 0.52201 | C: 0.49503\n",
      "** [JOINT LOSS] ** : 0.927299\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001973 | Grad Max: 0.035214\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033502 | Grad Max: 0.182230\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012180 | Grad Max: 0.053922\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.170640 | Grad Max: 0.170640\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000309 | Grad Max: 0.008612\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004865 | Grad Max: 0.041755\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000083 | Grad Max: 0.002131\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002060 | Grad Max: 0.008576\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000024 | Grad Max: 0.000476\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000727 | Grad Max: 0.002451\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000326\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000404 | Grad Max: 0.001385\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001549 | Grad Max: 0.004311\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018003 | Grad Max: 0.018003\n",
      "[GRADIENT NORM TOTAL] 0.7247\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.534139   0.46586105] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 91/1957 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58345 | C: 0.72724\n",
      "[LOGITS Ex2 A] Mean Abs: 0.553 | Max: 3.283\n",
      "[LOSS Ex2] A: 0.51347 | B: 0.52830 | C: 0.50176\n",
      "** [JOINT LOSS] ** : 0.951405\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001723 | Grad Max: 0.036281\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.028444 | Grad Max: 0.168157\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005885 | Grad Max: 0.026101\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.074053 | Grad Max: 0.074053\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000264 | Grad Max: 0.007171\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004193 | Grad Max: 0.034501\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000072 | Grad Max: 0.001939\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001827 | Grad Max: 0.006853\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000021 | Grad Max: 0.000422\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000639 | Grad Max: 0.002097\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000327\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000358 | Grad Max: 0.001349\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001439 | Grad Max: 0.004335\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016146 | Grad Max: 0.016146\n",
      "[GRADIENT NORM TOTAL] 0.5895\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.034\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5340767  0.46592334] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 106/1942 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58072 | C: 0.68197\n",
      "[LOGITS Ex2 A] Mean Abs: 0.617 | Max: 3.397\n",
      "[LOSS Ex2] A: 0.49364 | B: 0.53407 | C: 0.49343\n",
      "** [JOINT LOSS] ** : 0.927944\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002786 | Grad Max: 0.059028\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.058287 | Grad Max: 0.323596\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010988 | Grad Max: 0.050373\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.143760 | Grad Max: 0.143760\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000584 | Grad Max: 0.015452\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008989 | Grad Max: 0.089380\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000151 | Grad Max: 0.002956\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003768 | Grad Max: 0.011814\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000044 | Grad Max: 0.000611\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001307 | Grad Max: 0.003585\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000036 | Grad Max: 0.000644\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000716 | Grad Max: 0.002261\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002592 | Grad Max: 0.006206\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.029877 | Grad Max: 0.029877\n",
      "[GRADIENT NORM TOTAL] 1.1868\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.062\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53455365 0.46544632] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 88/1768 | C: 6/2042\n",
      "[LOSS Ex1] A: 0.80110 | B: 0.58107 | C: 0.79896\n",
      "[LOGITS Ex2 A] Mean Abs: 0.591 | Max: 3.396\n",
      "[LOSS Ex2] A: 0.50675 | B: 0.53947 | C: 0.48644\n",
      "** [JOINT LOSS] ** : 1.237928\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002493 | Grad Max: 0.054817\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033338 | Grad Max: 0.171187\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016842 | Grad Max: 0.090031\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.220238 | Grad Max: 0.220238\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000287 | Grad Max: 0.011637\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004322 | Grad Max: 0.071155\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000073 | Grad Max: 0.002061\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001817 | Grad Max: 0.007332\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000020 | Grad Max: 0.000478\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000616 | Grad Max: 0.002385\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000315\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000331 | Grad Max: 0.001180\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001110 | Grad Max: 0.003278\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013124 | Grad Max: 0.013124\n",
      "[GRADIENT NORM TOTAL] 0.7734\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.045\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5342933  0.46570674] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 96/1952 | C: 6/2042\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58211 | C: 0.68862\n",
      "[LOGITS Ex2 A] Mean Abs: 0.589 | Max: 3.286\n",
      "[LOSS Ex2] A: 0.50430 | B: 0.51972 | C: 0.49630\n",
      "** [JOINT LOSS] ** : 0.930350\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002653 | Grad Max: 0.059453\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.067895 | Grad Max: 0.400644\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009199 | Grad Max: 0.047140\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.128522 | Grad Max: 0.128522\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000617 | Grad Max: 0.015678\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010117 | Grad Max: 0.081528\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000170 | Grad Max: 0.003847\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004321 | Grad Max: 0.013927\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000048 | Grad Max: 0.000779\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001482 | Grad Max: 0.004454\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000039 | Grad Max: 0.000801\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000807 | Grad Max: 0.002671\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003039 | Grad Max: 0.007234\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.034842 | Grad Max: 0.034842\n",
      "[GRADIENT NORM TOTAL] 1.3465\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52413446 0.47586554] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 92/1956 | C: 7/2041\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58277 | C: 0.73905\n",
      "[LOGITS Ex2 A] Mean Abs: 0.610 | Max: 3.490\n",
      "[LOSS Ex2] A: 0.50323 | B: 0.52731 | C: 0.49172\n",
      "** [JOINT LOSS] ** : 0.948026\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001893 | Grad Max: 0.032377\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.028120 | Grad Max: 0.166275\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006979 | Grad Max: 0.023054\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.057896 | Grad Max: 0.057896\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000265 | Grad Max: 0.008719\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003997 | Grad Max: 0.052503\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000068 | Grad Max: 0.001770\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001690 | Grad Max: 0.006448\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000019 | Grad Max: 0.000400\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000584 | Grad Max: 0.001955\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000306\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000322 | Grad Max: 0.001206\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001316 | Grad Max: 0.003496\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.014958 | Grad Max: 0.014958\n",
      "[GRADIENT NORM TOTAL] 0.5797\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52929044 0.47070956] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 107/1941 | C: 4/2044\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58008 | C: 0.74483\n",
      "[LOGITS Ex2 A] Mean Abs: 0.607 | Max: 3.573\n",
      "[LOSS Ex2] A: 0.51368 | B: 0.52653 | C: 0.49531\n",
      "** [JOINT LOSS] ** : 0.953474\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001636 | Grad Max: 0.020393\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.046837 | Grad Max: 0.259384\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004576 | Grad Max: 0.018650\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.045861 | Grad Max: 0.045861\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000428 | Grad Max: 0.012478\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007048 | Grad Max: 0.069680\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000114 | Grad Max: 0.002571\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002961 | Grad Max: 0.009819\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000032 | Grad Max: 0.000518\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001001 | Grad Max: 0.002834\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000435\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000538 | Grad Max: 0.001699\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001856 | Grad Max: 0.004557\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022019 | Grad Max: 0.022019\n",
      "[GRADIENT NORM TOTAL] 0.9088\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5314897  0.46851036] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 88/1768 | C: 5/2043\n",
      "[LOSS Ex1] A: 0.79861 | B: 0.58038 | C: 0.76002\n",
      "[LOGITS Ex2 A] Mean Abs: 0.602 | Max: 3.542\n",
      "[LOSS Ex2] A: 0.50205 | B: 0.52874 | C: 0.48639\n",
      "** [JOINT LOSS] ** : 1.218729\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001758 | Grad Max: 0.045358\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.017261 | Grad Max: 0.080925\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014385 | Grad Max: 0.046141\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.153714 | Grad Max: 0.153714\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000175 | Grad Max: 0.005344\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002310 | Grad Max: 0.024956\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000041 | Grad Max: 0.001689\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000906 | Grad Max: 0.005914\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000295\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000297 | Grad Max: 0.001261\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000274\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000151 | Grad Max: 0.000764\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000507 | Grad Max: 0.002226\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005200 | Grad Max: 0.005200\n",
      "[GRADIENT NORM TOTAL] 0.4788\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5204365 0.4795635] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 96/1952 | C: 8/2040\n",
      "[LOSS Ex1] A: 0.79915 | B: 0.58145 | C: 0.71940\n",
      "[LOGITS Ex2 A] Mean Abs: 0.603 | Max: 3.563\n",
      "[LOSS Ex2] A: 0.48821 | B: 0.51558 | C: 0.46086\n",
      "** [JOINT LOSS] ** : 1.188217\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002157 | Grad Max: 0.048426\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.050270 | Grad Max: 0.296969\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009931 | Grad Max: 0.032603\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.095712 | Grad Max: 0.095712\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000441 | Grad Max: 0.012132\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007298 | Grad Max: 0.056956\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000122 | Grad Max: 0.002696\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003139 | Grad Max: 0.010382\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000034 | Grad Max: 0.000548\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001062 | Grad Max: 0.003115\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000027 | Grad Max: 0.000441\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000572 | Grad Max: 0.001802\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002055 | Grad Max: 0.006525\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024274 | Grad Max: 0.024274\n",
      "[GRADIENT NORM TOTAL] 0.9869\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.057\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5339898  0.46601024] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 94/1954 | C: 5/1371\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58254 | C: 0.75737\n",
      "[LOGITS Ex2 A] Mean Abs: 0.586 | Max: 3.484\n",
      "[LOSS Ex2] A: 0.50630 | B: 0.52794 | C: 0.49646\n",
      "** [JOINT LOSS] ** : 0.956871\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001727 | Grad Max: 0.029946\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.009659 | Grad Max: 0.049152\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004176 | Grad Max: 0.023135\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.030237 | Grad Max: 0.030237\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000174 | Grad Max: 0.008028\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001890 | Grad Max: 0.047106\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000037 | Grad Max: 0.001664\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000701 | Grad Max: 0.004456\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000010 | Grad Max: 0.000329\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000221 | Grad Max: 0.001360\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000205\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000114 | Grad Max: 0.000686\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000426 | Grad Max: 0.002056\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.003324 | Grad Max: 0.003324\n",
      "[GRADIENT NORM TOTAL] 0.3019\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0025\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0806 | Alpha: 0.5500\n",
      "No improve count: 4/15\n",
      "\n",
      "############################## EPOCH 8/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5343069  0.46569315] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 108/1940 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57972 | C: 0.74500\n",
      "[LOGITS Ex2 A] Mean Abs: 0.601 | Max: 3.508\n",
      "[LOSS Ex2] A: 0.50223 | B: 0.52658 | C: 0.47257\n",
      "** [JOINT LOSS] ** : 0.942033\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001774 | Grad Max: 0.037170\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.053029 | Grad Max: 0.301704\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004634 | Grad Max: 0.018545\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.045778 | Grad Max: 0.045778\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000479 | Grad Max: 0.013146\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007794 | Grad Max: 0.072171\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000127 | Grad Max: 0.002589\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003269 | Grad Max: 0.009817\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000615\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001074 | Grad Max: 0.003394\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000027 | Grad Max: 0.000457\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000561 | Grad Max: 0.001813\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001767 | Grad Max: 0.005141\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022019 | Grad Max: 0.022019\n",
      "[GRADIENT NORM TOTAL] 1.0282\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.035\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5341953  0.46580473] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 88/1768 | C: 4/2044\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57991 | C: 0.69172\n",
      "[LOGITS Ex2 A] Mean Abs: 0.644 | Max: 3.420\n",
      "[LOSS Ex2] A: 0.49620 | B: 0.53148 | C: 0.49215\n",
      "** [JOINT LOSS] ** : 0.930488\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002016 | Grad Max: 0.054820\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.039399 | Grad Max: 0.231915\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009570 | Grad Max: 0.042755\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.128605 | Grad Max: 0.128605\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000359 | Grad Max: 0.009759\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005557 | Grad Max: 0.059174\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000094 | Grad Max: 0.002960\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002345 | Grad Max: 0.010446\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000435\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000775 | Grad Max: 0.002439\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000329\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000413 | Grad Max: 0.001426\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001578 | Grad Max: 0.003861\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018034 | Grad Max: 0.018034\n",
      "[GRADIENT NORM TOTAL] 0.7963\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.063\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53467906 0.46532094] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 98/1950 | C: 4/2044\n",
      "[LOSS Ex1] A: 0.80146 | B: 0.58129 | C: 0.74643\n",
      "[LOGITS Ex2 A] Mean Abs: 0.615 | Max: 3.645\n",
      "[LOSS Ex2] A: 0.51000 | B: 0.51784 | C: 0.49282\n",
      "** [JOINT LOSS] ** : 1.216611\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002897 | Grad Max: 0.054311\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.038662 | Grad Max: 0.221337\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012755 | Grad Max: 0.066819\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.137915 | Grad Max: 0.137915\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000379 | Grad Max: 0.011009\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005656 | Grad Max: 0.068009\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000093 | Grad Max: 0.002704\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002268 | Grad Max: 0.008134\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000408\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000765 | Grad Max: 0.002085\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000371\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000403 | Grad Max: 0.001375\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001526 | Grad Max: 0.004130\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017641 | Grad Max: 0.017641\n",
      "[GRADIENT NORM TOTAL] 0.8433\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.045\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5343823 0.4656177] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 97/1951 | C: 4/2044\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58258 | C: 0.69347\n",
      "[LOGITS Ex2 A] Mean Abs: 0.631 | Max: 3.365\n",
      "[LOSS Ex2] A: 0.49905 | B: 0.52311 | C: 0.48459\n",
      "** [JOINT LOSS] ** : 0.927597\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002351 | Grad Max: 0.063171\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.032525 | Grad Max: 0.169735\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008675 | Grad Max: 0.040766\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.129364 | Grad Max: 0.129364\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000359 | Grad Max: 0.010435\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004753 | Grad Max: 0.062591\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000083 | Grad Max: 0.002200\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001945 | Grad Max: 0.006722\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000023 | Grad Max: 0.000417\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000638 | Grad Max: 0.002164\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000359\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000324 | Grad Max: 0.001317\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000956 | Grad Max: 0.002807\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011258 | Grad Max: 0.011258\n",
      "[GRADIENT NORM TOTAL] 0.7112\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52344453 0.47655547] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 109/1939 | C: 4/2044\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57939 | C: 0.63932\n",
      "[LOGITS Ex2 A] Mean Abs: 0.660 | Max: 3.663\n",
      "[LOSS Ex2] A: 0.49928 | B: 0.52990 | C: 0.48204\n",
      "** [JOINT LOSS] ** : 0.909977\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002333 | Grad Max: 0.036776\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.066328 | Grad Max: 0.348862\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015549 | Grad Max: 0.079818\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.211587 | Grad Max: 0.211587\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000565 | Grad Max: 0.013973\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009420 | Grad Max: 0.080485\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000151 | Grad Max: 0.003245\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003957 | Grad Max: 0.012150\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000041 | Grad Max: 0.000668\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001296 | Grad Max: 0.004099\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000031 | Grad Max: 0.000571\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000676 | Grad Max: 0.002270\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002114 | Grad Max: 0.004974\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.026923 | Grad Max: 0.026923\n",
      "[GRADIENT NORM TOTAL] 1.2987\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5291544 0.4708456] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 91/1765 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57990 | C: 0.74522\n",
      "[LOGITS Ex2 A] Mean Abs: 0.628 | Max: 3.473\n",
      "[LOSS Ex2] A: 0.50450 | B: 0.52494 | C: 0.48649\n",
      "** [JOINT LOSS] ** : 0.947017\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003147 | Grad Max: 0.088338\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.053197 | Grad Max: 0.307669\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004190 | Grad Max: 0.017087\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.045769 | Grad Max: 0.045769\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000563 | Grad Max: 0.013943\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008134 | Grad Max: 0.054113\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000144 | Grad Max: 0.004712\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003518 | Grad Max: 0.015833\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000039 | Grad Max: 0.000637\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001155 | Grad Max: 0.003413\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000030 | Grad Max: 0.000569\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000610 | Grad Max: 0.002027\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002395 | Grad Max: 0.005482\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.026646 | Grad Max: 0.026646\n",
      "[GRADIENT NORM TOTAL] 1.0921\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5310959  0.46890405] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 99/1949 | C: 7/2041\n",
      "[LOSS Ex1] A: 0.79857 | B: 0.58083 | C: 0.68033\n",
      "[LOGITS Ex2 A] Mean Abs: 0.631 | Max: 3.568\n",
      "[LOSS Ex2] A: 0.49315 | B: 0.52154 | C: 0.48779\n",
      "** [JOINT LOSS] ** : 1.187408\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002401 | Grad Max: 0.040452\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.031053 | Grad Max: 0.180995\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008511 | Grad Max: 0.036852\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.030198 | Grad Max: 0.030198\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000292 | Grad Max: 0.009839\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004316 | Grad Max: 0.039529\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000076 | Grad Max: 0.002018\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001829 | Grad Max: 0.006525\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000020 | Grad Max: 0.000374\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000590 | Grad Max: 0.001825\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000250\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000312 | Grad Max: 0.001100\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001235 | Grad Max: 0.003155\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013582 | Grad Max: 0.013582\n",
      "[GRADIENT NORM TOTAL] 0.6351\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5199625  0.48003748] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 98/1950 | C: 5/2043\n",
      "[LOSS Ex1] A: 0.79915 | B: 0.58197 | C: 0.67552\n",
      "[LOGITS Ex2 A] Mean Abs: 0.644 | Max: 3.549\n",
      "[LOSS Ex2] A: 0.48236 | B: 0.52246 | C: 0.48495\n",
      "** [JOINT LOSS] ** : 1.182135\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002687 | Grad Max: 0.061862\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.056929 | Grad Max: 0.302941\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008376 | Grad Max: 0.035504\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.020348 | Grad Max: 0.020348\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000555 | Grad Max: 0.015832\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008494 | Grad Max: 0.083537\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000140 | Grad Max: 0.003411\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003472 | Grad Max: 0.011943\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000038 | Grad Max: 0.000510\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001121 | Grad Max: 0.003156\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000028 | Grad Max: 0.000457\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000582 | Grad Max: 0.001879\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001914 | Grad Max: 0.004797\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022871 | Grad Max: 0.022871\n",
      "[GRADIENT NORM TOTAL] 1.1060\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.058\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53413135 0.4658686 ] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 6/2042\n",
      "[LOSS Ex1] A: 0.79857 | B: 0.57846 | C: 0.69139\n",
      "[LOGITS Ex2 A] Mean Abs: 0.620 | Max: 3.542\n",
      "[LOSS Ex2] A: 0.49997 | B: 0.51487 | C: 0.46668\n",
      "** [JOINT LOSS] ** : 1.183313\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002331 | Grad Max: 0.049044\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.062137 | Grad Max: 0.341683\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008195 | Grad Max: 0.027971\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.054980 | Grad Max: 0.054980\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000572 | Grad Max: 0.015007\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009139 | Grad Max: 0.083639\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000149 | Grad Max: 0.004394\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003837 | Grad Max: 0.013191\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000040 | Grad Max: 0.000657\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001237 | Grad Max: 0.003541\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000030 | Grad Max: 0.000478\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000637 | Grad Max: 0.001981\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002058 | Grad Max: 0.005574\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025328 | Grad Max: 0.025328\n",
      "[GRADIENT NORM TOTAL] 1.1936\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53438836 0.46561164] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 91/1765 | C: 3/2045\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57894 | C: 0.65610\n",
      "[LOGITS Ex2 A] Mean Abs: 0.617 | Max: 3.488\n",
      "[LOSS Ex2] A: 0.49672 | B: 0.53296 | C: 0.48945\n",
      "** [JOINT LOSS] ** : 0.918057\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003875 | Grad Max: 0.084079\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.108079 | Grad Max: 0.612313\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013252 | Grad Max: 0.062788\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.183374 | Grad Max: 0.183374\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000954 | Grad Max: 0.023231\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.015612 | Grad Max: 0.126837\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000259 | Grad Max: 0.005320\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006720 | Grad Max: 0.021015\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000069 | Grad Max: 0.000957\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002171 | Grad Max: 0.006099\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000052 | Grad Max: 0.000836\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001126 | Grad Max: 0.003594\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003886 | Grad Max: 0.008579\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.047657 | Grad Max: 0.047657\n",
      "[GRADIENT NORM TOTAL] 2.0738\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.035\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5343228  0.46567726] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 99/1949 | C: 9/2039\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57983 | C: 0.75264\n",
      "[LOGITS Ex2 A] Mean Abs: 0.663 | Max: 3.548\n",
      "[LOSS Ex2] A: 0.50048 | B: 0.51045 | C: 0.46494\n",
      "** [JOINT LOSS] ** : 0.936117\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003607 | Grad Max: 0.115861\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.111098 | Grad Max: 0.641616\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004207 | Grad Max: 0.025353\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.036954 | Grad Max: 0.036954\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001017 | Grad Max: 0.026527\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.016466 | Grad Max: 0.133813\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000275 | Grad Max: 0.005999\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.007039 | Grad Max: 0.023485\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000073 | Grad Max: 0.000939\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002260 | Grad Max: 0.005932\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000054 | Grad Max: 0.000895\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001167 | Grad Max: 0.003519\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004010 | Grad Max: 0.008953\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.049122 | Grad Max: 0.049122\n",
      "[GRADIENT NORM TOTAL] 2.1553\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5348536  0.46514642] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 99/1949 | C: 19/2029\n",
      "[LOSS Ex1] A: 0.80258 | B: 0.58113 | C: 0.73236\n",
      "[LOGITS Ex2 A] Mean Abs: 0.632 | Max: 3.639\n",
      "[LOSS Ex2] A: 0.48670 | B: 0.51857 | C: 0.46599\n",
      "** [JOINT LOSS] ** : 1.195776\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001939 | Grad Max: 0.034714\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.018807 | Grad Max: 0.096265\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010268 | Grad Max: 0.058715\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.115828 | Grad Max: 0.115828\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000173 | Grad Max: 0.006085\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002478 | Grad Max: 0.026458\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000039 | Grad Max: 0.001678\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000892 | Grad Max: 0.004735\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000010 | Grad Max: 0.000266\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000281 | Grad Max: 0.001276\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000221\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000148 | Grad Max: 0.000762\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000634 | Grad Max: 0.002870\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007100 | Grad Max: 0.007100\n",
      "[GRADIENT NORM TOTAL] 0.4615\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.046\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.534547 0.465453] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 110/1938 | C: 16/2032\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57768 | C: 0.73148\n",
      "[LOGITS Ex2 A] Mean Abs: 0.676 | Max: 3.348\n",
      "[LOSS Ex2] A: 0.50571 | B: 0.51934 | C: 0.47161\n",
      "** [JOINT LOSS] ** : 0.935273\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.005104 | Grad Max: 0.172791\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.129142 | Grad Max: 0.729656\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005925 | Grad Max: 0.034847\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.066076 | Grad Max: 0.066076\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001293 | Grad Max: 0.036116\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.019390 | Grad Max: 0.171901\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000329 | Grad Max: 0.007048\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.008238 | Grad Max: 0.027776\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000087 | Grad Max: 0.001068\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002642 | Grad Max: 0.006808\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000065 | Grad Max: 0.000983\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001356 | Grad Max: 0.004199\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004516 | Grad Max: 0.009749\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.054769 | Grad Max: 0.054769\n",
      "[GRADIENT NORM TOTAL] 2.5364\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52304345 0.47695655] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 92/1764 | C: 9/1367\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57824 | C: 0.70418\n",
      "[LOGITS Ex2 A] Mean Abs: 0.682 | Max: 3.631\n",
      "[LOSS Ex2] A: 0.48552 | B: 0.53049 | C: 0.46390\n",
      "** [JOINT LOSS] ** : 0.920780\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.004285 | Grad Max: 0.130743\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.108019 | Grad Max: 0.617781\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.007536 | Grad Max: 0.042418\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.110066 | Grad Max: 0.110066\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001056 | Grad Max: 0.029647\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.016274 | Grad Max: 0.146145\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000276 | Grad Max: 0.007483\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006969 | Grad Max: 0.026660\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000073 | Grad Max: 0.001040\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002217 | Grad Max: 0.006318\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000054 | Grad Max: 0.000828\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001132 | Grad Max: 0.003389\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003781 | Grad Max: 0.008069\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.045642 | Grad Max: 0.045642\n",
      "[GRADIENT NORM TOTAL] 2.1236\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0238\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0587 | Alpha: 0.5500\n",
      "No improve count: 5/15\n",
      "\n",
      "############################## EPOCH 9/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52931416 0.47068584] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 99/1949 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57906 | C: 0.69188\n",
      "[LOGITS Ex2 A] Mean Abs: 0.650 | Max: 3.770\n",
      "[LOSS Ex2] A: 0.49582 | B: 0.50283 | C: 0.47134\n",
      "** [JOINT LOSS] ** : 0.913645\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002072 | Grad Max: 0.040195\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.025895 | Grad Max: 0.143610\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009375 | Grad Max: 0.050434\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.128558 | Grad Max: 0.128558\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000265 | Grad Max: 0.013597\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003778 | Grad Max: 0.084516\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.002184\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001511 | Grad Max: 0.006690\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000306\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000482 | Grad Max: 0.001745\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000012 | Grad Max: 0.000259\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000251 | Grad Max: 0.000921\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001035 | Grad Max: 0.003016\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.011592 | Grad Max: 0.011592\n",
      "[GRADIENT NORM TOTAL] 0.5857\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53105974 0.4689403 ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 101/1947 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58074 | C: 0.77932\n",
      "[LOGITS Ex2 A] Mean Abs: 0.650 | Max: 3.850\n",
      "[LOSS Ex2] A: 0.48933 | B: 0.52016 | C: 0.47452\n",
      "** [JOINT LOSS] ** : 0.948024\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003496 | Grad Max: 0.107458\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.105561 | Grad Max: 0.630277\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003595 | Grad Max: 0.019470\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.003481 | Grad Max: 0.003481\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000960 | Grad Max: 0.026261\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.015658 | Grad Max: 0.128129\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000258 | Grad Max: 0.005948\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.006703 | Grad Max: 0.022708\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000067 | Grad Max: 0.000869\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002139 | Grad Max: 0.005653\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000050 | Grad Max: 0.000716\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001102 | Grad Max: 0.003253\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003875 | Grad Max: 0.008427\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.047475 | Grad Max: 0.047475\n",
      "[GRADIENT NORM TOTAL] 2.0486\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5199052 0.4800948] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 110/1938 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.79885 | B: 0.57685 | C: 0.77336\n",
      "[LOGITS Ex2 A] Mean Abs: 0.655 | Max: 3.545\n",
      "[LOSS Ex2] A: 0.47023 | B: 0.51002 | C: 0.45599\n",
      "** [JOINT LOSS] ** : 1.195104\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001886 | Grad Max: 0.045393\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.016355 | Grad Max: 0.076714\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015084 | Grad Max: 0.056535\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.179254 | Grad Max: 0.179254\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000161 | Grad Max: 0.015970\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002114 | Grad Max: 0.098667\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000035 | Grad Max: 0.001407\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000707 | Grad Max: 0.003638\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000213\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000215 | Grad Max: 0.001108\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000189\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000116 | Grad Max: 0.000626\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000760 | Grad Max: 0.002877\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005661 | Grad Max: 0.005661\n",
      "[GRADIENT NORM TOTAL] 0.5101\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.059\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53422314 0.4657769 ] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 93/1763 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57763 | C: 0.73492\n",
      "[LOGITS Ex2 A] Mean Abs: 0.656 | Max: 3.585\n",
      "[LOSS Ex2] A: 0.48632 | B: 0.52707 | C: 0.46850\n",
      "** [JOINT LOSS] ** : 0.931480\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002614 | Grad Max: 0.070951\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.078880 | Grad Max: 0.453448\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004474 | Grad Max: 0.023393\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.061675 | Grad Max: 0.061675\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000727 | Grad Max: 0.020460\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011572 | Grad Max: 0.117080\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000191 | Grad Max: 0.004429\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004918 | Grad Max: 0.016034\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000050 | Grad Max: 0.000715\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001545 | Grad Max: 0.004353\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000036 | Grad Max: 0.000574\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000778 | Grad Max: 0.002489\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002455 | Grad Max: 0.005469\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.030874 | Grad Max: 0.030874\n",
      "[GRADIENT NORM TOTAL] 1.5242\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5344935  0.46550655] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 99/1949 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57830 | C: 0.74517\n",
      "[LOGITS Ex2 A] Mean Abs: 0.648 | Max: 3.679\n",
      "[LOSS Ex2] A: 0.48695 | B: 0.50596 | C: 0.45424\n",
      "** [JOINT LOSS] ** : 0.923543\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001386 | Grad Max: 0.018273\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.017065 | Grad Max: 0.086296\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003945 | Grad Max: 0.024115\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.045537 | Grad Max: 0.045537\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000195 | Grad Max: 0.006399\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002818 | Grad Max: 0.035624\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000042 | Grad Max: 0.001715\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000953 | Grad Max: 0.005417\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000284 | Grad Max: 0.001321\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000195\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000138 | Grad Max: 0.000600\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000553 | Grad Max: 0.002430\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.004289 | Grad Max: 0.004289\n",
      "[GRADIENT NORM TOTAL] 0.3792\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.035\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5344119  0.46558806] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 101/1947 | C: 14/2034\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.58008 | C: 0.73965\n",
      "[LOGITS Ex2 A] Mean Abs: 0.694 | Max: 3.769\n",
      "[LOSS Ex2] A: 0.47376 | B: 0.51599 | C: 0.46201\n",
      "** [JOINT LOSS] ** : 0.923827\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002629 | Grad Max: 0.096527\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.076145 | Grad Max: 0.442795\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004172 | Grad Max: 0.025554\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.058113 | Grad Max: 0.058113\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000697 | Grad Max: 0.020286\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011193 | Grad Max: 0.097735\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000185 | Grad Max: 0.004482\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004759 | Grad Max: 0.016033\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000048 | Grad Max: 0.000701\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001500 | Grad Max: 0.004165\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000034 | Grad Max: 0.000547\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000763 | Grad Max: 0.002288\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002612 | Grad Max: 0.006755\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.032443 | Grad Max: 0.032443\n",
      "[GRADIENT NORM TOTAL] 1.4756\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.065\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53491205 0.46508798] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 110/1938 | C: 18/2030\n",
      "[LOSS Ex1] A: 0.80329 | B: 0.57618 | C: 0.76256\n",
      "[LOGITS Ex2 A] Mean Abs: 0.671 | Max: 3.722\n",
      "[LOSS Ex2] A: 0.48388 | B: 0.50710 | C: 0.47582\n",
      "** [JOINT LOSS] ** : 1.202941\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002333 | Grad Max: 0.044610\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.020004 | Grad Max: 0.073880\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013337 | Grad Max: 0.074657\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.166630 | Grad Max: 0.166630\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000195 | Grad Max: 0.006647\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002660 | Grad Max: 0.033612\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000039 | Grad Max: 0.001680\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000847 | Grad Max: 0.004409\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000196\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000244 | Grad Max: 0.001219\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000212\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000121 | Grad Max: 0.000679\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000425 | Grad Max: 0.001896\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.004248 | Grad Max: 0.004248\n",
      "[GRADIENT NORM TOTAL] 0.5487\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.047\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5345794  0.46542057] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 93/1763 | C: 7/2041\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57705 | C: 0.79996\n",
      "[LOGITS Ex2 A] Mean Abs: 0.681 | Max: 3.564\n",
      "[LOSS Ex2] A: 0.47586 | B: 0.51234 | C: 0.47716\n",
      "** [JOINT LOSS] ** : 0.947461\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002347 | Grad Max: 0.059328\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.039653 | Grad Max: 0.215501\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005424 | Grad Max: 0.030597\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.037423 | Grad Max: 0.037423\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000404 | Grad Max: 0.016570\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005819 | Grad Max: 0.074567\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000098 | Grad Max: 0.002350\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002458 | Grad Max: 0.009751\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000444\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000748 | Grad Max: 0.002524\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000018 | Grad Max: 0.000297\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000369 | Grad Max: 0.001289\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001073 | Grad Max: 0.003547\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013579 | Grad Max: 0.013579\n",
      "[GRADIENT NORM TOTAL] 0.8008\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.031\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52280873 0.47719127] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 99/1949 | C: 9/2039\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57790 | C: 0.65633\n",
      "[LOGITS Ex2 A] Mean Abs: 0.687 | Max: 3.528\n",
      "[LOSS Ex2] A: 0.47222 | B: 0.50374 | C: 0.47761\n",
      "** [JOINT LOSS] ** : 0.895936\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002919 | Grad Max: 0.073445\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.057883 | Grad Max: 0.321399\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013071 | Grad Max: 0.059307\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.183471 | Grad Max: 0.183471\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000557 | Grad Max: 0.017533\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008425 | Grad Max: 0.106036\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000138 | Grad Max: 0.003275\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003492 | Grad Max: 0.011865\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000580\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001099 | Grad Max: 0.003341\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000025 | Grad Max: 0.000391\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000553 | Grad Max: 0.001656\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001881 | Grad Max: 0.004718\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023223 | Grad Max: 0.023223\n",
      "[GRADIENT NORM TOTAL] 1.1721\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5294934  0.47050658] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 101/1947 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57976 | C: 0.71370\n",
      "[LOGITS Ex2 A] Mean Abs: 0.671 | Max: 3.545\n",
      "[LOSS Ex2] A: 0.48515 | B: 0.50791 | C: 0.45692\n",
      "** [JOINT LOSS] ** : 0.914481\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001545 | Grad Max: 0.020488\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015025 | Grad Max: 0.070785\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006732 | Grad Max: 0.036135\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.095617 | Grad Max: 0.095617\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000167 | Grad Max: 0.011463\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002286 | Grad Max: 0.066297\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000032 | Grad Max: 0.001209\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000651 | Grad Max: 0.003287\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000282\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000197 | Grad Max: 0.001106\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000173\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000103 | Grad Max: 0.000653\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000503 | Grad Max: 0.002253\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005128 | Grad Max: 0.005128\n",
      "[GRADIENT NORM TOTAL] 0.3666\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.040\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53100795 0.46899202] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 112/1936 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57622 | C: 0.72299\n",
      "[LOGITS Ex2 A] Mean Abs: 0.697 | Max: 3.793\n",
      "[LOSS Ex2] A: 0.47404 | B: 0.50870 | C: 0.46495\n",
      "** [JOINT LOSS] ** : 0.915634\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002597 | Grad Max: 0.065969\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.068662 | Grad Max: 0.380552\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005696 | Grad Max: 0.028874\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.083135 | Grad Max: 0.083135\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000647 | Grad Max: 0.017304\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009996 | Grad Max: 0.098377\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000160 | Grad Max: 0.003668\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004119 | Grad Max: 0.012607\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000040 | Grad Max: 0.000583\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001263 | Grad Max: 0.003476\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000028 | Grad Max: 0.000466\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000621 | Grad Max: 0.001775\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001720 | Grad Max: 0.004467\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023336 | Grad Max: 0.023336\n",
      "[GRADIENT NORM TOTAL] 1.3175\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.51967096 0.480329  ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 93/1763 | C: 7/2041\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57658 | C: 0.70578\n",
      "[LOGITS Ex2 A] Mean Abs: 0.684 | Max: 3.704\n",
      "[LOSS Ex2] A: 0.46590 | B: 0.51920 | C: 0.46465\n",
      "** [JOINT LOSS] ** : 0.910704\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001854 | Grad Max: 0.029412\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015017 | Grad Max: 0.062247\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008261 | Grad Max: 0.041849\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.104179 | Grad Max: 0.104179\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000189 | Grad Max: 0.014844\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002504 | Grad Max: 0.089957\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000034 | Grad Max: 0.001412\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000677 | Grad Max: 0.004979\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000222\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000193 | Grad Max: 0.001284\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000178\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000099 | Grad Max: 0.000604\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000471 | Grad Max: 0.001978\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005147 | Grad Max: 0.005147\n",
      "[GRADIENT NORM TOTAL] 0.4119\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.060\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5342597  0.46574026] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 100/1948 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57747 | C: 0.58721\n",
      "[LOGITS Ex2 A] Mean Abs: 0.663 | Max: 3.664\n",
      "[LOSS Ex2] A: 0.47045 | B: 0.50261 | C: 0.47313\n",
      "** [JOINT LOSS] ** : 0.870290\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003194 | Grad Max: 0.061394\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.047576 | Grad Max: 0.243786\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.022053 | Grad Max: 0.103762\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.294244 | Grad Max: 0.294244\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000404 | Grad Max: 0.018279\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006237 | Grad Max: 0.113897\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000101 | Grad Max: 0.002934\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002522 | Grad Max: 0.010189\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000520\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000780 | Grad Max: 0.002581\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000018 | Grad Max: 0.000397\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000391 | Grad Max: 0.001441\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001406 | Grad Max: 0.004069\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017342 | Grad Max: 0.017342\n",
      "[GRADIENT NORM TOTAL] 1.0632\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5346662 0.4653338] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 103/1945 | C: 6/1370\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57922 | C: 0.72563\n",
      "[LOGITS Ex2 A] Mean Abs: 0.685 | Max: 3.733\n",
      "[LOSS Ex2] A: 0.47670 | B: 0.50628 | C: 0.48775\n",
      "** [JOINT LOSS] ** : 0.925193\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001877 | Grad Max: 0.032683\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.021569 | Grad Max: 0.115862\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004973 | Grad Max: 0.030520\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.073035 | Grad Max: 0.073035\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000248 | Grad Max: 0.009357\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003475 | Grad Max: 0.041632\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000057 | Grad Max: 0.002175\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001327 | Grad Max: 0.006591\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000340\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000384 | Grad Max: 0.001727\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000228\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000186 | Grad Max: 0.000834\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000452 | Grad Max: 0.001975\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.006090 | Grad Max: 0.006090\n",
      "[GRADIENT NORM TOTAL] 0.4755\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 0.9584\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0581 | Alpha: 0.5500\n",
      "No improve count: 6/15\n",
      "\n",
      "############################## EPOCH 10/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.036\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53463966 0.46536034] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 112/1936 | C: 15/2033\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57503 | C: 0.74248\n",
      "[LOGITS Ex2 A] Mean Abs: 0.728 | Max: 3.805\n",
      "[LOSS Ex2] A: 0.46540 | B: 0.50256 | C: 0.45530\n",
      "** [JOINT LOSS] ** : 0.913589\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001454 | Grad Max: 0.021356\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.027213 | Grad Max: 0.137233\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004099 | Grad Max: 0.020700\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.050433 | Grad Max: 0.050433\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000266 | Grad Max: 0.010233\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004142 | Grad Max: 0.057012\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000060 | Grad Max: 0.001815\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001513 | Grad Max: 0.005761\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000281\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000453 | Grad Max: 0.001666\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000219\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000218 | Grad Max: 0.000859\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000604 | Grad Max: 0.002478\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007857 | Grad Max: 0.007857\n",
      "[GRADIENT NORM TOTAL] 0.5464\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.066\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351733 0.4648267] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 93/1763 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.80419 | B: 0.57515 | C: 0.76677\n",
      "[LOGITS Ex2 A] Mean Abs: 0.698 | Max: 3.857\n",
      "[LOSS Ex2] A: 0.48049 | B: 0.50950 | C: 0.47730\n",
      "** [JOINT LOSS] ** : 1.204469\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003799 | Grad Max: 0.101562\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.061809 | Grad Max: 0.335119\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013320 | Grad Max: 0.072714\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.170943 | Grad Max: 0.170943\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000618 | Grad Max: 0.017253\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008820 | Grad Max: 0.073265\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000150 | Grad Max: 0.004018\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003644 | Grad Max: 0.013112\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000037 | Grad Max: 0.000536\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001119 | Grad Max: 0.003113\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000413\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000555 | Grad Max: 0.001837\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001990 | Grad Max: 0.004595\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024115 | Grad Max: 0.024115\n",
      "[GRADIENT NORM TOTAL] 1.2625\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53489476 0.4651052 ] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 101/1947 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57625 | C: 0.71913\n",
      "[LOGITS Ex2 A] Mean Abs: 0.707 | Max: 3.752\n",
      "[LOSS Ex2] A: 0.46026 | B: 0.49750 | C: 0.44846\n",
      "** [JOINT LOSS] ** : 0.900535\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001634 | Grad Max: 0.024856\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.013623 | Grad Max: 0.063545\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005516 | Grad Max: 0.034303\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.082433 | Grad Max: 0.082433\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000176 | Grad Max: 0.007962\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002178 | Grad Max: 0.042724\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000037 | Grad Max: 0.001577\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000763 | Grad Max: 0.005253\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000285\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000215 | Grad Max: 0.001363\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000162\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000097 | Grad Max: 0.000573\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000604 | Grad Max: 0.001945\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.002545 | Grad Max: 0.002545\n",
      "[GRADIENT NORM TOTAL] 0.3490\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.523131   0.47686896] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 106/1942 | C: 9/2039\n",
      "[LOSS Ex1] A: 0.79958 | B: 0.57848 | C: 0.75315\n",
      "[LOGITS Ex2 A] Mean Abs: 0.734 | Max: 3.637\n",
      "[LOSS Ex2] A: 0.46451 | B: 0.50793 | C: 0.47392\n",
      "** [JOINT LOSS] ** : 1.192521\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003073 | Grad Max: 0.054414\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.054818 | Grad Max: 0.307323\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.019883 | Grad Max: 0.100954\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.146765 | Grad Max: 0.146765\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000497 | Grad Max: 0.017400\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007852 | Grad Max: 0.096828\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000126 | Grad Max: 0.003090\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003265 | Grad Max: 0.012581\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000031 | Grad Max: 0.000438\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000982 | Grad Max: 0.002772\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000338\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000476 | Grad Max: 0.001535\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001388 | Grad Max: 0.003544\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018401 | Grad Max: 0.018401\n",
      "[GRADIENT NORM TOTAL] 1.1376\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5302437  0.46975628] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 113/1935 | C: 12/2036\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57413 | C: 0.76467\n",
      "[LOGITS Ex2 A] Mean Abs: 0.711 | Max: 3.677\n",
      "[LOSS Ex2] A: 0.48064 | B: 0.49608 | C: 0.46637\n",
      "** [JOINT LOSS] ** : 0.927300\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003173 | Grad Max: 0.088575\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.039125 | Grad Max: 0.230811\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003348 | Grad Max: 0.015423\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.017232 | Grad Max: 0.017232\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000452 | Grad Max: 0.018146\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005984 | Grad Max: 0.111156\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000105 | Grad Max: 0.002752\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002549 | Grad Max: 0.009014\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000437\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000782 | Grad Max: 0.002504\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000018 | Grad Max: 0.000308\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000383 | Grad Max: 0.001289\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001425 | Grad Max: 0.004042\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016651 | Grad Max: 0.016651\n",
      "[GRADIENT NORM TOTAL] 0.8538\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53153235 0.46846768] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 94/1762 | C: 8/2040\n",
      "[LOSS Ex1] A: 0.79903 | B: 0.57456 | C: 0.77178\n",
      "[LOGITS Ex2 A] Mean Abs: 0.702 | Max: 3.733\n",
      "[LOSS Ex2] A: 0.45931 | B: 0.51686 | C: 0.47494\n",
      "** [JOINT LOSS] ** : 1.198822\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001903 | Grad Max: 0.042041\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.014477 | Grad Max: 0.070941\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015044 | Grad Max: 0.055708\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.179924 | Grad Max: 0.179924\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000153 | Grad Max: 0.005806\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.001950 | Grad Max: 0.030675\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000033 | Grad Max: 0.001299\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000630 | Grad Max: 0.004133\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000008 | Grad Max: 0.000185\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000185 | Grad Max: 0.001061\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000151\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000097 | Grad Max: 0.000570\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000479 | Grad Max: 0.001826\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005191 | Grad Max: 0.005191\n",
      "[GRADIENT NORM TOTAL] 0.4765\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.042\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5200395 0.4799605] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 102/1946 | C: 12/2036\n",
      "[LOSS Ex1] A: 0.79951 | B: 0.57586 | C: 0.71074\n",
      "[LOGITS Ex2 A] Mean Abs: 0.721 | Max: 3.735\n",
      "[LOSS Ex2] A: 0.45718 | B: 0.49673 | C: 0.44301\n",
      "** [JOINT LOSS] ** : 1.161010\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003112 | Grad Max: 0.087449\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.054360 | Grad Max: 0.283354\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010017 | Grad Max: 0.035943\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.083164 | Grad Max: 0.083164\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000556 | Grad Max: 0.022434\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007786 | Grad Max: 0.105509\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000130 | Grad Max: 0.003122\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003174 | Grad Max: 0.012632\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000031 | Grad Max: 0.000483\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000935 | Grad Max: 0.002653\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000313\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000449 | Grad Max: 0.001381\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001303 | Grad Max: 0.003867\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017167 | Grad Max: 0.017167\n",
      "[GRADIENT NORM TOTAL] 1.0887\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.061\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53458536 0.46541464] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 107/1941 | C: 17/2031\n",
      "[LOSS Ex1] A: 0.69810 | B: 0.57830 | C: 0.73832\n",
      "[LOGITS Ex2 A] Mean Abs: 0.700 | Max: 3.663\n",
      "[LOSS Ex2] A: 0.45110 | B: 0.51140 | C: 0.46636\n",
      "** [JOINT LOSS] ** : 1.147860\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001993 | Grad Max: 0.049573\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.066616 | Grad Max: 0.382868\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006147 | Grad Max: 0.026028\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.043673 | Grad Max: 0.043673\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000546 | Grad Max: 0.022765\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009646 | Grad Max: 0.141703\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000146 | Grad Max: 0.004422\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003928 | Grad Max: 0.017738\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000529\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001173 | Grad Max: 0.003192\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000024 | Grad Max: 0.000359\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000578 | Grad Max: 0.001775\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002010 | Grad Max: 0.004855\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025886 | Grad Max: 0.025886\n",
      "[GRADIENT NORM TOTAL] 1.2736\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5349426 0.4650573] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.005\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 114/1934 | C: 5/2043\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57398 | C: 0.80099\n",
      "[LOGITS Ex2 A] Mean Abs: 0.704 | Max: 3.946\n",
      "[LOSS Ex2] A: 0.45970 | B: 0.49401 | C: 0.46481\n",
      "** [JOINT LOSS] ** : 0.931165\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002158 | Grad Max: 0.050256\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.029909 | Grad Max: 0.148580\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005283 | Grad Max: 0.032158\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.038156 | Grad Max: 0.038156\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000333 | Grad Max: 0.013603\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004420 | Grad Max: 0.060776\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000071 | Grad Max: 0.001906\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001722 | Grad Max: 0.006844\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000017 | Grad Max: 0.000363\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000501 | Grad Max: 0.002011\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000260\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000235 | Grad Max: 0.001112\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000607 | Grad Max: 0.002556\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008381 | Grad Max: 0.008381\n",
      "[GRADIENT NORM TOTAL] 0.6277\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.036\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5347783 0.4652217] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 94/1762 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57427 | C: 0.69393\n",
      "[LOGITS Ex2 A] Mean Abs: 0.751 | Max: 3.841\n",
      "[LOSS Ex2] A: 0.44440 | B: 0.51014 | C: 0.46565\n",
      "** [JOINT LOSS] ** : 0.896129\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002059 | Grad Max: 0.033582\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033312 | Grad Max: 0.167647\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008388 | Grad Max: 0.049005\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.127751 | Grad Max: 0.127751\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000310 | Grad Max: 0.019296\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004898 | Grad Max: 0.114094\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000068 | Grad Max: 0.002050\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001749 | Grad Max: 0.007226\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000324\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000515 | Grad Max: 0.001979\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000210\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000252 | Grad Max: 0.000960\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000802 | Grad Max: 0.002807\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010729 | Grad Max: 0.010729\n",
      "[GRADIENT NORM TOTAL] 0.6933\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.066\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5352292  0.46477082] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 104/1944 | C: 8/2040\n",
      "[LOSS Ex1] A: 0.80447 | B: 0.57593 | C: 0.72061\n",
      "[LOGITS Ex2 A] Mean Abs: 0.739 | Max: 3.884\n",
      "[LOSS Ex2] A: 0.46201 | B: 0.49581 | C: 0.43367\n",
      "** [JOINT LOSS] ** : 1.164168\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002001 | Grad Max: 0.028956\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.026550 | Grad Max: 0.112636\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009484 | Grad Max: 0.056490\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.097580 | Grad Max: 0.097580\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000237 | Grad Max: 0.010554\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003595 | Grad Max: 0.058604\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000052 | Grad Max: 0.001694\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001326 | Grad Max: 0.005953\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000012 | Grad Max: 0.000276\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000366 | Grad Max: 0.001464\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000206\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000173 | Grad Max: 0.000822\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000580 | Grad Max: 0.002222\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.006815 | Grad Max: 0.006815\n",
      "[GRADIENT NORM TOTAL] 0.5430\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.048\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5348779 0.4651221] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 108/1940 | C: 9/2039\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57816 | C: 0.67814\n",
      "[LOGITS Ex2 A] Mean Abs: 0.742 | Max: 3.778\n",
      "[LOSS Ex2] A: 0.44856 | B: 0.49872 | C: 0.44257\n",
      "** [JOINT LOSS] ** : 0.882048\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001609 | Grad Max: 0.022426\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.019330 | Grad Max: 0.089509\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010230 | Grad Max: 0.044546\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.146047 | Grad Max: 0.146047\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000209 | Grad Max: 0.009952\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002881 | Grad Max: 0.054461\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000046 | Grad Max: 0.001931\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001088 | Grad Max: 0.006898\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000289\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000308 | Grad Max: 0.001545\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000228\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000147 | Grad Max: 0.000784\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000731 | Grad Max: 0.002555\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.004724 | Grad Max: 0.004724\n",
      "[GRADIENT NORM TOTAL] 0.4773\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52258503 0.47741494] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 115/1933 | C: 15/2033\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57378 | C: 0.69828\n",
      "[LOGITS Ex2 A] Mean Abs: 0.775 | Max: 3.909\n",
      "[LOSS Ex2] A: 0.45574 | B: 0.49092 | C: 0.44475\n",
      "** [JOINT LOSS] ** : 0.887820\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003240 | Grad Max: 0.088680\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.029379 | Grad Max: 0.179588\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008547 | Grad Max: 0.038361\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.116039 | Grad Max: 0.116039\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000407 | Grad Max: 0.015933\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004696 | Grad Max: 0.065226\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000084 | Grad Max: 0.002433\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001870 | Grad Max: 0.008227\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000020 | Grad Max: 0.000297\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000568 | Grad Max: 0.001880\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000267\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000276 | Grad Max: 0.001082\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001219 | Grad Max: 0.003436\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.012746 | Grad Max: 0.012746\n",
      "[GRADIENT NORM TOTAL] 0.7752\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5301088  0.46989125] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.529 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 95/1761 | C: 3/1373\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57609 | C: 0.58487\n",
      "[LOGITS Ex2 A] Mean Abs: 0.752 | Max: 4.185\n",
      "[LOSS Ex2] A: 0.46216 | B: 0.50201 | C: 0.45805\n",
      "** [JOINT LOSS] ** : 0.861062\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003178 | Grad Max: 0.079815\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.031713 | Grad Max: 0.163496\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.022179 | Grad Max: 0.087868\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.289641 | Grad Max: 0.289641\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000342 | Grad Max: 0.018762\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004225 | Grad Max: 0.075240\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000070 | Grad Max: 0.002512\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001567 | Grad Max: 0.006760\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000317\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000455 | Grad Max: 0.001857\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000205\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000211 | Grad Max: 0.000979\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000704 | Grad Max: 0.002350\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008547 | Grad Max: 0.008547\n",
      "[GRADIENT NORM TOTAL] 0.8704\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0192\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0321 | Alpha: 0.5500\n",
      "No improve count: 7/15\n",
      "\n",
      "############################## EPOCH 11/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5312908 0.4687092] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 106/1942 | C: 6/2042\n",
      "[LOSS Ex1] A: 0.79853 | B: 0.57562 | C: 0.72914\n",
      "[LOGITS Ex2 A] Mean Abs: 0.773 | Max: 3.885\n",
      "[LOSS Ex2] A: 0.44659 | B: 0.49859 | C: 0.45527\n",
      "** [JOINT LOSS] ** : 1.167916\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003363 | Grad Max: 0.105272\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.115189 | Grad Max: 0.615997\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010668 | Grad Max: 0.035618\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.110718 | Grad Max: 0.110718\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001046 | Grad Max: 0.033988\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.016805 | Grad Max: 0.177769\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000273 | Grad Max: 0.006364\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.007144 | Grad Max: 0.026802\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000064 | Grad Max: 0.000885\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002068 | Grad Max: 0.005671\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000041 | Grad Max: 0.000653\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000961 | Grad Max: 0.003016\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002735 | Grad Max: 0.005858\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.037528 | Grad Max: 0.037528\n",
      "[GRADIENT NORM TOTAL] 2.1841\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5197214  0.48027864] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 109/1939 | C: 9/2039\n",
      "[LOSS Ex1] A: 0.79917 | B: 0.57741 | C: 0.73037\n",
      "[LOGITS Ex2 A] Mean Abs: 0.774 | Max: 3.941\n",
      "[LOSS Ex2] A: 0.44372 | B: 0.49405 | C: 0.43011\n",
      "** [JOINT LOSS] ** : 1.158278\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003568 | Grad Max: 0.110343\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.052322 | Grad Max: 0.277303\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011057 | Grad Max: 0.038161\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.110087 | Grad Max: 0.110087\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000577 | Grad Max: 0.023264\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007605 | Grad Max: 0.091240\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000128 | Grad Max: 0.004120\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003094 | Grad Max: 0.013862\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000030 | Grad Max: 0.000469\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000887 | Grad Max: 0.002594\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000355\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000413 | Grad Max: 0.001533\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001201 | Grad Max: 0.004035\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.015670 | Grad Max: 0.015670\n",
      "[GRADIENT NORM TOTAL] 1.0878\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.062\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5346709  0.46532905] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 116/1932 | C: 11/2037\n",
      "[LOSS Ex1] A: 0.69760 | B: 0.57290 | C: 0.76244\n",
      "[LOGITS Ex2 A] Mean Abs: 0.727 | Max: 3.890\n",
      "[LOSS Ex2] A: 0.44587 | B: 0.49194 | C: 0.47224\n",
      "** [JOINT LOSS] ** : 1.147666\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003814 | Grad Max: 0.101020\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.148265 | Grad Max: 0.831776\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006202 | Grad Max: 0.024503\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.005100 | Grad Max: 0.005100\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001241 | Grad Max: 0.039144\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.021128 | Grad Max: 0.212376\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000333 | Grad Max: 0.007230\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.008929 | Grad Max: 0.032079\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000077 | Grad Max: 0.000995\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002569 | Grad Max: 0.006618\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000050 | Grad Max: 0.000708\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001195 | Grad Max: 0.003368\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003504 | Grad Max: 0.007321\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.047882 | Grad Max: 0.047882\n",
      "[GRADIENT NORM TOTAL] 2.8014\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53506637 0.46493366] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/1615 | B: 99/1757 | C: 9/2039\n",
      "[LOSS Ex1] A: 0.79870 | B: 0.57787 | C: 0.77655\n",
      "[LOGITS Ex2 A] Mean Abs: 0.729 | Max: 4.163\n",
      "[LOSS Ex2] A: 0.44483 | B: 0.51434 | C: 0.46445\n",
      "** [JOINT LOSS] ** : 1.192246\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.006409 | Grad Max: 0.108075\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.171827 | Grad Max: 0.956173\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.028822 | Grad Max: 0.159124\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.191602 | Grad Max: 0.191602\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001447 | Grad Max: 0.039402\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.024339 | Grad Max: 0.227854\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000385 | Grad Max: 0.010123\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.010261 | Grad Max: 0.040937\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000089 | Grad Max: 0.001051\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002960 | Grad Max: 0.007173\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000057 | Grad Max: 0.000791\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001378 | Grad Max: 0.003963\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004178 | Grad Max: 0.008507\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.056622 | Grad Max: 0.056622\n",
      "[GRADIENT NORM TOTAL] 3.2991\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53493357 0.46506643] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 107/1941 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57488 | C: 0.71179\n",
      "[LOGITS Ex2 A] Mean Abs: 0.791 | Max: 4.337\n",
      "[LOSS Ex2] A: 0.42984 | B: 0.48819 | C: 0.44694\n",
      "** [JOINT LOSS] ** : 0.883880\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002238 | Grad Max: 0.041423\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.051776 | Grad Max: 0.300432\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006567 | Grad Max: 0.033975\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.094059 | Grad Max: 0.094059\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000468 | Grad Max: 0.017648\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007567 | Grad Max: 0.107744\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000107 | Grad Max: 0.003140\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002855 | Grad Max: 0.012510\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000024 | Grad Max: 0.000368\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000808 | Grad Max: 0.002492\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000248\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000372 | Grad Max: 0.001199\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001108 | Grad Max: 0.003296\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.015012 | Grad Max: 0.015012\n",
      "[GRADIENT NORM TOTAL] 1.0094\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.067\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5354116  0.46458846] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 111/1937 | C: 9/2039\n",
      "[LOSS Ex1] A: 0.80571 | B: 0.57695 | C: 0.67745\n",
      "[LOGITS Ex2 A] Mean Abs: 0.829 | Max: 3.675\n",
      "[LOSS Ex2] A: 0.45932 | B: 0.52226 | C: 0.47674\n",
      "** [JOINT LOSS] ** : 1.172811\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.006372 | Grad Max: 0.197466\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.219375 | Grad Max: 1.197377\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008160 | Grad Max: 0.038492\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.038327 | Grad Max: 0.038327\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001974 | Grad Max: 0.064707\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.031615 | Grad Max: 0.348311\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000508 | Grad Max: 0.012171\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.013339 | Grad Max: 0.048831\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000118 | Grad Max: 0.001371\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.003813 | Grad Max: 0.009327\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000075 | Grad Max: 0.000957\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001752 | Grad Max: 0.005023\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.005007 | Grad Max: 0.010785\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.067307 | Grad Max: 0.067307\n",
      "[GRADIENT NORM TOTAL] 4.1545\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.049\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5350892  0.46491084] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 117/1931 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.59710 | B: 0.57233 | C: 0.73515\n",
      "[LOGITS Ex2 A] Mean Abs: 0.876 | Max: 3.756\n",
      "[LOSS Ex2] A: 0.49000 | B: 0.53166 | C: 0.46319\n",
      "** [JOINT LOSS] ** : 1.129809\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.010117 | Grad Max: 0.366208\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.347061 | Grad Max: 1.897229\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014944 | Grad Max: 0.063713\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.213606 | Grad Max: 0.213606\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.003189 | Grad Max: 0.110982\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.050251 | Grad Max: 0.550971\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000810 | Grad Max: 0.020265\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.021118 | Grad Max: 0.078663\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000188 | Grad Max: 0.002221\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.006034 | Grad Max: 0.014838\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000119 | Grad Max: 0.001531\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.002766 | Grad Max: 0.007886\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.008117 | Grad Max: 0.017929\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.107377 | Grad Max: 0.107377\n",
      "[GRADIENT NORM TOTAL] 6.6187\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52253336 0.47746664] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 100/1756 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.80024 | B: 0.57738 | C: 0.71704\n",
      "[LOGITS Ex2 A] Mean Abs: 0.857 | Max: 3.763\n",
      "[LOSS Ex2] A: 0.45519 | B: 0.51606 | C: 0.47233\n",
      "** [JOINT LOSS] ** : 1.179413\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.005919 | Grad Max: 0.163400\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.209776 | Grad Max: 1.141140\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017232 | Grad Max: 0.082612\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.101080 | Grad Max: 0.101080\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001819 | Grad Max: 0.065509\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.030125 | Grad Max: 0.343945\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000480 | Grad Max: 0.011323\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.012749 | Grad Max: 0.049153\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000111 | Grad Max: 0.001306\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.003608 | Grad Max: 0.009232\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000069 | Grad Max: 0.000945\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001640 | Grad Max: 0.004920\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004616 | Grad Max: 0.010759\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.063002 | Grad Max: 0.063002\n",
      "[GRADIENT NORM TOTAL] 3.9895\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5304163  0.46958366] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 107/1941 | C: 15/2033\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57420 | C: 0.72857\n",
      "[LOGITS Ex2 A] Mean Abs: 0.798 | Max: 3.945\n",
      "[LOSS Ex2] A: 0.45164 | B: 0.49468 | C: 0.47352\n",
      "** [JOINT LOSS] ** : 0.907534\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.004777 | Grad Max: 0.170974\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.091786 | Grad Max: 0.516182\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004720 | Grad Max: 0.026011\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.072209 | Grad Max: 0.072209\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000985 | Grad Max: 0.035994\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.013824 | Grad Max: 0.145119\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000231 | Grad Max: 0.006741\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005736 | Grad Max: 0.022686\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000053 | Grad Max: 0.000630\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001644 | Grad Max: 0.004128\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000034 | Grad Max: 0.000502\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000760 | Grad Max: 0.002293\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002553 | Grad Max: 0.005471\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.031919 | Grad Max: 0.031919\n",
      "[GRADIENT NORM TOTAL] 1.8630\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53135675 0.46864325] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 114/1934 | C: 17/2031\n",
      "[LOSS Ex1] A: 0.79861 | B: 0.57691 | C: 0.74961\n",
      "[LOGITS Ex2 A] Mean Abs: 0.792 | Max: 4.023\n",
      "[LOSS Ex2] A: 0.44246 | B: 0.49786 | C: 0.44429\n",
      "** [JOINT LOSS] ** : 1.169914\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.005730 | Grad Max: 0.191425\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.165436 | Grad Max: 0.921439\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012680 | Grad Max: 0.047938\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.142967 | Grad Max: 0.142967\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001560 | Grad Max: 0.050915\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.024253 | Grad Max: 0.244080\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000394 | Grad Max: 0.010823\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.010200 | Grad Max: 0.040706\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000090 | Grad Max: 0.001083\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002904 | Grad Max: 0.007210\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000057 | Grad Max: 0.000782\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001332 | Grad Max: 0.003925\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.004090 | Grad Max: 0.008796\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.053021 | Grad Max: 0.053021\n",
      "[GRADIENT NORM TOTAL] 3.1900\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.519539   0.48046097] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 117/1931 | C: 8/2040\n",
      "[LOSS Ex1] A: 0.79912 | B: 0.57176 | C: 0.69161\n",
      "[LOGITS Ex2 A] Mean Abs: 0.796 | Max: 4.106\n",
      "[LOSS Ex2] A: 0.43384 | B: 0.48796 | C: 0.44513\n",
      "** [JOINT LOSS] ** : 1.143141\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003011 | Grad Max: 0.067478\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.022798 | Grad Max: 0.134652\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010321 | Grad Max: 0.041826\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.056474 | Grad Max: 0.056474\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000329 | Grad Max: 0.015641\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003910 | Grad Max: 0.064739\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000069 | Grad Max: 0.002617\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001453 | Grad Max: 0.008270\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000015 | Grad Max: 0.000252\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000403 | Grad Max: 0.001490\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000191\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000185 | Grad Max: 0.000728\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000792 | Grad Max: 0.002833\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008108 | Grad Max: 0.008108\n",
      "[GRADIENT NORM TOTAL] 0.6147\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.063\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5347794 0.4652206] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 100/1756 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.59555 | B: 0.57686 | C: 0.70090\n",
      "[LOGITS Ex2 A] Mean Abs: 0.811 | Max: 3.786\n",
      "[LOSS Ex2] A: 0.46420 | B: 0.51202 | C: 0.44904\n",
      "** [JOINT LOSS] ** : 1.099523\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.005289 | Grad Max: 0.162561\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.164182 | Grad Max: 0.891768\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017524 | Grad Max: 0.079602\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.257353 | Grad Max: 0.257353\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001505 | Grad Max: 0.053001\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.023563 | Grad Max: 0.281619\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000377 | Grad Max: 0.010302\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.009829 | Grad Max: 0.037585\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000086 | Grad Max: 0.001029\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002777 | Grad Max: 0.007036\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000053 | Grad Max: 0.000714\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001254 | Grad Max: 0.003724\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003497 | Grad Max: 0.007225\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.047673 | Grad Max: 0.047673\n",
      "[GRADIENT NORM TOTAL] 3.1569\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351509  0.46484914] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 109/1939 | C: 19/2029\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57407 | C: 0.72108\n",
      "[LOGITS Ex2 A] Mean Abs: 0.813 | Max: 3.937\n",
      "[LOSS Ex2] A: 0.45331 | B: 0.49986 | C: 0.43367\n",
      "** [JOINT LOSS] ** : 0.894000\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.004503 | Grad Max: 0.168680\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.166676 | Grad Max: 0.936819\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005839 | Grad Max: 0.028678\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.083882 | Grad Max: 0.083882\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.001539 | Grad Max: 0.053365\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.024434 | Grad Max: 0.267800\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000385 | Grad Max: 0.010157\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.010080 | Grad Max: 0.038097\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000088 | Grad Max: 0.001100\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.002838 | Grad Max: 0.007139\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000055 | Grad Max: 0.000727\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.001286 | Grad Max: 0.003673\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.003582 | Grad Max: 0.007497\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.049022 | Grad Max: 0.049022\n",
      "[GRADIENT NORM TOTAL] 3.2217\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5350231  0.46497694] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 114/1934 | C: 10/1366\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57632 | C: 0.73666\n",
      "[LOGITS Ex2 A] Mean Abs: 0.811 | Max: 3.974\n",
      "[LOSS Ex2] A: 0.42658 | B: 0.49429 | C: 0.46650\n",
      "** [JOINT LOSS] ** : 0.900113\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002058 | Grad Max: 0.039761\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.036514 | Grad Max: 0.187583\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004149 | Grad Max: 0.025005\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.061858 | Grad Max: 0.061858\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000363 | Grad Max: 0.013903\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005866 | Grad Max: 0.080570\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000072 | Grad Max: 0.002088\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001887 | Grad Max: 0.008429\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000015 | Grad Max: 0.000435\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000527 | Grad Max: 0.002232\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000190\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000248 | Grad Max: 0.000950\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000791 | Grad Max: 0.002294\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010881 | Grad Max: 0.010881\n",
      "[GRADIENT NORM TOTAL] 0.7553\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0819\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0241 | Alpha: 0.5500\n",
      "No improve count: 8/15\n",
      "\n",
      "############################## EPOCH 12/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53550625 0.46449375] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 118/1930 | C: 14/2034\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57129 | C: 0.73922\n",
      "[LOGITS Ex2 A] Mean Abs: 0.776 | Max: 3.902\n",
      "[LOSS Ex2] A: 0.42961 | B: 0.48566 | C: 0.44743\n",
      "** [JOINT LOSS] ** : 1.160020\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002678 | Grad Max: 0.039571\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.069683 | Grad Max: 0.369457\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011129 | Grad Max: 0.063520\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.128286 | Grad Max: 0.128286\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000572 | Grad Max: 0.020718\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009638 | Grad Max: 0.124422\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000146 | Grad Max: 0.003586\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003912 | Grad Max: 0.014335\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000033 | Grad Max: 0.000593\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001106 | Grad Max: 0.003410\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000320\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000501 | Grad Max: 0.001664\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001441 | Grad Max: 0.004109\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.019737 | Grad Max: 0.019737\n",
      "[GRADIENT NORM TOTAL] 1.3271\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53516334 0.46483666] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57695 | C: 0.73172\n",
      "[LOGITS Ex2 A] Mean Abs: 0.774 | Max: 3.825\n",
      "[LOSS Ex2] A: 0.42745 | B: 0.50496 | C: 0.46009\n",
      "** [JOINT LOSS] ** : 1.099161\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002319 | Grad Max: 0.040787\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.071115 | Grad Max: 0.371156\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014531 | Grad Max: 0.061789\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.208474 | Grad Max: 0.208474\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000558 | Grad Max: 0.021591\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009783 | Grad Max: 0.128984\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000133 | Grad Max: 0.003622\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003659 | Grad Max: 0.015964\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000029 | Grad Max: 0.000453\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001039 | Grad Max: 0.002945\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000313\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000477 | Grad Max: 0.001663\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001308 | Grad Max: 0.003186\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.019742 | Grad Max: 0.019742\n",
      "[GRADIENT NORM TOTAL] 1.3633\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52230316 0.47769678] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 14/2034\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57370 | C: 0.75388\n",
      "[LOGITS Ex2 A] Mean Abs: 0.801 | Max: 3.981\n",
      "[LOSS Ex2] A: 0.42583 | B: 0.48252 | C: 0.44213\n",
      "** [JOINT LOSS] ** : 1.159608\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003627 | Grad Max: 0.046988\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.084388 | Grad Max: 0.422204\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.020919 | Grad Max: 0.105321\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.151082 | Grad Max: 0.151082\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000670 | Grad Max: 0.024404\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011499 | Grad Max: 0.142678\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000169 | Grad Max: 0.004695\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004635 | Grad Max: 0.018618\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000038 | Grad Max: 0.000579\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001325 | Grad Max: 0.003847\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000023 | Grad Max: 0.000382\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000605 | Grad Max: 0.001961\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001831 | Grad Max: 0.004891\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024842 | Grad Max: 0.024842\n",
      "[GRADIENT NORM TOTAL] 1.6222\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53039867 0.46960133] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 115/1933 | C: 13/2035\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57632 | C: 0.73451\n",
      "[LOGITS Ex2 A] Mean Abs: 0.791 | Max: 4.205\n",
      "[LOSS Ex2] A: 0.44258 | B: 0.50404 | C: 0.45672\n",
      "** [JOINT LOSS] ** : 0.904724\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002270 | Grad Max: 0.046884\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.090349 | Grad Max: 0.496872\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004510 | Grad Max: 0.027127\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.064481 | Grad Max: 0.064481\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000746 | Grad Max: 0.026228\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.012631 | Grad Max: 0.144677\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000188 | Grad Max: 0.005685\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005086 | Grad Max: 0.022704\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000042 | Grad Max: 0.000604\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001441 | Grad Max: 0.003931\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000389\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000661 | Grad Max: 0.001874\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001926 | Grad Max: 0.004039\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.026711 | Grad Max: 0.026711\n",
      "[GRADIENT NORM TOTAL] 1.6936\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5312773 0.4687227] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 8/2040\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.77352\n",
      "[LOGITS Ex2 A] Mean Abs: 0.777 | Max: 3.757\n",
      "[LOSS Ex2] A: 0.43637 | B: 0.47818 | C: 0.44014\n",
      "** [JOINT LOSS] ** : 0.899836\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002007 | Grad Max: 0.038331\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.073795 | Grad Max: 0.398194\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003891 | Grad Max: 0.018423\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.002956 | Grad Max: 0.002956\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000590 | Grad Max: 0.021480\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010204 | Grad Max: 0.111738\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000156 | Grad Max: 0.004051\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004269 | Grad Max: 0.016146\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000461\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001207 | Grad Max: 0.003256\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000022 | Grad Max: 0.000334\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000552 | Grad Max: 0.001614\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001700 | Grad Max: 0.004907\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022604 | Grad Max: 0.022604\n",
      "[GRADIENT NORM TOTAL] 1.3810\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5194276  0.48057237] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 103/1753 | C: 17/2031\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57694 | C: 0.69957\n",
      "[LOGITS Ex2 A] Mean Abs: 0.783 | Max: 4.255\n",
      "[LOSS Ex2] A: 0.42075 | B: 0.50694 | C: 0.44615\n",
      "** [JOINT LOSS] ** : 1.149747\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002283 | Grad Max: 0.056313\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.090479 | Grad Max: 0.513845\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009867 | Grad Max: 0.038414\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.072204 | Grad Max: 0.072204\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000730 | Grad Max: 0.029953\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.012749 | Grad Max: 0.167174\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000191 | Grad Max: 0.005530\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005214 | Grad Max: 0.021721\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000043 | Grad Max: 0.000577\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001470 | Grad Max: 0.004029\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000027 | Grad Max: 0.000472\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000673 | Grad Max: 0.002174\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001993 | Grad Max: 0.004615\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.027227 | Grad Max: 0.027227\n",
      "[GRADIENT NORM TOTAL] 1.7211\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5348092 0.4651908] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 15/2033\n",
      "[LOSS Ex1] A: 0.59510 | B: 0.57370 | C: 0.74216\n",
      "[LOGITS Ex2 A] Mean Abs: 0.758 | Max: 3.922\n",
      "[LOSS Ex2] A: 0.44091 | B: 0.48543 | C: 0.47260\n",
      "** [JOINT LOSS] ** : 1.103298\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002578 | Grad Max: 0.054661\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.092261 | Grad Max: 0.495845\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013309 | Grad Max: 0.062587\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.199485 | Grad Max: 0.199485\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000727 | Grad Max: 0.022870\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.012625 | Grad Max: 0.127161\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000190 | Grad Max: 0.004466\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005155 | Grad Max: 0.019746\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000043 | Grad Max: 0.000592\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001464 | Grad Max: 0.003975\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000027 | Grad Max: 0.000370\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000676 | Grad Max: 0.001947\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.002175 | Grad Max: 0.005025\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.028598 | Grad Max: 0.028598\n",
      "[GRADIENT NORM TOTAL] 1.7392\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351706  0.46482942] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 115/1933 | C: 12/2036\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57632 | C: 0.74750\n",
      "[LOGITS Ex2 A] Mean Abs: 0.769 | Max: 3.885\n",
      "[LOSS Ex2] A: 0.42640 | B: 0.50769 | C: 0.45779\n",
      "** [JOINT LOSS] ** : 0.905229\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002330 | Grad Max: 0.047328\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.078928 | Grad Max: 0.422459\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005573 | Grad Max: 0.022448\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.045092 | Grad Max: 0.045092\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000640 | Grad Max: 0.020211\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010991 | Grad Max: 0.115933\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000167 | Grad Max: 0.004722\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004526 | Grad Max: 0.018403\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000037 | Grad Max: 0.000475\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001282 | Grad Max: 0.003363\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000023 | Grad Max: 0.000422\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000594 | Grad Max: 0.002018\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001871 | Grad Max: 0.004321\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025104 | Grad Max: 0.025104\n",
      "[GRADIENT NORM TOTAL] 1.4717\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5350382  0.46496186] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 22/2026\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.74153\n",
      "[LOGITS Ex2 A] Mean Abs: 0.813 | Max: 4.053\n",
      "[LOSS Ex2] A: 0.42001 | B: 0.48368 | C: 0.44827\n",
      "** [JOINT LOSS] ** : 0.888260\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001842 | Grad Max: 0.040219\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.070681 | Grad Max: 0.391845\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003966 | Grad Max: 0.020817\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.051744 | Grad Max: 0.051744\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000573 | Grad Max: 0.019263\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009933 | Grad Max: 0.110538\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000147 | Grad Max: 0.004342\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004033 | Grad Max: 0.016972\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000033 | Grad Max: 0.000485\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001140 | Grad Max: 0.003082\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000327\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000519 | Grad Max: 0.001567\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001585 | Grad Max: 0.004621\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021411 | Grad Max: 0.021411\n",
      "[GRADIENT NORM TOTAL] 1.3299\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5355063  0.46449372] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 11/2037\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57694 | C: 0.78176\n",
      "[LOGITS Ex2 A] Mean Abs: 0.779 | Max: 3.891\n",
      "[LOSS Ex2] A: 0.44279 | B: 0.50802 | C: 0.46329\n",
      "** [JOINT LOSS] ** : 1.193214\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002905 | Grad Max: 0.051193\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.073809 | Grad Max: 0.405459\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015814 | Grad Max: 0.081787\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.199508 | Grad Max: 0.199508\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000607 | Grad Max: 0.023422\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010235 | Grad Max: 0.108035\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000155 | Grad Max: 0.004440\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004140 | Grad Max: 0.018057\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000491\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001170 | Grad Max: 0.003354\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000396\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000534 | Grad Max: 0.001967\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001545 | Grad Max: 0.003557\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021885 | Grad Max: 0.021885\n",
      "[GRADIENT NORM TOTAL] 1.4419\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53516334 0.46483666] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 22/2026\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57370 | C: 0.73449\n",
      "[LOGITS Ex2 A] Mean Abs: 0.770 | Max: 4.004\n",
      "[LOSS Ex2] A: 0.43510 | B: 0.48819 | C: 0.44726\n",
      "** [JOINT LOSS] ** : 1.091680\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002546 | Grad Max: 0.047202\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.075331 | Grad Max: 0.371577\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015099 | Grad Max: 0.067264\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.217408 | Grad Max: 0.217408\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000592 | Grad Max: 0.017562\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010325 | Grad Max: 0.112562\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000142 | Grad Max: 0.002852\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003913 | Grad Max: 0.012925\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000031 | Grad Max: 0.000435\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001110 | Grad Max: 0.002945\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000019 | Grad Max: 0.000306\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000510 | Grad Max: 0.001668\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001481 | Grad Max: 0.003912\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021392 | Grad Max: 0.021392\n",
      "[GRADIENT NORM TOTAL] 1.4349\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52230275 0.4776973 ] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57631 | C: 0.70208\n",
      "[LOGITS Ex2 A] Mean Abs: 0.799 | Max: 4.056\n",
      "[LOSS Ex2] A: 0.42794 | B: 0.50618 | C: 0.43334\n",
      "** [JOINT LOSS] ** : 1.148871\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003209 | Grad Max: 0.042300\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.085946 | Grad Max: 0.449597\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.016155 | Grad Max: 0.078727\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.077264 | Grad Max: 0.077264\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000696 | Grad Max: 0.024640\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011977 | Grad Max: 0.147151\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000180 | Grad Max: 0.004408\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004880 | Grad Max: 0.019945\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000040 | Grad Max: 0.000604\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001393 | Grad Max: 0.004071\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000024 | Grad Max: 0.000359\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000637 | Grad Max: 0.001887\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001852 | Grad Max: 0.004548\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.025995 | Grad Max: 0.025995\n",
      "[GRADIENT NORM TOTAL] 1.6334\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5303984  0.46960157] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 19/2029\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.74370\n",
      "[LOGITS Ex2 A] Mean Abs: 0.771 | Max: 4.161\n",
      "[LOSS Ex2] A: 0.44532 | B: 0.47950 | C: 0.45518\n",
      "** [JOINT LOSS] ** : 0.898328\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002902 | Grad Max: 0.058902\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.071080 | Grad Max: 0.368506\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003919 | Grad Max: 0.023583\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.048507 | Grad Max: 0.048507\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000634 | Grad Max: 0.021300\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010244 | Grad Max: 0.121800\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000161 | Grad Max: 0.004889\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004242 | Grad Max: 0.018072\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000036 | Grad Max: 0.000496\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001200 | Grad Max: 0.003178\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000023 | Grad Max: 0.000352\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000551 | Grad Max: 0.001632\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001791 | Grad Max: 0.004428\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022990 | Grad Max: 0.022990\n",
      "[GRADIENT NORM TOTAL] 1.3472\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53127694 0.46872303] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 103/1753 | C: 7/1369\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.76885\n",
      "[LOGITS Ex2 A] Mean Abs: 0.774 | Max: 3.866\n",
      "[LOSS Ex2] A: 0.42517 | B: 0.50784 | C: 0.43172\n",
      "** [JOINT LOSS] ** : 0.903507\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001970 | Grad Max: 0.043060\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.079572 | Grad Max: 0.433935\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003531 | Grad Max: 0.016145\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.002444 | Grad Max: 0.002444\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000630 | Grad Max: 0.021964\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011076 | Grad Max: 0.126589\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000165 | Grad Max: 0.004622\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004491 | Grad Max: 0.015315\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000036 | Grad Max: 0.000517\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001273 | Grad Max: 0.003422\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000022 | Grad Max: 0.000356\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000582 | Grad Max: 0.002117\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001664 | Grad Max: 0.004654\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023209 | Grad Max: 0.023209\n",
      "[GRADIENT NORM TOTAL] 1.4857\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0361\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0177 | Alpha: 0.5500\n",
      "No improve count: 9/15\n",
      "\n",
      "############################## EPOCH 13/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5194273 0.4805727] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 109/1939 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57370 | C: 0.75975\n",
      "[LOGITS Ex2 A] Mean Abs: 0.781 | Max: 3.989\n",
      "[LOSS Ex2] A: 0.41666 | B: 0.48965 | C: 0.45708\n",
      "** [JOINT LOSS] ** : 1.165245\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002164 | Grad Max: 0.063410\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.087709 | Grad Max: 0.492541\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013535 | Grad Max: 0.049218\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.159580 | Grad Max: 0.159580\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000706 | Grad Max: 0.024843\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.012369 | Grad Max: 0.142094\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000189 | Grad Max: 0.005066\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.005127 | Grad Max: 0.021159\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000042 | Grad Max: 0.000602\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001444 | Grad Max: 0.003998\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000026 | Grad Max: 0.000409\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000660 | Grad Max: 0.002126\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001992 | Grad Max: 0.004667\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.027167 | Grad Max: 0.027167\n",
      "[GRADIENT NORM TOTAL] 1.6874\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53480923 0.4651908 ] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.59509 | B: 0.57631 | C: 0.75269\n",
      "[LOGITS Ex2 A] Mean Abs: 0.760 | Max: 3.877\n",
      "[LOSS Ex2] A: 0.42092 | B: 0.49893 | C: 0.43537\n",
      "** [JOINT LOSS] ** : 1.093105\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002122 | Grad Max: 0.044956\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.077617 | Grad Max: 0.401789\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012227 | Grad Max: 0.057648\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.188130 | Grad Max: 0.188130\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000597 | Grad Max: 0.025219\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010566 | Grad Max: 0.154875\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000150 | Grad Max: 0.003571\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004136 | Grad Max: 0.015759\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000033 | Grad Max: 0.000495\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001163 | Grad Max: 0.003400\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000020 | Grad Max: 0.000340\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000532 | Grad Max: 0.001680\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001545 | Grad Max: 0.004191\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021780 | Grad Max: 0.021780\n",
      "[GRADIENT NORM TOTAL] 1.4648\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53517056 0.46482947] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 118/1930 | C: 14/2034\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.73879\n",
      "[LOGITS Ex2 A] Mean Abs: 0.770 | Max: 4.232\n",
      "[LOSS Ex2] A: 0.43523 | B: 0.48241 | C: 0.44671\n",
      "** [JOINT LOSS] ** : 0.891474\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002066 | Grad Max: 0.029097\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.028056 | Grad Max: 0.136190\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004597 | Grad Max: 0.022806\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.055674 | Grad Max: 0.055674\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000286 | Grad Max: 0.011503\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004376 | Grad Max: 0.063197\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.002247\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001642 | Grad Max: 0.008085\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000304\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000476 | Grad Max: 0.001841\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000187\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000224 | Grad Max: 0.000930\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000843 | Grad Max: 0.003105\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009971 | Grad Max: 0.009971\n",
      "[GRADIENT NORM TOTAL] 0.5823\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5350381  0.46496192] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 103/1753 | C: 14/2034\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.72271\n",
      "[LOGITS Ex2 A] Mean Abs: 0.816 | Max: 4.009\n",
      "[LOSS Ex2] A: 0.41787 | B: 0.50302 | C: 0.44926\n",
      "** [JOINT LOSS] ** : 0.889932\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001792 | Grad Max: 0.044380\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.078901 | Grad Max: 0.427635\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004941 | Grad Max: 0.028162\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.073564 | Grad Max: 0.073564\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000607 | Grad Max: 0.023162\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010805 | Grad Max: 0.130434\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000155 | Grad Max: 0.004002\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004251 | Grad Max: 0.016350\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000034 | Grad Max: 0.000596\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001200 | Grad Max: 0.003596\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000331\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000552 | Grad Max: 0.001600\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001677 | Grad Max: 0.004058\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.023075 | Grad Max: 0.023075\n",
      "[GRADIENT NORM TOTAL] 1.4723\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53550625 0.46449372] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 12/2036\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57370 | C: 0.74643\n",
      "[LOGITS Ex2 A] Mean Abs: 0.778 | Max: 3.859\n",
      "[LOSS Ex2] A: 0.43477 | B: 0.48192 | C: 0.43618\n",
      "** [JOINT LOSS] ** : 1.159944\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002645 | Grad Max: 0.042558\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.055353 | Grad Max: 0.288086\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011748 | Grad Max: 0.067308\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.139864 | Grad Max: 0.139864\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000475 | Grad Max: 0.019130\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007757 | Grad Max: 0.099397\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000116 | Grad Max: 0.003595\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003118 | Grad Max: 0.014295\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000425\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000890 | Grad Max: 0.002546\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000280\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000408 | Grad Max: 0.001421\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001241 | Grad Max: 0.003723\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016907 | Grad Max: 0.016907\n",
      "[GRADIENT NORM TOTAL] 1.0887\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53516334 0.46483666] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 22/2026\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57631 | C: 0.76033\n",
      "[LOGITS Ex2 A] Mean Abs: 0.771 | Max: 3.888\n",
      "[LOSS Ex2] A: 0.43027 | B: 0.49779 | C: 0.45010\n",
      "** [JOINT LOSS] ** : 1.103701\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002410 | Grad Max: 0.051493\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.085256 | Grad Max: 0.438667\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011406 | Grad Max: 0.049509\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.172128 | Grad Max: 0.172128\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000673 | Grad Max: 0.025817\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.011831 | Grad Max: 0.156200\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000162 | Grad Max: 0.004564\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004507 | Grad Max: 0.018696\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000455\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001270 | Grad Max: 0.003321\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000022 | Grad Max: 0.000325\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000583 | Grad Max: 0.001663\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001691 | Grad Max: 0.004144\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.024498 | Grad Max: 0.024498\n",
      "[GRADIENT NORM TOTAL] 1.6063\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.522302   0.47769803] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 118/1930 | C: 11/2037\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57129 | C: 0.72469\n",
      "[LOGITS Ex2 A] Mean Abs: 0.790 | Max: 3.904\n",
      "[LOSS Ex2] A: 0.43866 | B: 0.48235 | C: 0.45983\n",
      "** [JOINT LOSS] ** : 1.159190\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002915 | Grad Max: 0.035107\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.056243 | Grad Max: 0.297418\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017094 | Grad Max: 0.084763\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.101200 | Grad Max: 0.101200\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000460 | Grad Max: 0.016259\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007689 | Grad Max: 0.094513\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000114 | Grad Max: 0.003246\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003049 | Grad Max: 0.011813\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000455\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000860 | Grad Max: 0.002626\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000299\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000398 | Grad Max: 0.001276\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001341 | Grad Max: 0.003240\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016744 | Grad Max: 0.016744\n",
      "[GRADIENT NORM TOTAL] 1.0926\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5303981  0.46960196] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 103/1753 | C: 18/2030\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.73849\n",
      "[LOGITS Ex2 A] Mean Abs: 0.775 | Max: 3.991\n",
      "[LOSS Ex2] A: 0.44541 | B: 0.50353 | C: 0.45011\n",
      "** [JOINT LOSS] ** : 0.904827\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001842 | Grad Max: 0.042530\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.074949 | Grad Max: 0.418960\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004159 | Grad Max: 0.022605\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.047098 | Grad Max: 0.047098\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000597 | Grad Max: 0.021809\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010457 | Grad Max: 0.125711\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000156 | Grad Max: 0.004988\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004233 | Grad Max: 0.019309\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000035 | Grad Max: 0.000549\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001188 | Grad Max: 0.003449\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000294\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000538 | Grad Max: 0.001610\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001485 | Grad Max: 0.003954\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.021434 | Grad Max: 0.021434\n",
      "[GRADIENT NORM TOTAL] 1.4028\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53127646 0.46872357] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 109/1939 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57370 | C: 0.67153\n",
      "[LOGITS Ex2 A] Mean Abs: 0.775 | Max: 3.840\n",
      "[LOSS Ex2] A: 0.43112 | B: 0.49180 | C: 0.46667\n",
      "** [JOINT LOSS] ** : 0.878269\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002352 | Grad Max: 0.038534\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.061635 | Grad Max: 0.319105\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011567 | Grad Max: 0.058233\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.160692 | Grad Max: 0.160692\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000492 | Grad Max: 0.017661\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008400 | Grad Max: 0.097333\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000126 | Grad Max: 0.003859\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003421 | Grad Max: 0.013586\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000028 | Grad Max: 0.000394\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000973 | Grad Max: 0.002762\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000257\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000453 | Grad Max: 0.001337\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001405 | Grad Max: 0.003615\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.019554 | Grad Max: 0.019554\n",
      "[GRADIENT NORM TOTAL] 1.1759\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5194269 0.4805731] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 115/1933 | C: 12/2036\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57631 | C: 0.71138\n",
      "[LOGITS Ex2 A] Mean Abs: 0.780 | Max: 3.882\n",
      "[LOSS Ex2] A: 0.42105 | B: 0.49745 | C: 0.45045\n",
      "** [JOINT LOSS] ** : 1.151843\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001890 | Grad Max: 0.049926\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.075672 | Grad Max: 0.418189\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.009113 | Grad Max: 0.032574\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.082900 | Grad Max: 0.082900\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000603 | Grad Max: 0.023054\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.010574 | Grad Max: 0.126025\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000155 | Grad Max: 0.004372\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.004252 | Grad Max: 0.019542\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000034 | Grad Max: 0.000494\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001194 | Grad Max: 0.003244\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000021 | Grad Max: 0.000391\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000546 | Grad Max: 0.001904\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001628 | Grad Max: 0.004587\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.022492 | Grad Max: 0.022492\n",
      "[GRADIENT NORM TOTAL] 1.4297\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5348092 0.4651908] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 118/1930 | C: 24/2024\n",
      "[LOSS Ex1] A: 0.59509 | B: 0.57129 | C: 0.73695\n",
      "[LOGITS Ex2 A] Mean Abs: 0.758 | Max: 4.059\n",
      "[LOSS Ex2] A: 0.44177 | B: 0.48123 | C: 0.45636\n",
      "** [JOINT LOSS] ** : 1.094229\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002155 | Grad Max: 0.042095\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.047274 | Grad Max: 0.212081\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013872 | Grad Max: 0.066760\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.207322 | Grad Max: 0.207322\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000375 | Grad Max: 0.014337\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006322 | Grad Max: 0.091924\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000086 | Grad Max: 0.002845\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002330 | Grad Max: 0.009082\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000019 | Grad Max: 0.000403\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000655 | Grad Max: 0.002185\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000209\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000301 | Grad Max: 0.001247\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000973 | Grad Max: 0.002783\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013149 | Grad Max: 0.013149\n",
      "[GRADIENT NORM TOTAL] 0.9391\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53517044 0.46482953] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 103/1753 | C: 14/2034\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.73910\n",
      "[LOGITS Ex2 A] Mean Abs: 0.764 | Max: 3.974\n",
      "[LOSS Ex2] A: 0.42882 | B: 0.50418 | C: 0.44198\n",
      "** [JOINT LOSS] ** : 0.897007\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001909 | Grad Max: 0.035472\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.062142 | Grad Max: 0.340696\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003989 | Grad Max: 0.026491\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.049850 | Grad Max: 0.049850\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000515 | Grad Max: 0.019655\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008772 | Grad Max: 0.105066\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000129 | Grad Max: 0.003309\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003518 | Grad Max: 0.015347\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000028 | Grad Max: 0.000468\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000993 | Grad Max: 0.002909\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000302\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000454 | Grad Max: 0.001483\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001276 | Grad Max: 0.003747\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017911 | Grad Max: 0.017911\n",
      "[GRADIENT NORM TOTAL] 1.1648\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53503805 0.46496198] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 109/1939 | C: 19/2029\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57370 | C: 0.75594\n",
      "[LOGITS Ex2 A] Mean Abs: 0.808 | Max: 3.904\n",
      "[LOSS Ex2] A: 0.41912 | B: 0.48686 | C: 0.46353\n",
      "** [JOINT LOSS] ** : 0.899719\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001811 | Grad Max: 0.039901\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.057254 | Grad Max: 0.327328\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003323 | Grad Max: 0.014245\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.031838 | Grad Max: 0.031838\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000473 | Grad Max: 0.018253\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008177 | Grad Max: 0.095129\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000116 | Grad Max: 0.003047\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003197 | Grad Max: 0.013798\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000026 | Grad Max: 0.000411\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000895 | Grad Max: 0.002744\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000016 | Grad Max: 0.000239\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000409 | Grad Max: 0.001228\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001273 | Grad Max: 0.003213\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017322 | Grad Max: 0.017322\n",
      "[GRADIENT NORM TOTAL] 1.0939\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5355063  0.46449363] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 7/1369\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57631 | C: 0.73720\n",
      "[LOGITS Ex2 A] Mean Abs: 0.777 | Max: 4.051\n",
      "[LOSS Ex2] A: 0.43630 | B: 0.49262 | C: 0.45703\n",
      "** [JOINT LOSS] ** : 1.168769\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002390 | Grad Max: 0.043355\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.054053 | Grad Max: 0.279068\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.012167 | Grad Max: 0.064924\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.127817 | Grad Max: 0.127817\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000466 | Grad Max: 0.016851\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007668 | Grad Max: 0.102384\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000111 | Grad Max: 0.003503\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002927 | Grad Max: 0.011990\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000514\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000833 | Grad Max: 0.002761\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000239\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000379 | Grad Max: 0.001278\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001130 | Grad Max: 0.003554\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.015411 | Grad Max: 0.015411\n",
      "[GRADIENT NORM TOTAL] 1.0647\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0327\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0093 | Alpha: 0.5500\n",
      "No improve count: 10/15\n",
      "\n",
      "############################## EPOCH 14/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351634 0.4648366] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 118/1930 | C: 17/2031\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57129 | C: 0.78884\n",
      "[LOGITS Ex2 A] Mean Abs: 0.776 | Max: 3.761\n",
      "[LOSS Ex2] A: 0.41894 | B: 0.48493 | C: 0.45623\n",
      "** [JOINT LOSS] ** : 1.105514\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002722 | Grad Max: 0.083080\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033397 | Grad Max: 0.131179\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008679 | Grad Max: 0.037627\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.130696 | Grad Max: 0.130696\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000363 | Grad Max: 0.013994\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005134 | Grad Max: 0.086584\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000060 | Grad Max: 0.002430\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001312 | Grad Max: 0.007031\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000204\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000345 | Grad Max: 0.001355\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000151\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000162 | Grad Max: 0.000700\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000608 | Grad Max: 0.002302\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007499 | Grad Max: 0.007499\n",
      "[GRADIENT NORM TOTAL] 0.7465\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5223014  0.47769868] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 15/2033\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57694 | C: 0.74580\n",
      "[LOGITS Ex2 A] Mean Abs: 0.799 | Max: 3.997\n",
      "[LOSS Ex2] A: 0.42863 | B: 0.49960 | C: 0.45945\n",
      "** [JOINT LOSS] ** : 1.170394\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003141 | Grad Max: 0.037058\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.064221 | Grad Max: 0.312909\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.019370 | Grad Max: 0.095474\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.139618 | Grad Max: 0.139618\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000493 | Grad Max: 0.019116\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008585 | Grad Max: 0.118621\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000116 | Grad Max: 0.003297\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003183 | Grad Max: 0.011828\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000394\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000903 | Grad Max: 0.002682\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000241\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000414 | Grad Max: 0.001396\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001189 | Grad Max: 0.003329\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016882 | Grad Max: 0.016882\n",
      "[GRADIENT NORM TOTAL] 1.2373\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5303978 0.4696022] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 109/1939 | C: 22/2026\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57370 | C: 0.75189\n",
      "[LOGITS Ex2 A] Mean Abs: 0.786 | Max: 3.922\n",
      "[LOSS Ex2] A: 0.44420 | B: 0.49019 | C: 0.46020\n",
      "** [JOINT LOSS] ** : 0.906723\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001574 | Grad Max: 0.034684\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.052223 | Grad Max: 0.277659\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003287 | Grad Max: 0.017210\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.037031 | Grad Max: 0.037031\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000430 | Grad Max: 0.015408\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007393 | Grad Max: 0.093021\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000102 | Grad Max: 0.003060\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002829 | Grad Max: 0.011530\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000022 | Grad Max: 0.000406\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000807 | Grad Max: 0.002661\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000014 | Grad Max: 0.000247\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000376 | Grad Max: 0.001211\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001174 | Grad Max: 0.003028\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.016362 | Grad Max: 0.016362\n",
      "[GRADIENT NORM TOTAL] 0.9861\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53127617 0.46872383] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 115/1933 | C: 10/2038\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57631 | C: 0.75516\n",
      "[LOGITS Ex2 A] Mean Abs: 0.779 | Max: 4.049\n",
      "[LOSS Ex2] A: 0.43148 | B: 0.49792 | C: 0.45524\n",
      "** [JOINT LOSS] ** : 0.905373\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001902 | Grad Max: 0.038051\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.067081 | Grad Max: 0.352792\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004311 | Grad Max: 0.015120\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.027822 | Grad Max: 0.027822\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000540 | Grad Max: 0.019829\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009479 | Grad Max: 0.121785\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000138 | Grad Max: 0.004151\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003796 | Grad Max: 0.016258\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000030 | Grad Max: 0.000427\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001074 | Grad Max: 0.003047\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000019 | Grad Max: 0.000315\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000494 | Grad Max: 0.001594\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001529 | Grad Max: 0.004023\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.020989 | Grad Max: 0.020989\n",
      "[GRADIENT NORM TOTAL] 1.2686\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.51942676 0.48057324] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 118/1930 | C: 11/2037\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57129 | C: 0.72192\n",
      "[LOGITS Ex2 A] Mean Abs: 0.776 | Max: 3.994\n",
      "[LOSS Ex2] A: 0.43498 | B: 0.48565 | C: 0.45900\n",
      "** [JOINT LOSS] ** : 1.157241\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002030 | Grad Max: 0.039557\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.046392 | Grad Max: 0.265764\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.010900 | Grad Max: 0.041833\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.101555 | Grad Max: 0.101555\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000389 | Grad Max: 0.015369\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006478 | Grad Max: 0.068933\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000099 | Grad Max: 0.003597\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002705 | Grad Max: 0.011714\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000022 | Grad Max: 0.000375\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000762 | Grad Max: 0.002499\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000013 | Grad Max: 0.000238\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000346 | Grad Max: 0.001204\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000963 | Grad Max: 0.002998\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013554 | Grad Max: 0.013554\n",
      "[GRADIENT NORM TOTAL] 0.9005\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5348093  0.46519074] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 18/2030\n",
      "[LOSS Ex1] A: 0.59509 | B: 0.57694 | C: 0.71582\n",
      "[LOGITS Ex2 A] Mean Abs: 0.758 | Max: 3.965\n",
      "[LOSS Ex2] A: 0.44074 | B: 0.50077 | C: 0.43031\n",
      "** [JOINT LOSS] ** : 1.086557\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002250 | Grad Max: 0.049172\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.044628 | Grad Max: 0.206872\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015212 | Grad Max: 0.074440\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.233512 | Grad Max: 0.233512\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000354 | Grad Max: 0.017200\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006007 | Grad Max: 0.104937\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000077 | Grad Max: 0.002462\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002051 | Grad Max: 0.009528\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000326\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000572 | Grad Max: 0.001790\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000195\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000262 | Grad Max: 0.000957\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000790 | Grad Max: 0.002951\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010375 | Grad Max: 0.010375\n",
      "[GRADIENT NORM TOTAL] 0.9294\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53517044 0.46482953] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 109/1939 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57370 | C: 0.75953\n",
      "[LOGITS Ex2 A] Mean Abs: 0.757 | Max: 3.766\n",
      "[LOSS Ex2] A: 0.42965 | B: 0.49210 | C: 0.44425\n",
      "** [JOINT LOSS] ** : 0.899739\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001804 | Grad Max: 0.028488\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.037124 | Grad Max: 0.189371\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003911 | Grad Max: 0.014297\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.022070 | Grad Max: 0.022070\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000333 | Grad Max: 0.011105\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005379 | Grad Max: 0.062612\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000076 | Grad Max: 0.002539\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002015 | Grad Max: 0.008812\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000016 | Grad Max: 0.000346\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000579 | Grad Max: 0.001838\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000010 | Grad Max: 0.000207\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000267 | Grad Max: 0.000997\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000950 | Grad Max: 0.003080\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.012163 | Grad Max: 0.012163\n",
      "[GRADIENT NORM TOTAL] 0.7164\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.535038   0.46496198] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 115/1933 | C: 11/2037\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57631 | C: 0.75997\n",
      "[LOGITS Ex2 A] Mean Abs: 0.812 | Max: 3.879\n",
      "[LOSS Ex2] A: 0.42423 | B: 0.50216 | C: 0.43538\n",
      "** [JOINT LOSS] ** : 0.899353\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001676 | Grad Max: 0.034536\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.064001 | Grad Max: 0.343875\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003684 | Grad Max: 0.019192\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.022335 | Grad Max: 0.022335\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000511 | Grad Max: 0.021258\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.008989 | Grad Max: 0.112501\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000129 | Grad Max: 0.003150\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003593 | Grad Max: 0.014014\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000029 | Grad Max: 0.000434\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001014 | Grad Max: 0.002848\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000017 | Grad Max: 0.000286\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000462 | Grad Max: 0.001410\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001276 | Grad Max: 0.003780\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018610 | Grad Max: 0.018610\n",
      "[GRADIENT NORM TOTAL] 1.2001\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53550637 0.46449363] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 118/1930 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57129 | C: 0.74629\n",
      "[LOGITS Ex2 A] Mean Abs: 0.788 | Max: 3.843\n",
      "[LOSS Ex2] A: 0.42726 | B: 0.48224 | C: 0.46890\n",
      "** [JOINT LOSS] ** : 1.167604\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002276 | Grad Max: 0.040083\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.019654 | Grad Max: 0.092890\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.011702 | Grad Max: 0.067152\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.140419 | Grad Max: 0.140419\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000224 | Grad Max: 0.008875\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003031 | Grad Max: 0.044949\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000045 | Grad Max: 0.002068\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001013 | Grad Max: 0.005085\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000240\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000294 | Grad Max: 0.001329\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000162\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000140 | Grad Max: 0.000626\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000638 | Grad Max: 0.002170\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007314 | Grad Max: 0.007314\n",
      "[GRADIENT NORM TOTAL] 0.5249\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351634  0.46483657] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 13/2035\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57694 | C: 0.73249\n",
      "[LOGITS Ex2 A] Mean Abs: 0.781 | Max: 4.173\n",
      "[LOSS Ex2] A: 0.43953 | B: 0.49663 | C: 0.47647\n",
      "** [JOINT LOSS] ** : 1.106121\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002579 | Grad Max: 0.059555\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.044340 | Grad Max: 0.207859\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014514 | Grad Max: 0.061617\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.206814 | Grad Max: 0.206814\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000397 | Grad Max: 0.015537\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006320 | Grad Max: 0.095720\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000075 | Grad Max: 0.002204\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001908 | Grad Max: 0.007800\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000015 | Grad Max: 0.000324\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000515 | Grad Max: 0.001993\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000172\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000238 | Grad Max: 0.000846\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000711 | Grad Max: 0.001993\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010999 | Grad Max: 0.010999\n",
      "[GRADIENT NORM TOTAL] 0.9391\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5223006 0.4776994] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 22/2026\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57370 | C: 0.71363\n",
      "[LOGITS Ex2 A] Mean Abs: 0.800 | Max: 3.889\n",
      "[LOSS Ex2] A: 0.43269 | B: 0.49025 | C: 0.45385\n",
      "** [JOINT LOSS] ** : 1.154956\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002818 | Grad Max: 0.037701\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.033436 | Grad Max: 0.149458\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.017732 | Grad Max: 0.080943\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.086043 | Grad Max: 0.086043\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000291 | Grad Max: 0.011811\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004540 | Grad Max: 0.069063\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000058 | Grad Max: 0.001800\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001403 | Grad Max: 0.006329\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000012 | Grad Max: 0.000253\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000383 | Grad Max: 0.001615\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000169\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000178 | Grad Max: 0.000859\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000581 | Grad Max: 0.002065\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008237 | Grad Max: 0.008237\n",
      "[GRADIENT NORM TOTAL] 0.7254\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5303975  0.46960253] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 115/1933 | C: 17/2031\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57631 | C: 0.72544\n",
      "[LOGITS Ex2 A] Mean Abs: 0.783 | Max: 3.948\n",
      "[LOSS Ex2] A: 0.43332 | B: 0.49097 | C: 0.44614\n",
      "** [JOINT LOSS] ** : 0.890728\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002045 | Grad Max: 0.033126\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.064003 | Grad Max: 0.345642\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.005853 | Grad Max: 0.027510\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.079101 | Grad Max: 0.079101\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000535 | Grad Max: 0.021434\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.009053 | Grad Max: 0.108882\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000132 | Grad Max: 0.003395\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003586 | Grad Max: 0.014051\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000030 | Grad Max: 0.000448\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.001019 | Grad Max: 0.002841\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000018 | Grad Max: 0.000271\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000464 | Grad Max: 0.001492\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001397 | Grad Max: 0.004157\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.018923 | Grad Max: 0.018923\n",
      "[GRADIENT NORM TOTAL] 1.1982\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53127575 0.46872428] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 17/2031\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.69648\n",
      "[LOGITS Ex2 A] Mean Abs: 0.784 | Max: 3.816\n",
      "[LOSS Ex2] A: 0.43014 | B: 0.48221 | C: 0.43675\n",
      "** [JOINT LOSS] ** : 0.872294\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002000 | Grad Max: 0.027367\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.038876 | Grad Max: 0.214597\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008449 | Grad Max: 0.048105\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.117015 | Grad Max: 0.117015\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000340 | Grad Max: 0.012748\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005680 | Grad Max: 0.078135\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000081 | Grad Max: 0.003428\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002168 | Grad Max: 0.013900\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000018 | Grad Max: 0.000378\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000611 | Grad Max: 0.002150\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000227\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000286 | Grad Max: 0.001042\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001127 | Grad Max: 0.003517\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013197 | Grad Max: 0.013197\n",
      "[GRADIENT NORM TOTAL] 0.7818\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5194264  0.48057353] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 103/1753 | C: 6/1370\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57694 | C: 0.76563\n",
      "[LOGITS Ex2 A] Mean Abs: 0.779 | Max: 3.915\n",
      "[LOSS Ex2] A: 0.41591 | B: 0.50650 | C: 0.43758\n",
      "** [JOINT LOSS] ** : 1.167151\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002002 | Grad Max: 0.050258\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.057983 | Grad Max: 0.326561\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015229 | Grad Max: 0.051288\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.172918 | Grad Max: 0.172918\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000458 | Grad Max: 0.015071\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.007868 | Grad Max: 0.091679\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000114 | Grad Max: 0.003773\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.003112 | Grad Max: 0.015680\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000025 | Grad Max: 0.000431\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000866 | Grad Max: 0.002819\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000015 | Grad Max: 0.000260\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000400 | Grad Max: 0.001317\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.001255 | Grad Max: 0.003614\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.017036 | Grad Max: 0.017036\n",
      "[GRADIENT NORM TOTAL] 1.1294\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0350\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0184 | Alpha: 0.5500\n",
      "No improve count: 11/15\n",
      "\n",
      "############################## EPOCH 15/15 START ##############################\n",
      "\n",
      ">>> [TRAIN] BATCH 0 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53480935 0.46519065] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 16/2032\n",
      "[LOSS Ex1] A: 0.59509 | B: 0.57370 | C: 0.73255\n",
      "[LOGITS Ex2 A] Mean Abs: 0.764 | Max: 3.874\n",
      "[LOSS Ex2] A: 0.43826 | B: 0.49022 | C: 0.46656\n",
      "** [JOINT LOSS] ** : 1.098795\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002318 | Grad Max: 0.046085\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.035366 | Grad Max: 0.159188\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014020 | Grad Max: 0.067661\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.214992 | Grad Max: 0.214992\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000304 | Grad Max: 0.013619\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004838 | Grad Max: 0.082590\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000058 | Grad Max: 0.002350\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001463 | Grad Max: 0.007905\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000012 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000413 | Grad Max: 0.001666\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000007 | Grad Max: 0.000148\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000194 | Grad Max: 0.000781\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000609 | Grad Max: 0.002160\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008843 | Grad Max: 0.008843\n",
      "[GRADIENT NORM TOTAL] 0.7873\n",
      "\n",
      ">>> [TRAIN] BATCH 1 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351705 0.4648295] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 115/1933 | C: 19/2029\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57631 | C: 0.73350\n",
      "[LOGITS Ex2 A] Mean Abs: 0.771 | Max: 3.904\n",
      "[LOSS Ex2] A: 0.43667 | B: 0.50010 | C: 0.45882\n",
      "** [JOINT LOSS] ** : 0.901801\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001841 | Grad Max: 0.054623\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.024464 | Grad Max: 0.114421\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004366 | Grad Max: 0.025726\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.067344 | Grad Max: 0.067344\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000278 | Grad Max: 0.013539\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003846 | Grad Max: 0.076740\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000049 | Grad Max: 0.001992\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001062 | Grad Max: 0.005992\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000010 | Grad Max: 0.000209\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000296 | Grad Max: 0.001281\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000137\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000138 | Grad Max: 0.000652\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000474 | Grad Max: 0.001857\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.006660 | Grad Max: 0.006660\n",
      "[GRADIENT NORM TOTAL] 0.5418\n",
      "\n",
      ">>> [TRAIN] BATCH 2 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53503805 0.46496195] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 15/2033\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.74432\n",
      "[LOGITS Ex2 A] Mean Abs: 0.805 | Max: 4.007\n",
      "[LOSS Ex2] A: 0.42457 | B: 0.48438 | C: 0.44388\n",
      "** [JOINT LOSS] ** : 0.889479\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001816 | Grad Max: 0.035884\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.032700 | Grad Max: 0.158470\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004776 | Grad Max: 0.025058\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.050136 | Grad Max: 0.050136\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000293 | Grad Max: 0.012204\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004746 | Grad Max: 0.065904\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000061 | Grad Max: 0.002132\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001590 | Grad Max: 0.007548\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000013 | Grad Max: 0.000254\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000442 | Grad Max: 0.001569\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000208\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000206 | Grad Max: 0.000830\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000735 | Grad Max: 0.002534\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009293 | Grad Max: 0.009293\n",
      "[GRADIENT NORM TOTAL] 0.6422\n",
      "\n",
      ">>> [TRAIN] BATCH 3 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5355065 0.4644935] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 103/1753 | C: 19/2029\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57694 | C: 0.76556\n",
      "[LOGITS Ex2 A] Mean Abs: 0.774 | Max: 3.875\n",
      "[LOSS Ex2] A: 0.43054 | B: 0.50198 | C: 0.44734\n",
      "** [JOINT LOSS] ** : 1.176400\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002229 | Grad Max: 0.045574\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.043166 | Grad Max: 0.213175\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013902 | Grad Max: 0.075820\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.177309 | Grad Max: 0.177309\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000350 | Grad Max: 0.014441\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005838 | Grad Max: 0.084205\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000083 | Grad Max: 0.002567\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002179 | Grad Max: 0.009998\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000018 | Grad Max: 0.000375\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000619 | Grad Max: 0.001908\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000241\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000287 | Grad Max: 0.001070\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000917 | Grad Max: 0.003262\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.012276 | Grad Max: 0.012276\n",
      "[GRADIENT NORM TOTAL] 0.8750\n",
      "\n",
      ">>> [TRAIN] BATCH 4 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351635  0.46483645] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 10/2038\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57369 | C: 0.73580\n",
      "[LOGITS Ex2 A] Mean Abs: 0.779 | Max: 3.944\n",
      "[LOSS Ex2] A: 0.42387 | B: 0.49993 | C: 0.44872\n",
      "** [JOINT LOSS] ** : 1.092776\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002410 | Grad Max: 0.051667\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.037667 | Grad Max: 0.155358\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014595 | Grad Max: 0.061280\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.210989 | Grad Max: 0.210989\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000343 | Grad Max: 0.013896\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005437 | Grad Max: 0.084874\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000066 | Grad Max: 0.003240\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001752 | Grad Max: 0.010973\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000283\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000495 | Grad Max: 0.001789\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000202\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000231 | Grad Max: 0.000884\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000765 | Grad Max: 0.002308\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010647 | Grad Max: 0.010647\n",
      "[GRADIENT NORM TOTAL] 0.8348\n",
      "\n",
      ">>> [TRAIN] BATCH 5 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.032\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.52229995 0.47770005] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 14/2034\n",
      "[LOSS Ex1] A: 0.80076 | B: 0.57631 | C: 0.78581\n",
      "[LOGITS Ex2 A] Mean Abs: 0.802 | Max: 4.001\n",
      "[LOSS Ex2] A: 0.44059 | B: 0.50448 | C: 0.43904\n",
      "** [JOINT LOSS] ** : 1.182329\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.003323 | Grad Max: 0.047925\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.049391 | Grad Max: 0.222644\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.023670 | Grad Max: 0.122110\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.197604 | Grad Max: 0.197604\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000392 | Grad Max: 0.013947\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.006520 | Grad Max: 0.084885\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000091 | Grad Max: 0.002823\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.002482 | Grad Max: 0.010612\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000019 | Grad Max: 0.000351\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000700 | Grad Max: 0.002660\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000011 | Grad Max: 0.000219\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000321 | Grad Max: 0.001071\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000914 | Grad Max: 0.002859\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.013358 | Grad Max: 0.013358\n",
      "[GRADIENT NORM TOTAL] 1.0577\n",
      "\n",
      ">>> [TRAIN] BATCH 6 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.176 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.033\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5303971  0.46960285] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 118/1930 | C: 17/2031\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.71216\n",
      "[LOGITS Ex2 A] Mean Abs: 0.773 | Max: 4.358\n",
      "[LOSS Ex2] A: 0.44775 | B: 0.48900 | C: 0.44273\n",
      "** [JOINT LOSS] ** : 0.887639\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001876 | Grad Max: 0.024748\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.027866 | Grad Max: 0.151986\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.006638 | Grad Max: 0.038181\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.097732 | Grad Max: 0.097732\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000264 | Grad Max: 0.009992\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.003974 | Grad Max: 0.053509\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000059 | Grad Max: 0.002726\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001493 | Grad Max: 0.007193\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000013 | Grad Max: 0.000315\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000430 | Grad Max: 0.001779\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000217\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000201 | Grad Max: 0.000933\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000726 | Grad Max: 0.002340\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008848 | Grad Max: 0.008848\n",
      "[GRADIENT NORM TOTAL] 0.5665\n",
      "\n",
      ">>> [TRAIN] BATCH 7 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5312753  0.46872473] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 103/1753 | C: 17/2031\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.73692\n",
      "[LOGITS Ex2 A] Mean Abs: 0.779 | Max: 3.948\n",
      "[LOSS Ex2] A: 0.42655 | B: 0.50363 | C: 0.45928\n",
      "** [JOINT LOSS] ** : 0.901109\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001481 | Grad Max: 0.025272\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.032105 | Grad Max: 0.173589\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.003907 | Grad Max: 0.018586\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.052439 | Grad Max: 0.052439\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000280 | Grad Max: 0.013049\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004633 | Grad Max: 0.079985\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.002683\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001654 | Grad Max: 0.010425\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000276\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000461 | Grad Max: 0.001930\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000009 | Grad Max: 0.000187\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000217 | Grad Max: 0.000920\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000781 | Grad Max: 0.002163\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009815 | Grad Max: 0.009815\n",
      "[GRADIENT NORM TOTAL] 0.6327\n",
      "\n",
      ">>> [TRAIN] BATCH 8 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.041\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5194261 0.4805739] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 2/2046 | B: 109/1939 | C: 23/2025\n",
      "[LOSS Ex1] A: 0.79889 | B: 0.57369 | C: 0.76316\n",
      "[LOGITS Ex2 A] Mean Abs: 0.786 | Max: 4.135\n",
      "[LOSS Ex2] A: 0.42140 | B: 0.48664 | C: 0.45806\n",
      "** [JOINT LOSS] ** : 1.167283\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002229 | Grad Max: 0.045402\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.031565 | Grad Max: 0.143701\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.013835 | Grad Max: 0.051120\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.163985 | Grad Max: 0.163985\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000293 | Grad Max: 0.010803\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004309 | Grad Max: 0.067610\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000053 | Grad Max: 0.001863\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001283 | Grad Max: 0.006946\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000273\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000346 | Grad Max: 0.001474\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000161\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000160 | Grad Max: 0.000667\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000567 | Grad Max: 0.002463\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.007358 | Grad Max: 0.007358\n",
      "[GRADIENT NORM TOTAL] 0.6852\n",
      "\n",
      ">>> [TRAIN] BATCH 9 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.169 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.064\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53480947 0.46519053] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 8/2040\n",
      "[LOSS Ex1] A: 0.59509 | B: 0.57631 | C: 0.71922\n",
      "[LOGITS Ex2 A] Mean Abs: 0.767 | Max: 3.981\n",
      "[LOSS Ex2] A: 0.44016 | B: 0.49640 | C: 0.46637\n",
      "** [JOINT LOSS] ** : 1.097847\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002354 | Grad Max: 0.049116\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.037345 | Grad Max: 0.156704\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.014998 | Grad Max: 0.075083\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.235840 | Grad Max: 0.235840\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000307 | Grad Max: 0.015008\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004832 | Grad Max: 0.091695\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000060 | Grad Max: 0.002338\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001531 | Grad Max: 0.008379\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000013 | Grad Max: 0.000272\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000433 | Grad Max: 0.001674\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000186\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000207 | Grad Max: 0.000816\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000865 | Grad Max: 0.002321\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.010565 | Grad Max: 0.010565\n",
      "[GRADIENT NORM TOTAL] 0.8158\n",
      "\n",
      ">>> [TRAIN] BATCH 10 START <<<\n",
      "[DATA A] Shape: torch.Size([1616, 32]) | Mean: 0.050 | Std: 0.168 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.078 | Max: 0.039\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53517044 0.46482953] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/1616 | B: 118/1930 | C: 13/2035\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57129 | C: 0.73274\n",
      "[LOGITS Ex2 A] Mean Abs: 0.759 | Max: 4.025\n",
      "[LOSS Ex2] A: 0.44588 | B: 0.48338 | C: 0.45814\n",
      "** [JOINT LOSS] ** : 0.897142\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001929 | Grad Max: 0.035685\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.015630 | Grad Max: 0.069839\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004837 | Grad Max: 0.022566\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.062879 | Grad Max: 0.062879\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000227 | Grad Max: 0.011487\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.002827 | Grad Max: 0.064266\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000041 | Grad Max: 0.001663\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.000766 | Grad Max: 0.004713\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000009 | Grad Max: 0.000265\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000204 | Grad Max: 0.001446\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000005 | Grad Max: 0.000141\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000093 | Grad Max: 0.000693\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000508 | Grad Max: 0.002257\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.004384 | Grad Max: 0.004384\n",
      "[GRADIENT NORM TOTAL] 0.4093\n",
      "\n",
      ">>> [TRAIN] BATCH 11 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.053 | Std: 0.174 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.037\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.53503805 0.46496198] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 0/2048 | B: 103/1753 | C: 9/2039\n",
      "[LOSS Ex1] A: -0.00000 | B: 0.57694 | C: 0.72980\n",
      "[LOGITS Ex2 A] Mean Abs: 0.808 | Max: 4.223\n",
      "[LOSS Ex2] A: 0.42583 | B: 0.50210 | C: 0.42841\n",
      "** [JOINT LOSS] ** : 0.887691\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001790 | Grad Max: 0.031037\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.031153 | Grad Max: 0.172055\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.004494 | Grad Max: 0.029600\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.065726 | Grad Max: 0.065726\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000292 | Grad Max: 0.013360\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004726 | Grad Max: 0.071524\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000063 | Grad Max: 0.002073\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001671 | Grad Max: 0.007610\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000013 | Grad Max: 0.000278\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000465 | Grad Max: 0.001843\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000208\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000213 | Grad Max: 0.000861\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000705 | Grad Max: 0.002548\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.008588 | Grad Max: 0.008588\n",
      "[GRADIENT NORM TOTAL] 0.6352\n",
      "\n",
      ">>> [TRAIN] BATCH 12 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.051 | Std: 0.170 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.068\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5355066  0.46449342] | Indices: [0 1] | Label Real: 0\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 109/1939 | C: 14/2034\n",
      "[LOSS Ex1] A: 0.80684 | B: 0.57369 | C: 0.69096\n",
      "[LOGITS Ex2 A] Mean Abs: 0.783 | Max: 3.957\n",
      "[LOSS Ex2] A: 0.43345 | B: 0.47999 | C: 0.45112\n",
      "** [JOINT LOSS] ** : 1.145353\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.001804 | Grad Max: 0.030473\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.032962 | Grad Max: 0.161750\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.008244 | Grad Max: 0.042594\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.057359 | Grad Max: 0.057359\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000287 | Grad Max: 0.011317\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.004727 | Grad Max: 0.067740\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000061 | Grad Max: 0.002361\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001629 | Grad Max: 0.007775\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000014 | Grad Max: 0.000283\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000469 | Grad Max: 0.001821\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000008 | Grad Max: 0.000168\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000219 | Grad Max: 0.000805\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000661 | Grad Max: 0.002228\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.009389 | Grad Max: 0.009389\n",
      "[GRADIENT NORM TOTAL] 0.6551\n",
      "\n",
      ">>> [TRAIN] BATCH 13 START <<<\n",
      "[DATA A] Shape: torch.Size([2048, 32]) | Mean: 0.052 | Std: 0.172 | Min: 0.000 | Max: 1.000\n",
      "[LOGITS Ex1 A] Mean Abs: 0.079 | Max: 0.050\n",
      "[SAMPLE 0 PREDICTION A] Top2 Probs: [0.5351636  0.46483636] | Indices: [0 1] | Label Real: 1\n",
      "[CONFIDENCE A] Mean: 0.530 | Std: 0.006\n",
      "[MASKS] A(Pass/Fail): 1/2047 | B: 115/1933 | C: 12/1364\n",
      "[LOSS Ex1] A: 0.59631 | B: 0.57631 | C: 0.72705\n",
      "[LOGITS Ex2 A] Mean Abs: 0.780 | Max: 4.037\n",
      "[LOSS Ex2] A: 0.42910 | B: 0.49757 | C: 0.42455\n",
      "** [JOINT LOSS] ** : 1.083631\n",
      "[GRADIENTS CHECK]\n",
      "  -> Layer: shared_layers.0.weight | Grad Mean: 0.002931 | Grad Max: 0.079179\n",
      "  -> Layer: shared_layers.0.bias | Grad Mean: 0.034903 | Grad Max: 0.135687\n",
      "  -> Layer: exit1_layers.0.weight | Grad Mean: 0.015424 | Grad Max: 0.065858\n",
      "  -> Layer: exit1_layers.0.bias | Grad Mean: 0.222248 | Grad Max: 0.222248\n",
      "  -> Layer: exit2_layers.0.weight | Grad Mean: 0.000352 | Grad Max: 0.015524\n",
      "  -> Layer: exit2_layers.0.bias | Grad Mean: 0.005073 | Grad Max: 0.095070\n",
      "  -> Layer: exit2_layers.3.weight | Grad Mean: 0.000056 | Grad Max: 0.001950\n",
      "  -> Layer: exit2_layers.3.bias | Grad Mean: 0.001224 | Grad Max: 0.006472\n",
      "  -> Layer: exit2_layers.6.weight | Grad Mean: 0.000011 | Grad Max: 0.000240\n",
      "  -> Layer: exit2_layers.6.bias | Grad Mean: 0.000327 | Grad Max: 0.001442\n",
      "  -> Layer: exit2_layers.9.weight | Grad Mean: 0.000006 | Grad Max: 0.000165\n",
      "  -> Layer: exit2_layers.9.bias | Grad Mean: 0.000148 | Grad Max: 0.000771\n",
      "  -> Layer: exit2_layers.12.weight | Grad Mean: 0.000699 | Grad Max: 0.002522\n",
      "  -> Layer: exit2_layers.12.bias | Grad Mean: 0.005286 | Grad Max: 0.005286\n",
      "[GRADIENT NORM TOTAL] 0.8354\n",
      "\n",
      "[EPOCH SUMMARY] Train Loss: 1.0292\n",
      "\n",
      "[VALIDATION] Starting...\n",
      "[VAL] Processando primeiro batch de validação...\n",
      "[EPOCH END] Val Loss: 1.0100 | Alpha: 0.5500\n",
      "No improve count: 12/15\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo treinado e salvo em 'models/teste_ljoint9.pth'\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "limiar = train_model(\n",
    "    model, \n",
    "    train_loaders, \n",
    "    val_loaders, \n",
    "    epochs,\n",
    "    current_threshold=0.55,\n",
    "    lr=0.0001,\n",
    "    device=device\n",
    ")\n",
    "torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "print(f\"\\nModelo treinado e salvo em 'models/{modelname}.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e248ef82-7a24-4504-8cbc-ae2e4e3422fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 'models/teste_ljoint9.pth' carregado\n",
      "\n",
      "Base: UNSW\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 8428\n",
      "True Negatives (TN): 6521\n",
      "False Positives (FP): 2479\n",
      "False Negatives (FN): 572\n",
      "\n",
      "F1 Score: 0.8467\n",
      "True Positive Rate (TPR) / Recall: 0.9364\n",
      "True Negative Rate (TNR) / Specificity: 0.7246\n",
      "--------------------\n",
      "  Accuracy: 83.0500%\n",
      "  Avg. Inference Time: 0.0030 ms\n",
      "  Early Exit Rate: 0.0667% (12/18000)\n",
      "--------------------\n",
      "\n",
      "Base: BOT\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApIAAAIjCAYAAACwHvu2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdBZJREFUeJzt3XdYFFfbBvB7QVh6U2pELNhQsWCi2AuKSmyxl4iKGgsxYiexa8QYe40tggZ7j8aGNSqxoCggGkUMUWmK9A7z/eHHvq6gsBPWRfb+vddcr3vmzMwzy2oennPmrEQQBAFERERERArSUHUARERERPRpYiJJRERERKIwkSQiIiIiUZhIEhEREZEoTCSJiIiISBQmkkREREQkChNJIiIiIhKFiSQRERERicJEkoiIiIhEYSJJJTJv3jxIJBKlXkMikWDevHlKvcbH9vPPP6N69erQ1NREo0aNlHKNqVOnwtDQEO7u7khISICDgwOCg4NL/To3btyAtrY2/vnnH4WOU8ZnZ/jw4ahatWqpnlNRVatWxfDhw1UaQ2nbuXMn6tSpAy0tLZiYmAAA2rVrh3bt2qk0Lvqw+/fvo0KFCggNDVV1KKSGmEiWMb6+vpBIJJBIJLhy5Uqh/YIgwNbWFhKJBF9++aWoayxevBhHjhz5j5F+GvLy8rB9+3a0a9cOZmZmkEqlqFq1KkaMGIFbt24p9dpnzpzB9OnT0bJlS2zfvh2LFy8u9WukpqZi48aNWLBgAcLCwlCpUiUYGBjA0dGx1K/1ww8/YNCgQbCzs5O1tWvXDvXr1y/1a6mzw4cPo2vXrqhUqRK0tbVhY2OD/v374/z580q97oMHDzB8+HDUqFEDW7ZswebNm5V6vU/dtWvX0KpVK+jp6cHKygoTJ05EampqiY4t+Df+3W3JkiVy/Qp+CXt309HRkevn4OAANzc3zJkzp9Tuj6ikKqg6ACqajo4Odu3ahVatWsm1X7p0Cc+ePYNUKhV97sWLF6Nv377o1atXiY+ZNWsWZs6cKfqaqpCRkYGvvvoKp06dQps2bfD999/DzMwMT58+xb59++Dn54eoqChUrlxZKdc/f/48NDQ0sG3bNmhrayvlGjo6Orh//z7s7Ozg5eWFFy9ewMrKChoapfs7YnBwMAICAnDt2jWFj1XGZ2fLli3Iz88v1XOqmiAIGDlyJHx9fdG4cWNMnjwZVlZWiI6OxuHDh9GxY0dcvXoVLVq0UMr1L168iPz8fKxevRr29vay9jNnzijlep+y4OBgdOzYEXXr1sWKFSvw7NkzLFu2DI8ePcLJkydLdI5OnTph2LBhcm2NGzcusu/GjRthYGAge62pqVmoz9ixY9GtWzdERESgRo0aCtwN0X/DRLKM6tatG/bv3481a9agQoX//Zh27doFJycnvHz58qPEkZaWBn19fVSoUEEujk/BtGnTcOrUKaxcuRKTJk2S2zd37lysXLlSqdePi4uDrq6u0pJIAKhQoYJchdDGxkYp19m+fTuqVKmC5s2bK3ysMj47WlpapXau/Px8ZGdnF6ryfGzLly+Hr68vJk2ahBUrVshNB/jhhx+wc+dOpf4djIuLAwDZkHYBZX5+P1Xff/89TE1NcfHiRRgZGQF4M9Vh9OjROHPmDDp37lzsOWrVqoWhQ4eW6Hp9+/ZFpUqVPtjHxcUFpqam8PPzw4IFC0p0XqLSwKHtMmrQoEF49eoVzp49K2vLzs7GgQMHMHjw4CKPWbZsGVq0aIGKFStCV1cXTk5OOHDggFwfiUSCtLQ0+Pn5yYZJCuZ5FQyj3L9/H4MHD4apqamsIvruPLfhw4e/d3imuHmOWVlZ8PLygrm5OQwNDdGjRw88e/asyL7Pnz/HyJEjYWlpCalUinr16uHXX38t7u3Ds2fPsGnTJnTq1KlQEgm8+Y1+6tSpctXIO3fuoGvXrjAyMoKBgQE6duyIv/76S+64gqkHV69exeTJk2Fubg59fX307t0b8fHxsn4SiQTbt29HWlqa7H3x9fXF06dPZX9+17vvXUpKCiZNmoSqVatCKpXCwsICnTp1wu3bt2V9Ll68iL59+6JKlSqQSqWwtbWFl5cXMjIyCp3//PnzaN26NfT19WFiYoKePXsiPDy82PcSAI4cOYIOHTqImutY1BxJiUQCT09P7N+/Hw4ODtDV1YWzszNCQkIAAJs2bYK9vT10dHTQrl07PH36VO74ouZIluTz//a1/f39Ua9ePUilUpw6deq98QuCgEWLFqFy5crQ09ND+/btERYWVmTfxMRETJo0Cba2tpBKpbC3t8dPP/1UbPU0IyMDPj4+qFOnDpYtW1bk+/z111/jiy++kL1+8uQJ+vXrBzMzM+jp6aF58+Y4ceKE3DEXL16ERCLBvn378OOPP6Jy5crQ0dFBx44d8fjxY1m/qlWrYu7cuQAAc3Nzuc/iu3Mks7OzMWfOHDg5OcHY2Bj6+vpo3bo1Lly4IHftgs/6smXLsHnzZtSoUQNSqRSff/45bt68Wej+Hjx4gP79+8Pc3By6urqoXbs2fvjhB9n+f/75B+PHj0ft2rWhq6uLihUrol+/foU+G8qWnJyMs2fPYujQobIkEgCGDRsGAwMD7Nu3r8TnysjIQGZmZrH9BEFAcnIyBEF4bx8tLS20a9cOR48eLfH1iUrDp1ViUiNVq1aFs7Mzdu/eja5duwIATp48iaSkJAwcOBBr1qwpdMzq1avRo0cPDBkyBNnZ2dizZw/69euH48ePw83NDcCbyfSjRo3CF198gTFjxgBAoWGQfv36oWbNmli8ePF7/+H65ptv4OLiItd26tQp+Pv7w8LC4oP3NmrUKPz2228YPHgwWrRogfPnz8vie1tsbCyaN28u+w+/ubk5Tp48CQ8PDyQnJxeZIBY4efIkcnNz8fXXX38wlgJhYWFo3bo1jIyMMH36dGhpaWHTpk1o164dLl26hGbNmsn1//bbb2Fqaoq5c+fi6dOnWLVqFTw9PbF3714Ab97nzZs348aNG9i6dSsAKDwkOXbsWBw4cACenp5wcHDAq1evcOXKFYSHh6NJkyYAgH379iEjIwPjx4+HmZkZbty4gbVr1+LZs2fYv3+/7FwBAQHo2rUrqlevjnnz5iEjIwNr165Fy5Ytcfv27Q8+uPL8+XNERUXJrlla/vzzTxw7dgwTJkwAAPj4+ODLL7/E9OnTsWHDBowfPx6vX7/G0qVLMXLkyGLnCJbk81/g/Pnz2LdvHzw9PVGpUqUP3v+cOXOwaNEidOvWDd26dcPt27fRuXNnZGdny/VLT09H27Zt8fz5c3zzzTeoUqUKrl27Bm9vb0RHR2PVqlXvvcaVK1eQkJCASZMmFTls+a7Y2Fi0aNEC6enpmDhxIipWrAg/Pz/06NEDBw4cQO/eveX6L1myBBoaGpg6dSqSkpKwdOlSDBkyBNevXwcArFq1Cjt27MDhw4dlw6jvm2ebnJyMrVu3YtCgQRg9ejRSUlKwbds2uLq64saNG4UeKtu1axdSUlLwzTffQCKRYOnSpfjqq6/w5MkTWWX53r17aN26NbS0tDBmzBhUrVoVERER+P333/Hjjz8CAG7evIlr165h4MCBqFy5Mp4+fYqNGzeiXbt2uH//PvT09D74nr1+/Rp5eXnFvrd6enofPFdISAhyc3PRtGlTuXZtbW00atQId+7cKfYawJtfSjds2ABBEFC3bl3MmjXrvUWC6tWrIzU1Ffr6+ujVqxeWL18OS0vLQv2cnJxw9OhRJCcnyyW5REolUJmyfft2AYBw8+ZNYd26dYKhoaGQnp4uCIIg9OvXT2jfvr0gCIJgZ2cnuLm5yR1b0K9Adna2UL9+faFDhw5y7fr6+oK7u3uha8+dO1cAIAwaNOi9+97n0aNHgrGxsdCpUychNzf3vf2Cg4MFAML48ePl2gcPHiwAEObOnStr8/DwEKytrYWXL1/K9R04cKBgbGxc6H7f5uXlJQAQ7ty5894+b+vVq5egra0tREREyNpevHghGBoaCm3atJG1Ffx8XFxchPz8fLnraWpqComJibI2d3d3QV9fX+46kZGRAgBh+/bthWJ49/6NjY2FCRMmfDDutLS0Qm0+Pj6CRCIR/vnnH1lbo0aNBAsLC+HVq1eytrt37woaGhrCsGHDPniNgIAAAYDw+++/F9rXtm1boV69eh88vqjPDgBBKpUKkZGRsrZNmzYJAAQrKyshOTlZ1u7t7S0AkOvr7u4u2NnZyZ2zpJ9/AIKGhoYQFhb2wbgFQRDi4uIEbW1twc3NTe7n/f333wsA5P4eLVy4UNDX1xf+/vtvuXPMnDlT0NTUFKKiot57ndWrVwsAhMOHDxcbkyAIwqRJkwQAwp9//ilrS0lJEapVqyZUrVpVyMvLEwRBEC5cuCAAEOrWrStkZWUVul5ISIisreDnFB8fL3ettm3bCm3btpW9zs3NlTuXIAjC69evBUtLS2HkyJGytoLPesWKFYWEhARZ+9GjRwt9ntq0aSMYGhrKfWYFQZB7z4v6+x4YGCgAEHbs2FH0G/UWOzs7AUCx29t/B4uyf/9+AYBw+fLlQvv69esnWFlZFRtLixYthFWrVglHjx4VNm7cKNSvX18AIGzYsEGu36pVqwRPT0/B399fOHDggPDdd98JFSpUEGrWrCkkJSUVOu+uXbsEAML169eLjYGotHBouwzr378/MjIycPz4caSkpOD48ePv/Y0VAHR1dWV/fv36NZKSktC6dWu5odCSGDt2rEL909LS0Lt3b5iammL37t0frKj88ccfAICJEyfKtb9bXRQEAQcPHkT37t0hCAJevnwp21xdXZGUlPTB+0pOTgYAGBoaFht/Xl4ezpw5g169eqF69eqydmtrawwePBhXrlyRna/AmDFj5IYfW7dujby8PIWXxvkQExMTXL9+HS9evHhvn7crJ2lpaXj58iVatGgBQRBklZHo6GgEBwdj+PDhMDMzk/V3dHREp06dZD+T93n16hUAwNTU9L/cTiEdO3aUqwQWVH379Okj93MraH/y5MkHz6fI579t27ZwcHAoNsaAgABkZ2fj22+/lft5F1UN379/P1q3bg1TU1O5z6uLiwvy8vJw+fLl915Hkc8r8Obv0RdffCH3MJ6BgQHGjBmDp0+f4v79+3L9R4wYITfXsXXr1gCKf0+LoqmpKTtXfn4+EhISZBW6ot7rAQMGyH123r12fHw8Ll++jJEjR6JKlSpyx779nr/9883JycGrV69gb28PExOTEv0b5+/vj7Nnzxa7vfsAzLsKpo0U9cCjjo5OkdNK3nX16lV899136NGjB8aOHYugoCDUr18f33//vdzx3333HdauXYvBgwejT58+WLVqFfz8/PDo0SNs2LCh0HkL3uePNYeeCODQdplmbm4OFxcX7Nq1C+np6cjLy0Pfvn3f2//48eNYtGgRgoODkZWVJWtXdF5btWrVFOo/evRoRERE4Nq1a6hYseIH+/7zzz/Q0NAoNJxeu3Ztudfx8fFITEzE5s2b37sMScHDAUUpGNZJSUkpNv74+Hikp6cXigEA6tati/z8fPz777+oV6+erP3d/+AV/AP++vXrYq9XUkuXLoW7uztsbW3h5OSEbt26YdiwYXLJblRUFObMmYNjx44VunZSUhIAyJLb993f6dOnZQ9VfYjwgflZYrz7HhobGwMAbG1ti2wv7r1V5PNf0s94wXtXs2ZNuXZzc/NCifWjR49w7949mJubF3mu0vq8FsT17nQL4M3Ps2D/28sylfbn1c/PD8uXL8eDBw+Qk5Mjay/qfS3u2gUJZXHLSBXMI92+fTueP38u93ks+Kx/SMuWLYvtUxIFCe3bn7ECmZmZcglvSWlra8PT01OWVL67WsfbBg8ejClTpiAgIKDQaggF74my1/wlehsTyTJu8ODBGD16NGJiYtC1a9dCT1QW+PPPP9GjRw+0adMGGzZsgLW1NbS0tLB9+3bs2rVLoWsq8g/h6tWrsXv3bvz222+luuB2wcMJQ4cOhbu7e5F9PrRWYp06dQC8mc+kjIXA31d1LS7Zet8/8EXN3erfvz9at26Nw4cP48yZM/j555/x008/4dChQ+jatSvy8vLQqVMnJCQkYMaMGahTpw709fXx/PlzDB8+vNSWxyn45aA0k2Tg/e+hmPdW0c+/mP/YFyc/Px+dOnXC9OnTi9xfq1at9x779udVkWW5Skrs57Uov/32G4YPH45evXph2rRpsLCwgKamJnx8fBAREaG0a3/77bfYvn07Jk2aBGdnZxgbG0MikWDgwIEl+qzHx8eXaI6kgYGB3FI777K2tgbwptL/rujoaNErJxT8ApWQkFCivkX1K/g7WtwT3kSliYlkGde7d2988803+Ouvv2QPchTl4MGD0NHRwenTp+WGXLZv316ob2n9tvrnn39i6tSpmDRpEoYMGVKiY+zs7JCfn4+IiAi5CtnDhw/l+hU80Z2Xl1fooZ6S6Nq1KzQ1NfHbb78V+8CNubk59PT0CsUAvHmSVENDo1CVTKyCakxiYqJc+/uGxK2trTF+/HiMHz8ecXFxaNKkCX788Ud07doVISEh+Pvvv+Hn5yc3HPf2k/4AZMsDve/+KlWq9MFqZEGSExkZWfwNqogin39FFLx3jx49kqsEx8fHF0qsa9SogdTUVFGf11atWsmmhnz//ffFPnBjZ2f33p/n23Erw4EDB1C9enUcOnRI7t+Sgqe+FVXwvhb3rSwHDhyAu7s7li9fLmvLzMws9HfpfT7//PMSTT2ZO3fuB1eeqF+/PipUqIBbt26hf//+svbs7GwEBwfLtSmioDL7vop2AUEQ8PTp0yLXnIyMjISGhsYHf2khKm2cI1nGGRgYYOPGjZg3bx66d+/+3n6ampqQSCRyv3E/ffq0yG+w0dfXL/E/vu8THR2N/v37o1WrVvj5559LfFzBE+jvPnX+7hOtmpqa6NOnDw4ePFjkf2DeXmqnKLa2trI13dauXVtof35+PpYvX45nz55BU1MTnTt3xtGjR+WWEomNjZUtCl9aT0AaGRmhUqVKhebLvTvfKS8vr9BwnYWFBWxsbGRDagXJxtuVHUEQsHr1arnjrK2t0ahRI/j5+cn93ENDQ3HmzBl069btgzF/9tlnsLW1Vfo3Af0Xinz+FeHi4gItLS2sXbtW7n0u6gns/v37IzAwEKdPny60LzExEbm5ue+9jp6eHmbMmIHw8HDMmDGjyGrdb7/9hhs3bgB4s87sjRs3EBgYKNuflpaGzZs3o2rVqiWa/ylWUZ+769evy8WiCHNzc7Rp0wa//voroqKi5Pa9fQ1NTc1C78vatWtLVGUESm+OpLGxMVxcXPDbb7/JTUXYuXMnUlNT0a9fP1lbeno6Hjx4IDdnsah/u1JSUrBq1SpUqlQJTk5OH+y7ceNGxMfHo0uXLoX2BQUFoV69erLpIEQfAyuSn4D3De2+zc3NDStWrECXLl0wePBgxMXFYf369bC3t8e9e/fk+jo5OSEgIAArVqyAjY0NqlWrVuR8qw+ZOHEi4uPjMX36dOzZs0dun6Oj43uHnRs1aoRBgwZhw4YNSEpKQosWLXDu3Dm5Ne0KLFmyBBcuXECzZs0wevRoODg4ICEhAbdv30ZAQECxQ0DLly9HREQEJk6ciEOHDuHLL7+EqakpoqKisH//fjx48AADBw4EACxatAhnz55Fq1atMH78eFSoUAGbNm1CVlYWli5dqtB7U5xRo0ZhyZIlGDVqFJo2bYrLly/j77//luuTkpKCypUro2/fvmjYsCEMDAwQEBCAmzdvyioyderUQY0aNTB16lQ8f/4cRkZGOHjwYJFD0D///DO6du0KZ2dneHh4yJb/MTY2LtH3m/fs2ROHDx+GIAiFKtrx8fFYtGhRoWOqVatW4kr1f6XI518R5ubmmDp1qmxpom7duuHOnTs4efJkoeHDadOm4dixY/jyyy8xfPhwODk5IS0tDSEhIThw4ACePn36wSHHadOmISwsDMuXL8eFCxfQt29fWFlZISYmBkeOHMGNGzdk3yw0c+ZM2dJgEydOhJmZGfz8/BAZGYmDBw+W+jcbve3LL7/EoUOH0Lt3b7i5uSEyMhK//PILHBwcSvwVge9as2YNWrVqhSZNmmDMmDGoVq0anj59ihMnTsi+N/7LL7/Ezp07YWxsDAcHBwQGBiIgIKDYedkFSmuOJAD8+OOPaNGiBdq2bYsxY8bg2bNnWL58OTp37iyX4N24cQPt27eXq3KuX78eR44cQffu3VGlShVER0fLkuidO3fKPRRlZ2eHAQMGoEGDBtDR0cGVK1ewZ88eNGrUCN98841cTDk5Obh06RLGjx9favdJVCIf+SlxKsbby/98SFHL/2zbtk2oWbOmIJVKhTp16gjbt28vcumVBw8eCG3atBF0dXXlljB53/Ifb+8r0LZtW9HLZ2RkZAgTJ04UKlasKOjr6wvdu3cX/v333yKPjY2NFSZMmCDY2toKWlpagpWVldCxY0dh8+bNH7xGgdzcXGHr1q1C69atBWNjY0FLS0uws7MTRowYUWhpoNu3bwuurq6CgYGBoKenJ7Rv3164du2aXJ/3/XwKllm5cOGCrK2o5X8E4c0yJh4eHoKxsbFgaGgo9O/fX4iLi5O7/6ysLGHatGlCw4YNBUNDQ0FfX19o2LBhoeVB7t+/L7i4uAgGBgZCpUqVhNGjRwt3794tcomhgIAAoWXLloKurq5gZGQkdO/eXbh//36J3sfbt28XWm5GED78OejYsaMgCO9f/ufdpY0Klov5+eef5doL3tv9+/fL2opa/qekn/+irv0heXl5wvz58wVra2tBV1dXaNeunRAaGirY2dkVWkYrJSVF8Pb2Fuzt7QVtbW2hUqVKQosWLYRly5YJ2dnZJbregQMHhM6dOwtmZmZChQoVBGtra2HAgAHCxYsX5fpFREQIffv2FUxMTAQdHR3hiy++EI4fPy7Xp6j3ThCKXoaqpMv/5OfnC4sXLxbs7OwEqVQqNG7cWDh+/Hihn8n7fp6CUHipK0EQhNDQUKF3796CkZGRAECoXbu2MHv2bNn+169fCyNGjBAqVaokGBgYCK6ursKDBw+K/Dl8DH/++afQokULQUdHRzA3NxcmTJggt2yVIPzv/X/7Xs+cOSN06tRJsLKyErS0tAQTExOhc+fOwrlz5wpdY9SoUYKDg4NgaGgoaGlpCfb29sKMGTMKXUcQBOHkyZMCAOHRo0elfq9EHyIRhFJ+FJOIyqWOHTvCxsYGO3fuVHUoVM65uLhg+vTpJfqqQXqjV69ekEgkOHz4sKpDITXDRJKISuT69eto3bo1Hj16pNSHOYhWr16NoKAg7NixQ9WhfBLCw8PRoEEDBAcHF7uMElFpYyJJRERlwu7du5GWlgZfX19YWFjg0KFDqg6JiIrBp7aJiKhMCAsLg6enJ54/f46pU6eqOhwiKgFWJImIiIhIFFYkiYiIiEgUJpJEREREJAoTSSIiIiISpVx+s41uY09Vh0BESvL65jpVh0BESqKjwqxEmblDxp3y++8WK5JEREREJEq5rEgSERERKUTC2poYTCSJiIiIJBJVR/BJYvpNRERERKKwIklERETEoW1R+K4RERERkSisSBIRERFxjqQorEgSERERlREbN26Eo6MjjIyMYGRkBGdnZ5w8eVK2v127dpBIJHLb2LFj5c4RFRUFNzc36OnpwcLCAtOmTUNubq5cn4sXL6JJkyaQSqWwt7eHr6+vqHhZkSQiIiIqI3MkK1eujCVLlqBmzZoQBAF+fn7o2bMn7ty5g3r16gEARo8ejQULFsiO0dPTk/05Ly8Pbm5usLKywrVr1xAdHY1hw4ZBS0sLixcvBgBERkbCzc0NY8eOhb+/P86dO4dRo0bB2toarq6uCsUrEQRBKIX7LlP4zTZE5Re/2Yao/FLpN9t8MVVp5864sew/HW9mZoaff/4ZHh4eaNeuHRo1aoRVq1YV2ffkyZP48ssv8eLFC1haWgIAfvnlF8yYMQPx8fHQ1tbGjBkzcOLECYSGhsqOGzhwIBITE3Hq1CmFYisb6TcRERGRKkkkStuysrKQnJwst2VlZRUbUl5eHvbs2YO0tDQ4OzvL2v39/VGpUiXUr18f3t7eSE9Pl+0LDAxEgwYNZEkkALi6uiI5ORlhYWGyPi4uLnLXcnV1RWBgoMJvGxNJIiIiIomG0jYfHx8YGxvLbT4+Pu8NJSQkBAYGBpBKpRg7diwOHz4MBwcHAMDgwYPx22+/4cKFC/D29sbOnTsxdOhQ2bExMTFySSQA2euYmJgP9klOTkZGRoZCbxvnSBIREREpkbe3NyZPnizXJpVK39u/du3aCA4ORlJSEg4cOAB3d3dcunQJDg4OGDNmjKxfgwYNYG1tjY4dOyIiIgI1atRQ2j28DxNJIiIiIiUu/yOVSj+YOL5LW1sb9vb2AAAnJyfcvHkTq1evxqZNmwr1bdasGQDg8ePHqFGjBqysrHDjxg25PrGxsQAAKysr2f8XtL3dx8jICLq6uiW/MXBom4iIiKhMy8/Pf++cyuDgYACAtbU1AMDZ2RkhISGIi4uT9Tl79iyMjIxkw+POzs44d+6c3HnOnj0rNw+zpFiRJCIiIiojy/94e3uja9euqFKlClJSUrBr1y5cvHgRp0+fRkREBHbt2oVu3bqhYsWKuHfvHry8vNCmTRs4OjoCADp37gwHBwd8/fXXWLp0KWJiYjBr1ixMmDBBVhUdO3Ys1q1bh+nTp2PkyJE4f/489u3bhxMnTigcLxNJIiIiojIiLi4Ow4YNQ3R0NIyNjeHo6IjTp0+jU6dO+PfffxEQEIBVq1YhLS0Ntra26NOnD2bNmiU7XlNTE8ePH8e4cePg7OwMfX19uLu7y607Wa1aNZw4cQJeXl5YvXo1KleujK1btyq8hiTAdSSJ6BPDdSSJyi+VriPZ8gelnTvj6o9KO7eqlY06LhERERF9cji0TURERFRG5kh+aphIEhERESlx+Z/yjOk3EREREYnCiiQRERERh7ZF4btGRERERKKwIklERETEiqQofNeIiIiISBRWJImIiIg0+NS2GKxIEhEREZEorEgSERERcY6kKEwkiYiIiLgguShMv4mIiIhIFFYkiYiIiDi0LQrfNSIiIiIShRVJIiIiIs6RFIUVSSIiIiIShRVJIiIiIs6RFIXvGhERERGJwookEREREedIisJEkoiIiIhD26LwXSMiIiIiUViRJCIiIuLQtiisSBIRERGRKKxIEhEREXGOpCh814iIiIhIFFYkiYiIiDhHUhRWJImIiIhIFFYkiYiIiDhHUhQmkkRERERMJEXhu0ZEREREorAiSURERMSHbURhRZKIiIiIRGFFkoiIiIhzJEXhu0ZEREREorAiSURERMQ5kqKwIklEREREorAiSURERMQ5kqIwkSQiIiLi0LYoTL+JiIiISBRWJImIiEjtSViRFIUVSSIiIiIShRVJIiIiUnusSIrDiiQRERERicKKJBERERELkqKwIklEREREorAiSURERGqPcyTFYSJJREREao+JpDgc2iYiIiIiUViRJCIiIrXHiqQ4rEgSERERkSisSBIREZHaY0VSHFYkiYiIiEgUViSJiIiIWJAUhRVJIiIiIhKFFUkiIiJSe5wjKQ4rkkREREQkCiuSREREpPZYkRSHiSQRERGpPSaS4nBom4iIiIhEYUWSiIiI1B4rkuKwIklEREREojCRJCIiIpIocVPAxo0b4ejoCCMjIxgZGcHZ2RknT56U7c/MzMSECRNQsWJFGBgYoE+fPoiNjZU7R1RUFNzc3KCnpwcLCwtMmzYNubm5cn0uXryIJk2aQCqVwt7eHr6+vooF+v+YSBIRERGVEZUrV8aSJUsQFBSEW7duoUOHDujZsyfCwsIAAF5eXvj999+xf/9+XLp0CS9evMBXX30lOz4vLw9ubm7Izs7GtWvX4OfnB19fX8yZM0fWJzIyEm5ubmjfvj2Cg4MxadIkjBo1CqdPn1Y4XokgCMJ/v+2yRbexp6pDICIleX1znapDICIl0VHhkxuVhu9R2rlf+g78T8ebmZnh559/Rt++fWFubo5du3ahb9++AIAHDx6gbt26CAwMRPPmzXHy5El8+eWXePHiBSwtLQEAv/zyC2bMmIH4+Hhoa2tjxowZOHHiBEJDQ2XXGDhwIBITE3Hq1CmFYmNFkoiIiEiJsrKykJycLLdlZWUVe1xeXh727NmDtLQ0ODs7IygoCDk5OXBxcZH1qVOnDqpUqYLAwEAAQGBgIBo0aCBLIgHA1dUVycnJsqpmYGCg3DkK+hScQxFMJImIiEjtSSQSpW0+Pj4wNjaW23x8fN4bS0hICAwMDCCVSjF27FgcPnwYDg4OiImJgba2NkxMTOT6W1paIiYmBgAQExMjl0QW7C/Y96E+ycnJyMjIUOh94/I/REREpPaUufyPt7c3Jk+eLNcmlUrf27927doIDg5GUlISDhw4AHd3d1y6dElp8f0XTCSJiIiIlEgqlX4wcXyXtrY27O3tAQBOTk64efMmVq9ejQEDBiA7OxuJiYlyVcnY2FhYWVkBAKysrHDjxg258xU81f12n3ef9I6NjYWRkRF0dXUVujcObRMRERGVkeV/ipKfn4+srCw4OTlBS0sL586dk+17+PAhoqKi4OzsDABwdnZGSEgI4uLiZH3Onj0LIyMjODg4yPq8fY6CPgXnUAQrkkRERERlhLe3N7p27YoqVaogJSUFu3btwsWLF3H69GkYGxvDw8MDkydPhpmZGYyMjPDtt9/C2dkZzZs3BwB07twZDg4O+Prrr7F06VLExMRg1qxZmDBhgqwqOnbsWKxbtw7Tp0/HyJEjcf78eezbtw8nTpxQOF4mkkRERKT2yspXJMbFxWHYsGGIjo6GsbExHB0dcfr0aXTq1AkAsHLlSmhoaKBPnz7IysqCq6srNmzYIDteU1MTx48fx7hx4+Ds7Ax9fX24u7tjwYIFsj7VqlXDiRMn4OXlhdWrV6Ny5crYunUrXF1dFY6X60gS0SeF60gSlV+qXEfSctR+pZ07dms/pZ1b1ViRJCIiIrVXViqSnxo+bENEREREoqisIpmcnFzivkZGRkqMhIiIiNQdK5LiqCyRNDExKfaHJggCJBIJ8vLyPlJUREREpI6YSIqjskTywoULqro0EREREZUClSWSbdu2VdWliYiIiOSxIClKmXpqOz09HVFRUcjOzpZrd3R0VFFERERERPQ+ZSKRjI+Px4gRI3Dy5Mki93OOJBERESkT50iKUyaW/5k0aRISExNx/fp16Orq4tSpU/Dz80PNmjVx7NgxVYdHREREREUoExXJ8+fP4+jRo2jatCk0NDRgZ2eHTp06wcjICD4+PnBzc1N1iERERFSOsSIpTpmoSKalpcHCwgIAYGpqivj4eABAgwYNcPv2bVWGRkRERETvUSYSydq1a+Phw4cAgIYNG2LTpk14/vw5fvnlF1hbW6s4OiIiIirvJBKJ0rbyrEwMbX/33XeIjo4GAMydOxddunSBv78/tLW14evrq9rgiIiIqPwr3/me0pSJRHLo0KGyPzs5OeGff/7BgwcPUKVKFVSqVEmFkRERERHR+5SJRPJdenp6aNKkiarDICIiIjVR3oeglaVMJJKCIODAgQO4cOEC4uLikJ+fL7f/0KFDKoqMiIiIiN6nTCSSkyZNwqZNm9C+fXtYWlrytwIiIiL6qJh7iFMmEsmdO3fi0KFD6Natm6pDISIiIqISKhOJpLGxMapXr67qMEhFRvdrhdF9W8POxgwAEP4kBos3n8SZq/cBAKe3fIc2TWvKHbPlwBVM/HGP7LWTQxUsnNgTjR1sIQjArdB/8MPqIwj5+3mh61W3rYS/ds9EXn4+rNtMV+KdEVFxtm3ZjDWrlmPI0GGY7v0DAODAvr04+cdxhN8PQ1paGv4MvAkjI6NCx16+dBGbNq7Ho78fQlsqRdOmn2PV2g0f+xaonGBFUpwykUjOmzcP8+fPx6+//gpdXV1Vh0Mf2fPYRMxeexSPo+IhgQRDuzfD/pVj0HzgEoQ/iQEAbDt4FQs3Hpcdk56ZI/uzvq42jq6fgBOXQvCdz15U0NTA7HFuOLZ+Amp2nYXc3P/Nua1QQQM7fEbg6p0ING9Y7ePdJBEVEhpyDwf270GtWrXl2jMzM9CiZWu0aNkaa1YtL/LYgDOnMX/ubHw7yQtfNGuOvNw8PH7898cIm4jeUiYSyf79+2P37t2wsLBA1apVoaWlJbef325Tvv1xOVTu9bz1v2N0v1b4wrGaLJHMyMxG7KuUIo+vXc0KFU30sXDjcTyLTQQA/LjpJG7t/x5VrM3w5N+X/zv3+O54GBmLCzceMpEkUqH0tDR4z5iGufMXYcumjXL7hg4bDgC4eeN6kcfm5ubipyU/wmvqNHzVp5+svYa9vdLipfKPFUlxykQi6e7ujqCgIAwdOpQP26g5DQ0J+nRqAn1dbVy/FylrH9CtKQZ2+xyxr5Lxx+VQ+Gw5iYz/r0r+/TQWL1+nwr1XCyzddhqamhoY3ssZ4U+i8c+LBNk52n5eC191aoxmA5egZ4eGH/3eiOh/Fi9agDZt2qK5c4tCiWRxwu/fR1xsLDQ0NNC/Ty+8evkStevUgdfU6ahZs5aSIqZyj6mHKGUikTxx4gROnz6NVq1aKXxsVlYWsrKy5NqE/DxINDRLKzz6COrZ2+Ci3xToaFdAakYWBkzZggf/X43ce/IWoqITEB2fhAY1bbDou56oZWeBgVO3AgBS07PgOno19q0YA+/RXQAAj6Pi0GPCeuTlvRnWNjPWx5b5QzFilh9S0jJVc5NEBAA4+ccJhIffx669B0Qd/+zZvwCAX9avw9TpM2Hz2WfY4bsdo4Z/jWMnTsPYxKQUoyWiDykT37Vta2tb5ETqkvDx8YGxsbHclhsbVMoRkrL9/TQWzQb6oM2wZdiy/wq2LPgadapbAQB+PXQVAYHhCHv8AntO3oLH7J3o2bERqlV+861HOlIt/DJ3CALvPkHbYcvQYcQK3I+IxqE146AjfTNNYsPsQdh76hau3o5Q2T0SERATHY2lS36Ez08/QyqVijqH8P9rDY8aMxYunV3hUK8+FvzoA4lEgjNnTpVmuKRG+F3b4pSJRHL58uWYPn06nj59qvCx3t7eSEpKktsqWDqVfpCkVDm5eXjy70vcCf8Xc9YeQ8jfzzFhULsi+94MeQoAqGFrDgAY0LUpqtiYYczc3xB0Pwo3Qp7C3dsXVT+riO7tHAEAbb+ohUlfd0TKzdVIubkav8wdAhNDPaTcXI1hPZt/jFskIgD374ch4dUrDOz3FZo4OqCJowNu3byBXf470cTRAXl5ecWeo5L5m7/71WvUkLVpa2vjs8q2iImOVlrsRFRYmRjaHjp0KNLT01GjRg3o6ekVetgmISHhPUcCUqm00G+1HNb+9GlIJJBqF/3xbFi7MgAg5mUSAEBPRxv5+QIEQZD1yRcECMKb8wBAO/fl0NT43+9NX7ZzxJThLmg/fAVexCUq6S6I6F3NmjfHgSO/y7XN/cEbVatXxwiP0dDULP7fb4d69aGtrY2nTyPRxKkpACAnJwcvXjyHtbWNUuKm8q+8Vw6VpUwkkqtWrVJ1CKRCC77tgdNXw/Bv9GsY6utgQNemaNO0JrqP34BqlSthQNemOH0lDK8S09Cg1mdYOuUr/Bn0CKGPXgAAzv31AIsn9cIq7/7YuOcSNCQSTB3RGbl5ebh0681yIA8jY+Wu2cShCvIFAfcjWL0g+pj09Q0KPRCjq6cHE2MTWfvL+Hi8fPkS/0ZFAQAeP/obenr6sLa2hrGJCQwMDNCv/0BsXL8WVlbWsLGxge/2bQCAzq5dPu4NEak5lSeSOTk5uHTpEmbPno1q1bgcizoyNzPAtoXDYFXJCEmpmQh99Bzdx2/A+esPUNnSBB2a1Ybn4PbQ19XGs9jXOHIuGEu2npYd//fTWPT5bhN++KYrLvpNQX6+gLsPnqHnhA2IeZmswjsjIjH279uDXzask70eMWwIAGDBIh/07P0VAMBr6nRoVqiAH7ynIyszEw0cG2LLr34wMjZWScz06WNBUhyJ8PZ4oIoYGxsjODi41BJJ3caepXIeIip7Xt9cV3wnIvok6aiwvGU/9aTSzv14WVelnVvVysTDNr169cKRI0dUHQYRERGpKT61LY7Kh7YBoGbNmliwYAGuXr0KJycn6Ovry+2fOHGiiiIjIiIidVDO8z2lKROJ5LZt22BiYoKgoCAEBcmvASmRSJhIEhEREZVBZSKRjIyMLL4TERERkZKU9yFoZSkTcyTfJgjy6wESERERUdlUZhLJHTt2oEGDBtDV1YWuri4cHR2xc+dOVYdFREREakAiUd5WnpWJoe0VK1Zg9uzZ8PT0RMuWLQEAV65cwdixY/Hy5Ut4eXmpOEIiIiIieleZSCTXrl2LjRs3YtiwYbK2Hj16oF69epg3bx4TSSIiIlIqDY1yXjpUkjIxtB0dHY0WLVoUam/RogWio/kVdkRERERlUZlIJO3t7bFv375C7Xv37kXNmjVVEBERERGpE86RFKdMDG3Pnz8fAwYMwOXLl2VzJK9evYpz584VmWASERERlSYu/yNOmahI9unTB9evX0fFihVx5MgRHDlyBJUqVcKNGzfQu3dvVYdHREREREUoExVJAHBycoK/v7+qwyAiIiI1xIKkOCpNJDU0NIotJUskEuTm5n6kiIiIiIiopFSaSB4+fPi9+wIDA7FmzRrk5+d/xIiIiIhIHXGOpDgqTSR79uxZqO3hw4eYOXMmfv/9dwwZMgQLFixQQWREREREVJwy8bANALx48QKjR49GgwYNkJubi+DgYPj5+cHOzk7VoREREVE5J5FIlLaVZypPJJOSkjBjxgzY29sjLCwM586dw++//4769eurOjQiIiIi+gCVDm0vXboUP/30E6ysrLB79+4ih7qJiIiIlK2cFw6VRqWJ5MyZM6Grqwt7e3v4+fnBz8+vyH6HDh36yJERERGROinvQ9DKotJEctiwYfzBEREREX2iVJpI+vr6qvLyRERERAA4tC2Wyh+2ISIiIqJPU5n5ikQiIiIiVeFUO3FYkSQiIiIiUViRJCIiIrXHgqQ4rEgSERERkSisSBIREZHa4xxJcViRJCIiIiJRWJEkIiIitceCpDhMJImIiEjtcWhbHA5tExEREZEorEgSERGR2mNBUhxWJImIiIjKCB8fH3z++ecwNDSEhYUFevXqhYcPH8r1adeuHSQSidw2duxYuT5RUVFwc3ODnp4eLCwsMG3aNOTm5sr1uXjxIpo0aQKpVAp7e3v4+voqHC8TSSIiIlJ77yZmpbkp4tKlS5gwYQL++usvnD17Fjk5OejcuTPS0tLk+o0ePRrR0dGybenSpbJ9eXl5cHNzQ3Z2Nq5duwY/Pz/4+vpizpw5sj6RkZFwc3ND+/btERwcjEmTJmHUqFE4ffq0QvFyaJuIiIiojDh16pTca19fX1hYWCAoKAht2rSRtevp6cHKyqrIc5w5cwb3799HQEAALC0t0ahRIyxcuBAzZszAvHnzoK2tjV9++QXVqlXD8uXLAQB169bFlStXsHLlSri6upY4XlYkiYiISO1JJMrbsrKykJycLLdlZWWVKK6kpCQAgJmZmVy7v78/KlWqhPr168Pb2xvp6emyfYGBgWjQoAEsLS1lba6urkhOTkZYWJisj4uLi9w5XV1dERgYqND7xkSSiIiISIl8fHxgbGwst/n4+BR7XH5+PiZNmoSWLVuifv36svbBgwfjt99+w4ULF+Dt7Y2dO3di6NChsv0xMTFySSQA2euYmJgP9klOTkZGRkaJ741D20RERKT2lLmOpLe3NyZPnizXJpVKiz1uwoQJCA0NxZUrV+Tax4wZI/tzgwYNYG1tjY4dOyIiIgI1atQonaBLiIkkERERqT1lLv8jlUpLlDi+zdPTE8ePH8fly5dRuXLlD/Zt1qwZAODx48eoUaMGrKyscOPGDbk+sbGxACCbV2llZSVre7uPkZERdHV1Sxwnh7aJiIiIyghBEODp6YnDhw/j/PnzqFatWrHHBAcHAwCsra0BAM7OzggJCUFcXJysz9mzZ2FkZAQHBwdZn3Pnzsmd5+zZs3B2dlYoXlYkiYiISO2Vla9InDBhAnbt2oWjR4/C0NBQNqfR2NgYurq6iIiIwK5du9CtWzdUrFgR9+7dg5eXF9q0aQNHR0cAQOfOneHg4ICvv/4aS5cuRUxMDGbNmoUJEybIKqNjx47FunXrMH36dIwcORLnz5/Hvn37cOLECYXiZUWSiIiIqIzYuHEjkpKS0K5dO1hbW8u2vXv3AgC0tbUREBCAzp07o06dOpgyZQr69OmD33//XXYOTU1NHD9+HJqamnB2dsbQoUMxbNgwLFiwQNanWrVqOHHiBM6ePYuGDRti+fLl2Lp1q0JL/wCARBAEoXRuvezQbeyp6hCISEle31yn6hCISEl0VDhO2mbFVaWd+/Lklko7t6qxIklEREREonCOJBEREam9MjJF8pPDiiQRERERicKKJBEREam9svLU9qeGiSQRERGpPeaR4nBom4iIiIhEEVWRjIiIwKpVqxAeHg4AcHBwwHfffffRv9+RiIiIqDRwaFschSuSp0+fhoODA27cuAFHR0c4Ojri+vXrqFevHs6ePauMGImIiIioDFK4Ijlz5kx4eXlhyZIlhdpnzJiBTp06lVpwRERERB8DC5LiKFyRDA8Ph4eHR6H2kSNH4v79+6USFBERERGVfQonkubm5ggODi7UHhwcDAsLi9KIiYiIiOij0pBIlLaVZwoPbY8ePRpjxozBkydP0KJFCwDA1atX8dNPP2Hy5MmlHiARERERlU0KJ5KzZ8+GoaEhli9fDm9vbwCAjY0N5s2bh4kTJ5Z6gERERETKVs4Lh0qjUCKZm5uLXbt2YfDgwfDy8kJKSgoAwNDQUCnBEREREX0MXP5HHIXmSFaoUAFjx45FZmYmgDcJJJNIIiIiIvWk8MM2X3zxBe7cuaOMWIiIiIhUQkOivK08U3iO5Pjx4zFlyhQ8e/YMTk5O0NfXl9vv6OhYasERERERUdmlcCI5cOBAAJB7sEYikUAQBEgkEuTl5ZVedEREREQfAedIiqNwIhkZGamMOIiIiIjoE6NwImlnZ6eMOIiIiIhUhgVJcUqUSB47dgxdu3aFlpYWjh079sG+PXr0KJXAiIiIiKhsK1Ei2atXL8TExMDCwgK9evV6bz/OkSQiIqJPkQQsSYpRokQyPz+/yD8TERERlQflfZkeZVF4Hcm3FSxMTkRERETqR+FEMi8vDwsXLsRnn30GAwMDPHnyBMCb7+Detm1bqQdIREREpGwSiURpW3mmcCL5448/wtfXF0uXLoW2trasvX79+ti6dWupBkdEREREZZfCieSOHTuwefNmDBkyBJqamrL2hg0b4sGDB6UaHBEREdHHIJEobyvPFE4knz9/Dnt7+0Lt+fn5yMnJKZWgiIiIiKjsUziRdHBwwJ9//lmo/cCBA2jcuHGpBEVERET0MWlIJErbyjOFv9lmzpw5cHd3x/Pnz5Gfn49Dhw7h4cOH2LFjB44fP66MGImIiIioDFK4ItmzZ0/8/vvvCAgIgL6+PubMmYPw8HD8/vvv6NSpkzJiJCIiIlIqzpEUR+GKJAC0bt0aZ8+eLe1YiIiIiFSivC/Toyz/aUFyIiIiIlJfJapImpqaljhTT0hI+E8BEREREX1sLEiKU6JEctWqVbI/v3r1CosWLYKrqyucnZ0BAIGBgTh9+jRmz56tlCCJiIiIqOyRCIIgKHJAnz590L59e3h6esq1r1u3DgEBAThy5EhpxieKbmPP4jsR0Sfp9c11qg6BiJRER9STG6VjgN8dpZ17r3v5XR5R4TmSp0+fRpcuXQq1d+nSBQEBAaUSFBERERGVfQonkhUrVsTRo0cLtR89ehQVK1YslaCIiIiIPiaJErfyTOEi8vz58zFq1ChcvHgRzZo1AwBcv34dp06dwpYtW0o9QCIiIiIqmxROJIcPH466detizZo1OHToEACgbt26uHLliiyxJCIiIvqUcB1JcURNa23WrBn8/f1LOxYiIiIildBgHinKf3o+KjMzE9nZ2XJtRkZG/ykgIiIiIvo0KJxIpqenY/r06di3bx9evXpVaH9eXl6pBEZERET0sXBoWxyFn9qeNm0azp8/j40bN0IqlWLr1q2YP38+bGxssGPHDmXESERERERlkMIVyd9//x07duxAu3btMGLECLRu3Rr29vaws7ODv78/hgwZoow4iYiIiJSGBUlxFK5IJiQkoHr16gDezIcs+G7tVq1a4fLly6UbHRERERGVWQonktWrV0dkZCQAoE6dOti3bx+AN5VKExOTUg2OiIiI6GOQSCRK28ozhRPJESNG4O7duwCAmTNnYv369dDR0YGXlxemTZtW6gESERERUdmk8BxJLy8v2Z9dXFzw4MEDBAUFwd7eHo6OjqUaHBEREdHHwHUkxflP60gCgJ2dHezs7EojFiIiIiKVKO9D0MpSokRyzZo1JT7hxIkTRQdDRERERJ+OEiWSK1eulHsdHx+P9PR02cM1iYmJ0NPTg4WFBRNJIiIi+uSwHilOiR62iYyMlG0//vgjGjVqhPDwcCQkJCAhIQHh4eFo0qQJFi5cqOx4iYiIiKiMUHiO5OzZs3HgwAHUrl1b1la7dm2sXLkSffv25YLkRERE9MnR4BxJURRe/ic6Ohq5ubmF2vPy8hAbG1sqQRERERFR2adwItmxY0d88803uH37tqwtKCgI48aNg4uLS6kGR0RERPQxSCTK28ozhRPJX3/9FVZWVmjatCmkUimkUim++OILWFpaYuvWrcqIkYiIiIjKIIXmSAqCgIyMDBw8eBDPnj1DeHg4gDdflVirVi2lBEhERESkbFxHUhyFE0l7e3uEhYWhZs2aqFmzprLiIiIiIqIyTqGhbQ0NDdSsWROvXr1SVjxEREREHx3nSIqj8BzJJUuWYNq0aQgNDVVGPEREREQfnYZEorStPFN4Hclhw4YhPT0dDRs2hLa2NnR1deX2JyQklFpwRERERFR2KZxIrlq1SglhEBEREalOWSkc+vj44NChQ3jw4AF0dXXRokUL/PTTT3JfBJOZmYkpU6Zgz549yMrKgqurKzZs2ABLS0tZn6ioKIwbNw4XLlyAgYEB3N3d4ePjgwoV/pf6Xbx4EZMnT0ZYWBhsbW0xa9YsDB8+XKF4FU4k3d3dFT2EiIiIiErg0qVLmDBhAj7//HPk5ubi+++/R+fOnXH//n3o6+sDALy8vHDixAns378fxsbG8PT0xFdffYWrV68CePMlMW5ubrCyssK1a9cQHR2NYcOGQUtLC4sXLwbw5uuv3dzcMHbsWPj7++PcuXMYNWoUrK2t4erqWuJ4JYIgCIreZEREBLZv346IiAisXr0aFhYWOHnyJKpUqYJ69eoperpSp9vYU9UhEJGSvL65TtUhEJGS6Chc3io9Ew6HK+3c63vXFX1sfHw8LCwscOnSJbRp0wZJSUkwNzfHrl270LdvXwDAgwcPULduXQQGBqJ58+Y4efIkvvzyS7x48UJWpfzll18wY8YMxMfHQ1tbGzNmzMCJEyfknnkZOHAgEhMTcerUqRLHp/DDNpcuXUKDBg1w/fp1HDp0CKmpqQCAu3fvYu7cuYqejoiIiKhcy8rKQnJystyWlZVVomOTkpIAAGZmZgDefJtgTk6O3LcJ1qlTB1WqVEFgYCAAIDAwEA0aNJAb6nZ1dUVycjLCwsJkfd79RkJXV1fZOUpK4dx/5syZWLRoESZPngxDQ0NZe4cOHbBuXdmoFCxbN1XVIRCRkvTZdkPVIRCRkpz45guVXVvhypoCfHx8MH/+fLm2uXPnYt68eR88Lj8/H5MmTULLli1Rv359AEBMTAy0tbVhYmIi19fS0hIxMTGyPm8nkQX7C/Z9qE9ycjIyMjIKPUz9PgonkiEhIdi1a1ehdgsLC7x8+VLR0xERERGVa97e3pg8ebJcm1QqLfa4CRMmIDQ0FFeuXFFWaP+ZwomkiYkJoqOjUa1aNbn2O3fu4LPPPiu1wIiIiIg+FmV+RaJUKi1R4vg2T09PHD9+HJcvX0blypVl7VZWVsjOzkZiYqJcVTI2NhZWVlayPjduyI/exMbGyvYV/H9B29t9jIyMSlyNBERUcgcOHIgZM2YgJiYGEokE+fn5uHr1KqZOnYphw4YpejoiIiIildOQKG9ThCAI8PT0xOHDh3H+/PlChTsnJydoaWnh3LlzsraHDx8iKioKzs7OAABnZ2eEhIQgLi5O1ufs2bMwMjKCg4ODrM/b5yjoU3COklI4kVy8eDHq1KkDW1tbpKamwsHBAW3atEGLFi0wa9YsRU9HRERERP9vwoQJ+O2337Br1y4YGhoiJiYGMTExyMjIAAAYGxvDw8MDkydPxoULFxAUFIQRI0bA2dkZzZs3BwB07twZDg4O+Prrr3H37l2cPn0as2bNwoQJE2SV0bFjx+LJkyeYPn06Hjx4gA0bNmDfvn3w8vJSKN4SL//Tt29fjBo1Cq6urpBIJPj3338REhKC1NRUNG7cGDVr1lTowsq0/upTVYdAREryR2hc8Z2I6JOkyodtJh97oLRzr+hRp8R93zfEvn37dtli4QULku/evVtuQfKCYWsA+OeffzBu3DhcvHgR+vr6cHd3x5IlSwotSO7l5YX79++jcuXKmD17tsILkpc4kezYsSMuXrwIGxsbjBgxAiNGjChUbi0rmEgSlV9MJInKLyaSn54SD22fO3cOT548gYeHB3777TfY29ujQ4cO2LVrV4nXQiIiIiIqiyQSidK28kyhOZJ2dnaYN28enjx5grNnz8LGxgajR4+GtbU1JkyYgKCgIGXFSURERERljOgvI+rQoQM6dOiAlJQU7Nq1C99//z02bdqE3Nzc0oyPiIiISOkUfbqa3vhP32oZGRkJX19f+Pr6IikpqdBX7RARERFR+aVwIpmZmYkDBw7g119/xeXLl2FrawsPDw+MGDECtra2yoiRiIiISKnK+VRGpSlxInnjxg38+uuv2Lt3LzIzM9G7d2+cOnUKHTt2LPcTSYmIiKh802AuI0qJE8nmzZujYcOGWLhwIYYMGQJTU1NlxkVEREREZVyJE8lbt26hSZMmyoyFiIiISCUU/qo/AqDA+8YkkoiIiIje9p+e2iYiIiIqDzhFUhxWcomIiIhIFFYkiYiISO3xqW1xRCeS8fHxePjwIQCgdu3aMDc3L7WgiIiIiKjsU3hoOy0tDSNHjoSNjQ3atGmDNm3awMbGBh4eHkhPT1dGjERERERKJZEobyvPFE4kJ0+ejEuXLuHYsWNITExEYmIijh49ikuXLmHKlCnKiJGIiIhIqTQkytvKM4WHtg8ePIgDBw6gXbt2srZu3bpBV1cX/fv3x8aNG0szPiIiIiIqoxROJNPT02FpaVmo3cLCgkPbRERE9EniwzbiKDy07ezsjLlz5yIzM1PWlpGRgfnz58PZ2blUgyMiIiKiskvhiuTq1avh6uqKypUro2HDhgCAu3fvQkdHB6dPny71AImIiIiUjQVJcRROJOvXr49Hjx7B398fDx48AAAMGjQIQ4YMga6ubqkHSERERERlk6h1JPX09DB69OjSjoWIiIhIJcr709XKUqJE8tixYyU+YY8ePUQHQ0RERESfjhIlkr169SrRySQSCfLy8v5LPEREREQfnQQsSYpRokQyPz9f2XEQERERqQyHtsVRePkfIiIiIiJAxMM2CxYs+OD+OXPmiA6GiIiISBVYkRRH4UTy8OHDcq9zcnIQGRmJChUqoEaNGkwkiYiIiNSEwonknTt3CrUlJydj+PDh6N27d6kERURERPQxSbgiuSilMkfSyMgI8+fPx+zZs0vjdERERET0CRC1IHlRkpKSkJSUVFqnIyIiIvpoOEdSHIUTyTVr1si9FgQB0dHR2LlzJ7p27VpqgRERERFR2aZwIrly5Uq51xoaGjA3N4e7uzu8vb1LLTAiIiKij4VTJMVROJGMjIxURhxEREREKqPBTFIUhR+2GTlyJFJSUgq1p6WlYeTIkaUSFBERERGVfQonkn5+fsjIyCjUnpGRgR07dpRKUEREREQfk4ZEeVt5VuKh7eTkZAiCAEEQkJKSAh0dHdm+vLw8/PHHH7CwsFBKkERERERU9pQ4kTQxMYFEIoFEIkGtWrUK7ZdIJJg/f36pBkdERET0MXCKpDglTiQvXLgAQRDQoUMHHDx4EGZmZrJ92trasLOzg42NjVKCJCIiIqKyp8SJZNu2bQG8eWrb1tYWGhql8qU4RERERCqnAZYkxVB4+R87OzskJiZi27ZtCA8PBwDUq1cPI0eOhLGxcakHSERERERlU7FlxSdPnsi9vnXrFmrUqIGVK1ciISEBCQkJWLFiBWrUqIHbt28rLVAiIiIiZZFIlLeVZ8VWJPfs2YOIiAhs2bIFGhoa8PLyQo8ePbBlyxZUqPDm8NzcXIwaNQqTJk3C5cuXlR40ERERUWkq78v0KEuxFckpU6ZAU1MT3bp1A/CmIjljxgxZEgkAFSpUwPTp03Hr1i3lRUpEREREZUqxiaRUKsXmzZsxbNgwAICRkRGioqIK9fv3339haGhY+hESERERKZmGRKK0rTwr8aPXgwcPBgAMGDAAHh4e2Lt3L/7991/8+++/2LNnD0aNGoVBgwYpLVAiIiIiKlsUfmp72bJlkEgkGDZsGHJzcwEAWlpaGDduHJYsWVLqARIREREpWzkvHCqNwomktrY2Vq9eDR8fH0RERAAAatSoAT09vSK/g5uIiIiIyifRq4rr6emhQYMGaNCgATQ1NbFixQpUq1atNGMjIiIi+ig4R1KcEieSWVlZ8Pb2RtOmTdGiRQscOXIEALB9+3ZUq1YNK1euhJeXl7LiJCIiIqIypsRD23PmzMGmTZvg4uKCa9euoV+/fhgxYgT++usvrFixAv369YOmpqYyYyUiIiJSinJeOFSaEieS+/fvx44dO9CjRw+EhobC0dERubm5uHv3LiR894mIiOgTJnqun5or8fv27NkzODk5AQDq168PqVQKLy8vJpFEREREaqrEFcm8vDxoa2v/78AKFWBgYKCUoIiIiIg+JhbGxClxIikIAoYPHw6pVAoAyMzMxNixY6Gvry/X79ChQ6UbIRERERGVSSVOJN3d3eVeDx06tNSDISIiIlIF1iPFKXEiuX37dmXGQURERESfGIW/2YaIiIiovCnvC4crC592JyIiIiJRWJEkIiIitcd6pDhMJImIiEjtcWRbHA5tExEREZEorEgSERGR2uOC5OKwIklERERUhly+fBndu3eHjY0NJBIJjhw5Ird/+PDhkEgkcluXLl3k+iQkJGDIkCEwMjKCiYkJPDw8kJqaKtfn3r17aN26NXR0dGBra4ulS5cqHCsTSSIiIlJ7GkrcFJWWloaGDRti/fr17+3TpUsXREdHy7bdu3fL7R8yZAjCwsJw9uxZHD9+HJcvX8aYMWNk+5OTk9G5c2fY2dkhKCgIP//8M+bNm4fNmzcrFCuHtomIiIjKkK5du6Jr164f7COVSmFlZVXkvvDwcJw6dQo3b95E06ZNAQBr165Ft27dsGzZMtjY2MDf3x/Z2dn49ddfoa2tjXr16iE4OBgrVqyQSziLw4okERERqb13h4pLc8vKykJycrLclpWV9Z/ivXjxIiwsLFC7dm2MGzcOr169ku0LDAyEiYmJLIkEABcXF2hoaOD69euyPm3atIG2trasj6urKx4+fIjXr1+XOA4mkkRERERK5OPjA2NjY7nNx8dH9Pm6dOmCHTt24Ny5c/jpp59w6dIldO3aFXl5eQCAmJgYWFhYyB1ToUIFmJmZISYmRtbH0tJSrk/B64I+JcGhbSIiIlJ7ynxm29vbG5MnT5Zrk0qlos83cOBA2Z8bNGgAR0dH1KhRAxcvXkTHjh1Fn1cMViSJiIiIlEgqlcLIyEhu+y+J5LuqV6+OSpUq4fHjxwAAKysrxMXFyfXJzc1FQkKCbF6llZUVYmNj5foUvH7f3MuiMJEkIiIitafMOZLK9uzZM7x69QrW1tYAAGdnZyQmJiIoKEjW5/z588jPz0ezZs1kfS5fvoycnBxZn7Nnz6J27dowNTUt8bWZSBIREZHaK0vL/6SmpiI4OBjBwcEAgMjISAQHByMqKgqpqamYNm0a/vrrLzx9+hTnzp1Dz549YW9vD1dXVwBA3bp10aVLF4wePRo3btzA1atX4enpiYEDB8LGxgYAMHjwYGhra8PDwwNhYWHYu3cvVq9eXWgIviTvGxERERGVEbdu3ULjxo3RuHFjAMDkyZPRuHFjzJkzB5qamrh37x569OiBWrVqwcPDA05OTvjzzz/lhsv9/f1Rp04ddOzYEd26dUOrVq3k1og0NjbGmTNnEBkZCScnJ0yZMgVz5sxRaOkfAJAIgiCUzm2XHeuvPlV1CESkJH+ExhXfiYg+SSe++UJl1z58r+RPKiuqt2PJ5xx+aliRJCIiIiJRuPwPERERqT3lPxJTPrEiSURERESisCJJREREau8jrNJTLrEiSURERESisCJJREREak+DsyRFYSJJREREao9D2+JwaJuIiIiIRGFFkoiIiNSehEPborAiSURERESisCJJREREao9zJMVhRZKIiIiIRGFFkoiIiNQel/8RhxVJIiIiIhKlTCSSf/75J4YOHQpnZ2c8f/4cALBz505cuXJFxZERERGROpBIlLeVZypPJA8ePAhXV1fo6urizp07yMrKAgAkJSVh8eLFKo6OiIiI1AETSXFUnkguWrQIv/zyC7Zs2QItLS1Ze8uWLXH79m0VRkZEREREH6Lyh20ePnyINm3aFGo3NjZGYmLixw+IiIiI1A4XJBdH5RVJKysrPH78uFD7lStXUL16dRVEREREREQlofJEcvTo0fjuu+9w/fp1SCQSvHjxAv7+/pg6dSrGjRun6vCIiIhIDWhIlLeVZyof2p45cyby8/PRsWNHpKeno02bNpBKpZg6dSq+/fZbVYdHRERERO+h8kRSIpHghx9+wLRp0/D48WOkpqbCwcEBBgYGqg6NiIiI1ATnSIqj8kSygLa2NhwcHFQdBhERERGVkMoTyfbt20PygUWWzp8//xGjISIiInVU3td7VBaVJ5KNGjWSe52Tk4Pg4GCEhobC3d1dNUERERGRWuHQtjgqTyRXrlxZZPu8efOQmpr6kaMhIiIiopJS+fI/7zN06FD8+uuvqg6DiIiI1ACX/xGnzCaSgYGB0NHRUXUYRERERPQeKh/a/uqrr+ReC4KA6Oho3Lp1C7Nnz1ZRVERERKROOEdSHJUnksbGxnKvNTQ0ULt2bSxYsACdO3dWUVREREREVByVJ5Lbt29XdQikYs8fhiDo1H7EP32EtKQEuHnORY0mLeT6JLyIwtUD2/D84T3k5+XBzMYObhNmw7CiBQAgLSkBV/Ztxb9ht5GdmQ5TK1t8/uVA2DdtLTtHZmoyLu3agCfBb76O096pFdoMHgdtHd2Per9E6qRfI2u0qGaKyia6yM7LR3hMKrZf/xfPkzJlfTxbV0Wjz4xgpq+NzJw8hMe+6fMs8U2fama66NfYBg5WBjDS0UJcShb+uB+HY6GxctdqYG2IUc5VYGemi/jUbOy9/QIBf7/8qPdLny4u/yOOyhNJopysTJjbVke9Vq44sX5Bof2JcS9wwGcyHFp3QbOeX0NbVw8Jz/+Bppa2rM+ZrT8jOz0VX06cB10DYzy8fgEnNy7GgDlrYWFnDwA4vfknpCUloPcUH+Tn5eLsr8tx3m8Vunzj/dHulUjdNLAxxImwOPwdnwZNCeD+hS0WudXG2H0hyMrNBwA8fpmGC49fIT4lC4Y6FTDE6TMs7FYbHrvvIl8A7M31kZiRg2Xnn+BlajbqWhnAs3VV5AsCjofFAQAsDbUxr2st/HE/DsvOR6DhZ0aY2LYaEtJzcPtZkirfAqJyTeWJpKmp6QcXJH9bQkKCkqMhVajq+DmqOn7+3v2Bh3xh5/gFWvUfJWszsbCR6xPz+D7aff0trKrXAQB80X0wgs8cQtw/j2BhZ4+EF1H4J/QWBsxeC8tqtQAAbYeMx7FVs9Gq/xgYmFZUwp0R0Zw//pZ7veLiE+x2bwJ7c32ERacAAE6Fx8v2x6VmY8fNZ1jfrwEsDKWISc7C2YfyVcWYlCzUsTRAi2pmskSym4MFYlKysO2vfwEA/yZmwsHKEL0cLZlIUomwICmOyhPJ2bNnY9GiRXB1dYWzszOAN09snz59GrNnz4aZmZmKIyRVEvLz8fTuDTh17Ycjy79HfNRjGFWyQlO3gXLD31b2Dnh04xKqOX4BqZ4BHt28jNycbFSu7QgAiI4Ih1TPQJZEAkAVhyaQSCSIffIABk4tP/q9EakjfW1NAEBqZm6R+6UVNNCptjlikjPxMjX7g+dJyfrfOepYGiD4ebJcn9vPkjDGuUopRE3qQINj26KoPJG8evUqFixYAE9PT1nbxIkTsW7dOgQEBODIkSMfPD4rKwtZWVlybTnZWdDSliojXPrI0lMSkZOVgVt/7IXzV8PRsp8H/gm5hRPrF+Cr6UtliWK3cT/g5MbF2DyxHzQ0NVFBWwo3z7kwsfzszXmSEqBraCJ3bg1NTejoGyItmZVuoo9BAmBMCzuERafgn9cZcvvcHCwworktdLU08e/rDPxw4iFy84Uiz1PX0gCtq5th3qn/VTtNdbWRmC5feUxMz4G+tAK0NSXIziv6XET036h8HcnTp0+jS5cuhdq7dOmCgICAYo/38fGBsbGx3HZm50ZlhEoqIPz/f0iqN3ZG485fwbxKDTR1G4BqDZsh9MIJWb/Aw37ISk9F76lLMGD2WjTu3AcnN/6Il88iVRU6Eb1jXCs72Jnp4qdzjwvtu/D4FSYeCMX0Y+F4kZQJbxd7aGkWrhDZmepitmtN7Ap6gTvPkgvtJxJLosStPFN5IlmxYkUcPXq0UPvRo0dRsWLx89a8vb2RlJQkt3X+epwyQiUV0DU0goamJsxs7OTazaxtkZLwZm5UYtwL3Dt3DC4jJ8PWoTHMq9RAs55DYVm1Ju6dPwYA0DM2Q0ZKotw58vPykJmWAn0jTp8gUraxLe3whZ0JvH8Px6u0nEL707Pz8CI5C2HRKVh89jEqm+igRVVTuT62Jjr48cs6OBUej713Xsjte52RDRM9Lbk2Ez0tpGXlshpJpEQqH9qeP38+Ro0ahYsXL6JZs2YAgOvXr+PUqVPYsmVLscdLpVJIpfLD2FraHKosLzQraMGiai28jnkm1/465rls6Z/c7DdTGyQS+d+LJBqasoqmdY26yEpPRdzTR7CoWhMA8G94MARBgOX/P6BDRMoxtqUdnKuZwvtYOGJT3j/v8V1amv/7O13FVBeLv6yDc3+/xI6bzwr1fRCbiqa2JnJtjT8zxoO4VNFxk5op76VDJVF5RXL48OG4evUqjIyMcOjQIRw6dAhGRka4cuUKhg8frurw6CPIzsxAfFQE4qMiAADJL2MQHxWBlFdvKo5OXfrh0Y1LCL30BxJjn+PuuaOIvPsXHNt3BwCYWtnC2MIG53esRsyTB0iMe4Hbpw4g6v5t2QM5ZjZVYFe/Kc75rkLMkwd48SgMl/zXo9YXbfnENpESjW9lh/Y1K+LncxHIyMmHqa4WTHW1oP3/w9ZWhlL0a2QN+0p6MDfQRl1LA3zfyR7ZeQJuRiUCeDOc7dO9Du48S8KRezGycxjp/K8W8sf9OFgZSTGimS0qm+jAzcECrWuY4ci92KLCIqJSIhEEodzV/NdffarqEEgBzx7cxaGl0wu1123ZCZ08pgIAwv48jVsn9iD19UuYWlVGs15fo0bj/z21nRj7HFcPbMOLR2HIycyAiYUNGnfpi7otXGR9MlOTcdF/PSKDr0OiUbAg+XguSP6J+SM0TtUhkAJOfPNFke0rLzxBwN8vYaanhYltq8G+kj4MpJpIzMhBaHQKdge9kC1aPtjpMwxp+lmhc8SmZGHkrruy1w2sDTG6RRVUMdXFy9Rs7OGC5J+c931ePobrEcpbJqpZDePiO32iylQimZmZiexs+WEPIyMjhc/DRJKo/GIiSVR+MZH89Kh8aDs9PR2enp6wsLCAvr4+TE1N5TYiIiIiZZNIlLeVZypPJKdNm4bz589j48aNkEql2Lp1K+bPnw8bGxvs2LFD1eERERGRGuDyP+Ko/Knt33//HTt27EC7du0wYsQItG7dGvb29rCzs4O/vz+GDBmi6hCJiIiIqAgqr0gmJCSgevXqAN7Mhyz4Pu1WrVrh8uXLqgyNiIiI1AVLkqKoPJGsXr06IiPffPtInTp1sG/fPgBvKpUmJiYqjIyIiIiIPkTlieSIESNw9+6b5RtmzpyJ9evXQ0dHB15eXpg2bZqKoyMiIiJ1IFHi/8ozlc+R9PLykv3ZxcUFDx48QFBQEOzt7eHo6KjCyIiIiIjoQ1RekdyxYweysrJkr+3s7PDVV1+hTp06fGqbiIiIPgou/yOOyhPJESNGICmp8CKgKSkpGDFihAoiIiIiIqKSUPnQtiAIkBSRrj979gzGxuV3JXgiIiIqO8p54VBpVJZINm7cGBKJBBKJBB07dkSFCv8LJS8vD5GRkejSpYuqwiMiIiJ1wkxSFJUlkr169QIABAcHw9XVFQYGBrJ92traqFq1KurXr6+i6IiIiIioOCpLJOfOnQsAqFq1KgYMGAAdHR0Ab+ZG7t69GytXrkRQUBDy8vJUFSIRERGpifK+TI+yqPxhG3d3d+jo6ODy5ctwd3eHtbU1li1bhg4dOuCvv/5SdXhERERE9B4qfdgmJiYGvr6+2LZtG5KTk9G/f39kZWXhyJEjcHBwUGVoREREpEbK+zI9yqKyimT37t1Ru3Zt3Lt3D6tWrcKLFy+wdu1aVYVDRERERApSWUXy5MmTmDhxIsaNG4eaNWuqKgwiIiIizpAUSWUVyStXriAlJQVOTk5o1qwZ1q1bh5cvX6oqHCIiIiJSkMoSyebNm2PLli2Ijo7GN998gz179sDGxgb5+fk4e/YsUlJSVBUaERERqRuJErdyTOVPbevr62PkyJG4cuUKQkJCMGXKFCxZsgQWFhbo0aOHqsMjIiIiNSBR4v/KM5Unkm+rXbs2li5dimfPnmH37t2qDoeIiIiIPqBMJZIFNDU10atXLxw7dkzVoRAREZEakEiUtynq8uXL6N69O2xsbCCRSHDkyBG5/YIgYM6cObC2toauri5cXFzw6NEjuT4JCQkYMmQIjIyMYGJiAg8PD6Smpsr1uXfvHlq3bg0dHR3Y2tpi6dKlCsdaJhNJIiIiInWVlpaGhg0bYv369UXuX7p0KdasWYNffvkF169fh76+PlxdXZGZmSnrM2TIEISFheHs2bM4fvw4Ll++jDFjxsj2Jycno3PnzrCzs0NQUBB+/vlnzJs3D5s3b1YoVokgCIK42yy71l99quoQiEhJ/giNU3UIRKQkJ775QmXXDn2WWnwnkepXNhB9rEQiweHDh9GrVy8Ab6qRNjY2mDJlCqZOnQoASEpKgqWlJXx9fTFw4ECEh4fDwcEBN2/eRNOmTQEAp06dQrdu3fDs2TPY2Nhg48aN+OGHHxATEwNtbW0AwMyZM3HkyBE8ePCgxPGxIklERESkRFlZWUhOTpbbsrKyRJ0rMjISMTExcHFxkbUZGxujWbNmCAwMBAAEBgbCxMRElkQCgIuLCzQ0NHD9+nVZnzZt2siSSABwdXXFw4cP8fr16xLHw0SSiIiISInL//j4+MDY2Fhu8/HxERVmTEwMAMDS0lKu3dLSUrYvJiYGFhYWcvsrVKgAMzMzuT5FnePta5SESr9rm4iIiKi88/b2xuTJk+XapFKpiqIpXUwkiYiISO0pc71HqVRaaomjlZUVACA2NhbW1tay9tjYWDRq1EjWJy5Ofj55bm4uEhISZMdbWVkhNjZWrk/B64I+JcGhbSIiIqJPRLVq1WBlZYVz587J2pKTk3H9+nU4OzsDAJydnZGYmIigoCBZn/PnzyM/Px/NmjWT9bl8+TJycnJkfc6ePYvatWvD1NS0xPEwkSQiIiK1V5bWkUxNTUVwcDCCg4MBvHnAJjg4GFFRUZBIJJg0aRIWLVqEY8eOISQkBMOGDYONjY3sye66deuiS5cuGD16NG7cuIGrV6/C09MTAwcOhI2NDQBg8ODB0NbWhoeHB8LCwrB3716sXr260BB8cTi0TURERGqvLH2R4a1bt9C+fXvZ64Lkzt3dHb6+vpg+fTrS0tIwZswYJCYmolWrVjh16hR0dHRkx/j7+8PT0xMdO3aEhoYG+vTpgzVr1sj2Gxsb48yZM5gwYQKcnJxQqVIlzJkzR26tyZLgOpJE9EnhOpJE5Zcq15EMf5GmtHPXtdFX2rlVjRVJIiIiorJUkvyEcI4kEREREYnCiiQRERGpPWUu/1OesSJJRERERKKwIklERERqT8wyPcSKJBERERGJxIokERERqT0WJMVhIklERETETFIUDm0TERERkSisSBIREZHa4/I/4rAiSURERESisCJJREREao/L/4jDiiQRERERicKKJBEREak9FiTFYUWSiIiIiERhRZKIiIiIJUlRmEgSERGR2uPyP+JwaJuIiIiIRGFFkoiIiNQel/8RhxVJIiIiIhKFFUkiIiJSeyxIisOKJBERERGJwookEREREUuSorAiSURERESisCJJREREao/rSIrDRJKIiIjUHpf/EYdD20REREQkCiuSREREpPZYkBSHFUkiIiIiEoUVSSIiIlJ7nCMpDiuSRERERCQKK5JEREREnCUpCiuSRERERCQKK5JERESk9jhHUhwmkkRERKT2mEeKw6FtIiIiIhKFFUkiIiJSexzaFocVSSIiIiIShRVJIiIiUnsSzpIUhRVJIiIiIhKFFUkiIiIiFiRFYUWSiIiIiERhRZKIiIjUHguS4jCRJCIiIrXH5X/E4dA2EREREYnCiiQRERGpPS7/Iw4rkkREREQkCiuSRERERCxIisKKJBERERGJwookERERqT0WJMVhRZKIiIiIRGFFkoiIiNQe15EUh4kkERERqT0u/yMOh7aJiIiISBRWJImIiEjtcWhbHFYkiYiIiEgUJpJEREREJAoTSSIiIiIShXMkiYiISO1xjqQ4rEgSERERkSisSBIREZHa4zqS4jCRJCIiIrXHoW1xOLRNREREVEbMmzcPEolEbqtTp45sf2ZmJiZMmICKFSvCwMAAffr0QWxsrNw5oqKi4ObmBj09PVhYWGDatGnIzc1VSrysSBIREZHaK0sFyXr16iEgIED2ukKF/6VrXl5eOHHiBPbv3w9jY2N4enriq6++wtWrVwEAeXl5cHNzg5WVFa5du4bo6GgMGzYMWlpaWLx4canHykSSiIiISImysrKQlZUl1yaVSiGVSovsX6FCBVhZWRVqT0pKwrZt27Br1y506NABALB9+3bUrVsXf/31F5o3b44zZ87g/v37CAgIgKWlJRo1aoSFCxdixowZmDdvHrS1tUv13ji0TURERCRR3ubj4wNjY2O5zcfH572hPHr0CDY2NqhevTqGDBmCqKgoAEBQUBBycnLg4uIi61unTh1UqVIFgYGBAIDAwEA0aNAAlpaWsj6urq5ITk5GWFjYf36b3sWKJBEREZESeXt7Y/LkyXJt76tGNmvWDL6+vqhduzaio6Mxf/58tG7dGqGhoYiJiYG2tjZMTEzkjrG0tERMTAwAICYmRi6JLNhfsK+0MZEkIiIitafM5X8+NIz9rq5du8r+7OjoiGbNmsHOzg779u2Drq6uskIUjUPbRERERGWUiYkJatWqhcePH8PKygrZ2dlITEyU6xMbGyubU2llZVXoKe6C10XNu/yvmEgSERGR2pNIlLf9F6mpqYiIiIC1tTWcnJygpaWFc+fOyfY/fPgQUVFRcHZ2BgA4OzsjJCQEcXFxsj5nz56FkZERHBwc/lswReDQNhEREVEZMXXqVHTv3h12dnZ48eIF5s6dC01NTQwaNAjGxsbw8PDA5MmTYWZmBiMjI3z77bdwdnZG8+bNAQCdO3eGg4MDvv76ayxduhQxMTGYNWsWJkyYUOLhdUUwkSQiIiK1V1bWkXz27BkGDRqEV69ewdzcHK1atcJff/0Fc3NzAMDKlSuhoaGBPn36ICsrC66urtiwYYPseE1NTRw/fhzjxo2Ds7Mz9PX14e7ujgULFiglXokgCIJSzqxC668+VXUIRKQkf4TGFd+JiD5JJ775QmXXTs9RXjqkp1VW0tTSxzmSRERERCQKh7aJiIhI7Slz+Z/yjBVJIiIiIhKFFUkiIiJSe/91mR51xYokEREREYlSLp/aJvWRlZUFHx8feHt7K2V9LCJSHf79Jir7mEjSJy05ORnGxsZISkqCkZGRqsMholLEv99EZR+HtomIiIhIFCaSRERERCQKE0kiIiIiEoWJJH3SpFIp5s6dy4n4ROUQ/34TlX182IaIiIiIRGFFkoiIiIhEYSJJRERERKIwkSQiIiIiUZhIEhXh4sWLkEgkSExMVHUoREREZRYTSVK64cOHQyKRYMmSJXLtR44cgUQiUVFURFTaAgMDoampCTc3N7n2efPmoVGjRqoJioiUiokkfRQ6Ojr46aef8Pr161I7Z3Z2dqmdi4j+u23btuHbb7/F5cuX8eLFC1WHQ0QfARNJ+ihcXFxgZWUFHx+f9/Y5ePAg6tWrB6lUiqpVq2L58uVy+6tWrYqFCxdi2LBhMDIywpgxY+Dr6wsTExMcP34ctWvXhp6eHvr27Yv09HT4+fmhatWqMDU1xcSJE5GXlyc7186dO9G0aVMYGhrCysoKgwcPRlxcnNLun6i8S01Nxd69ezFu3Di4ubnB19cXAODr64v58+fj7t27kEgkkEgksn0rVqxAgwYNoK+vD1tbW4wfPx6pqaly5/X19UWVKlWgp6eH3r17Y/ny5TAxMZHtHz58OHr16iV3zKRJk9CuXTvZ6/z8fPj4+KBatWrQ1dVFw4YNceDAASW8C0Tqh4kkfRSamppYvHgx1q5di2fPnhXaHxQUhP79+2PgwIEICQnBvHnzMHv2bNl/cAosW7YMDRs2xJ07dzB79mwAQHp6OtasWYM9e/bg1KlTuHjxInr37o0//vgDf/zxB3bu3IlNmzbJ/YcjJycHCxcuxN27d3HkyBE8ffoUw4cPV+ZbQFSu7du3D3Xq1EHt2rUxdOhQ/PrrrxAEAQMGDMCUKVNQr149REdHIzo6GgMGDAAAaGhoYM2aNQgLC4Ofnx/Onz+P6dOny855/fp1eHh4wNPTE8HBwWjfvj0WLVqkcGw+Pj7YsWMHfvnlF4SFhcHLywtDhw7FpUuXSu3+idSWQKRk7u7uQs+ePQVBEITmzZsLI0eOFARBEA4fPiwUfAQHDx4sdOrUSe64adOmCQ4ODrLXdnZ2Qq9eveT6bN++XQAgPH78WNb2zTffCHp6ekJKSoqszdXVVfjmm2/eG+PNmzcFALJjLly4IAAQXr9+rfgNE6mhFi1aCKtWrRIEQRBycnKESpUqCRcuXBAEQRDmzp0rNGzYsNhz7N+/X6hYsaLs9aBBg4Ru3brJ9RkwYIBgbGwse/32vy8FvvvuO6Ft27aCIAhCZmamoKenJ1y7dk2uj4eHhzBo0KCS3RwRvRcrkvRR/fTTT/Dz80N4eLhce3h4OFq2bCnX1rJlSzx69EhuSLpp06aFzqmnp4caNWrIXltaWqJq1aowMDCQa3t76DooKAjdu3dHlSpVYGhoiLZt2wIAoqKi/tsNEqmhhw8f4saNGxg0aBAAoEKFChgwYAC2bdv2weMCAgLQsWNHfPbZZzA0NMTXX3+NV69eIT09HcCbfxeaNWsmd4yzs7NCsT1+/Bjp6eno1KkTDAwMZNuOHTsQERGh0LmIqLAKqg6A1EubNm3g6uoKb29vUUPJ+vr6hdq0tLTkXkskkiLb8vPzAQBpaWlwdXWFq6sr/P39YW5ujqioKLi6uvIBHiIRtm3bhtzcXNjY2MjaBEGAVCrFunXrijzm6dOn+PLLLzFu3Dj8+OOPMDMzw5UrV+Dh4YHs7Gzo6emV6NoaGhoQ3vmm35ycHNmfC+ZcnjhxAp999plcP36HN9F/x0SSProlS5agUaNGqF27tqytbt26uHr1qly/q1evolatWtDU1CzV6z948ACvXr3CkiVLYGtrCwC4detWqV6DSF3k5uZix44dWL58OTp37iy3r1evXti9eze0tbXlRhaAN6MC+fn5WL58OTQ03gyO7du3T65P3bp1cf36dbm2v/76S+61ubk5QkND5dqCg4Nlv0w6ODhAKpUiKipKNvJARKWHiSR9dA0aNMCQIUOwZs0aWduUKVPw+eefY+HChRgwYAACAwOxbt06bNiwodSvX6VKFWhra2Pt2rUYO3YsQkNDsXDhwlK/DpE6OH78OF6/fg0PDw8YGxvL7evTpw+2bdsGLy8vREZGIjg4GJUrV4ahoSHs7e2Rk5ODtWvXonv37rh69Sp++eUXueMnTpyIli1bYtmyZejZsydOnz6NU6dOyfXp0KEDfv75Z+zYsQPOzs747bffEBoaisaNGwMADA0NMXXqVHh5eSE/Px+tWrVCUlISrl69CiMjI7i7uyv3DSIq5zhHklRiwYIFsqFmAGjSpAn27duHPXv2oH79+pgzZw4WLFiglCepzc3N4evri/3798PBwQFLlizBsmXLSv06ROpg27ZtcHFxKZREAm8SyVu3bqFevXro0qUL2rdvD3Nzc+zevRsNGzbEihUr8NNPP6F+/frw9/cvtDxY8+bNsWXLFqxevRoNGzbEmTNnMGvWLLk+rq6umD17NqZPn47PP/8cKSkpGDZsmFyfhQsXYvbs2fDx8UHdunXRpUsXnDhxAtWqVSv9N4RIzUiEdyeXEBERlVG+vr6YNGkSv76UqIxgRZKIiIiIRGEiSURERESicGibiIiIiERhRZKIiIiIRGEiSURERESiMJEkIiIiIlGYSBIRERGRKEwkiaiQzMxM/Pjjj3j8+LGqQyEiojKMiSQRFTJx4kQ8fvwY9vb2pXI+iUSCI0eOlMq5PraqVati1apVqg6DiKhMYiJJpAaGDx8OiUQCiUQCLS0tVKtWDdOnT0dmZmahvv7+/nj69Ck2b94s137x4kVIJBKVfKPI06dPZfFLJBJUrFgRnTt3xp07d5R+7Zs3b2LMmDEl6sukk4jUDRNJIjXRpUsXREdH48mTJ1i5ciU2bdqEuXPnFuo3ZMgQnDlzBlpaWiqI8sMCAgIQHR2N06dPIzU1FV27dn1vYpuTk1Mq1zQ3N4eenl6pnIuIqLxhIkmkJqRSKaysrGBra4tevXrBxcUFZ8+ele3PysrCxIkTYWFhAR0dHbRq1Qo3b94E8KYi2L59ewCAqakpJBIJhg8fDqDoKlyjRo0wb96898YSEhKCDh06QFdXFxUrVsSYMWOQmppa7D1UrFgRVlZWaNq0KZYtW4bY2Fhcv35dVrHcu3cv2rZtCx0dHfj7+wMAtm7dirp160JHRwd16tTBhg0bZOdr0aIFZsyYIXeN+Ph4aGlp4fLly4XuTxAEzJs3D1WqVIFUKoWNjQ0mTpwIAGjXrh3++ecfeHl5ySqnBQ4ePIh69epBKpWiatWqWL58ebH3SkT0KWAiSaSGQkNDce3aNWhra8vapk+fjoMHD8LPzw+3b9+Gvb09XF1dkZCQAFtbWxw8eBAA8PDhQ0RHR2P16tWirp2WlgZXV1eYmpri5s2b2L9/PwICAuDp6anQeXR1dQEA2dnZsraZM2fiu+++Q3h4OFxdXeHv7485c+bgxx9/RHh4OBYvXozZs2fDz88PwJvq6549e/D2F3zt3bsXNjY2aN26daFrHjx4UFbNffToEY4cOYIGDRoAAA4dOoTKlStjwYIFiI6ORnR0NAAgKCgI/fv3x8CBAxESEoJ58+Zh9uzZ8PX1Veh+iYjKJIGIyj13d3dBU1NT0NfXF6RSqQBA0NDQEA4cOCAIgiCkpqYKWlpagr+/v+yY7OxswcbGRli6dKkgCIJw4cIFAYDw+vVruXPb2dkJK1eulGtr2LChMHfuXNlrAMLhw4cFQRCEzZs3C6ampkJqaqps/4kTJwQNDQ0hJiamyPgjIyMFAMKdO3cEQRCE169fC7179xYMDAyEmJgY2f5Vq1bJHVejRg1h165dcm0LFy4UnJ2dBUEQhLi4OKFChQrC5cuXZfudnZ2FGTNmFHl/y5cvF2rVqiVkZ2cXGWdR78XgwYOFTp06ybVNmzZNcHBwKPIcRESfElYkidRE+/btERwcjOvXr8Pd3R0jRoxAnz59AAARERHIyclBy5YtZf21tLTwxRdfIDw8vFTjCA8PR8OGDaGvry9ra9myJfLz8/Hw4cMPHtuiRQsYGBjA1NQUd+/exd69e2FpaSnb37RpU9mf09LSEBERAQ8PDxgYGMi2RYsWISIiAsCb+Y+dO3eWDYNHRkYiMDAQQ4YMKfL6/fr1Q0ZGBqpXr47Ro0fj8OHDyM3NLfZ+335fC+730aNHyMvL++CxRERlHRNJIjWhr68Pe3t7NGzYEL/++iuuX7+Obdu2/efzamhoyA0NA6X3oMu79u7di7t37+L169eIiIhAt27d5Pa/nZwWzLncsmULgoODZVtoaCj++usvWb8hQ4bgwIEDyMnJwa5du9CgQQPZcPW7bG1t8fDhQ2zYsAG6uroYP3482rRpo7T7JSIq65hIEqkhDQ0NfP/995g1axYyMjJQo0YNaGtr4+rVq7I+OTk5uHnzJhwcHABANp/y3Sqaubm5bD4gACQnJyMyMvK9165bty7u3r2LtLQ0WdvVq1ehoaGB2rVrfzBuW1tb1KhRAyYmJsXeo6WlJWxsbPDkyRPY29vLbdWqVZP169mzJzIzM3Hq1Cns2rXrvdXIArq6uujevTvWrFmDixcvIjAwECEhIQDevEfvvj9169aVe18L7rdWrVrQ1NQs9j6IiMoyJpJEaqpfv37Q1NTE+vXroa+vj3HjxmHatGk4deoU7t+/j9GjRyM9PR0eHh4AADs7O0gkEhw/fhzx8fGyil+HDh2wc+dO/PnnnwgJCYG7u/sHE6QhQ4ZAR0cH7u7uCA0NxYULF/Dtt9/i66+/lhumLg3z58+Hj48P1qxZg7///hshISHYvn07VqxYIeujr6+PXr16Yfbs2QgPD8egQYPeez5fX19s27YNoaGhePLkCX777Tfo6urCzs4OwJsnvC9fvoznz5/j5cuXAIApU6bg3LlzWLhwIf7++2/4+flh3bp1mDp1aqneKxGRSqh6kiYRKZ+7u7vQs2fPQu0+Pj6Cubm5kJqaKmRkZAjffvutUKlSJUEqlQotW7YUbty4Idd/wYIFgpWVlSCRSAR3d3dBEAQhKSlJGDBggGBkZCTY2toKvr6+H3zYRhAE4d69e0L79u0FHR0dwczMTBg9erSQkpLy3vjffdhGkf3+/v5Co0aNBG1tbcHU1FRo06aNcOjQIbk+f/zxhwBAaNOmTaHj336A5vDhw0KzZs0EIyMjQV9fX2jevLkQEBAg6xsYGCg4OjrKHmgqcODAAcHBwUHQ0tISqlSpIvz888/vvVciok+JRBDemdxERERERFQCHNomIiIiIlGYSBIRERGRKEwkiYiIiEgUJpJEREREJAoTSSIiIiIShYkkEREREYnCRJKIiIiIRGEiSURERESiMJEkIiIiIlGYSBIRERGRKEwkiYiIiEiU/wNE1VIxghEtaQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 2320\n",
      "True Negatives (TN): 3584\n",
      "False Positives (FP): 416\n",
      "False Negatives (FN): 1680\n",
      "\n",
      "F1 Score: 0.6888\n",
      "True Positive Rate (TPR) / Recall: 0.5800\n",
      "True Negative Rate (TNR) / Specificity: 0.8960\n",
      "--------------------\n",
      "  Accuracy: 73.8000%\n",
      "  Avg. Inference Time: 0.0030 ms\n",
      "  Early Exit Rate: 5.5250% (442/8000)\n",
      "--------------------\n",
      "\n",
      "Base: CIC\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 12240\n",
      "True Negatives (TN): 9778\n",
      "False Positives (FP): 4222\n",
      "False Negatives (FN): 1760\n",
      "\n",
      "F1 Score: 0.8036\n",
      "True Positive Rate (TPR) / Recall: 0.8743\n",
      "True Negative Rate (TNR) / Specificity: 0.6984\n",
      "--------------------\n",
      "  Accuracy: 78.6357%\n",
      "  Avg. Inference Time: 0.0030 ms\n",
      "  Early Exit Rate: 0.6643% (186/28000)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/{modelname}.pth'))\n",
    "print(f\"Modelo 'models/{modelname}.pth' carregado\\n\")\n",
    "\n",
    "print(f\"Base: UNSW\")\n",
    "results = evaluate_model(model, test_loaders[0], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: BOT\")\n",
    "results = evaluate_model(model, test_loaders[1], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: CIC\")\n",
    "results = evaluate_model(model, test_loaders[2], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b99544-909f-49c5-a212-93324562d197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361685d-5c70-46b6-945c-4532ea630054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
