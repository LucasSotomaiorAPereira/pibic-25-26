{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff7fbd-35cd-4bf4-9064-7542fbe66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_creator import CreatorDL\n",
    "creator = CreatorDL(seed=42, bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108772d1-330c-4ea7-b4fa-a96a9c5c06ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 1118865 | Teste: 559433 | Validação: 559433\n",
      "Processando a categoria: 'Fuzzers'\n",
      "  -> Treino: 16908 | Teste: 8454 | Validação: 8454\n",
      "Processando a categoria: 'Exploits'\n",
      "  -> Treino: 21374 | Teste: 10687 | Validação: 10687\n",
      "Processando a categoria: 'Backdoor'\n",
      "  -> Treino: 2329 | Teste: 1165 | Validação: 1165\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 8537 | Teste: 4268 | Validação: 4269\n",
      "Processando a categoria: 'Generic'\n",
      "  -> Treino: 9825 | Teste: 4913 | Validação: 4913\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 2990 | Teste: 1495 | Validação: 1495\n",
      "Processando a categoria: 'Shellcode'\n",
      "  -> Treino: 1190 | Teste: 595 | Validação: 596\n",
      "Processando a categoria: 'Analysis'\n",
      "  -> Treino: 613 | Teste: 306 | Validação: 307\n",
      "Processando a categoria: 'Worms'\n",
      "  -> Treino: 79 | Teste: 39 | Validação: 40\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 1182710 linhas\n",
      "Categorias presentes: ['Benign' 'Exploits' 'Reconnaissance' 'Fuzzers' 'DoS' 'Generic' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            1118865\n",
      "Exploits            21374\n",
      "Fuzzers             16908\n",
      "Generic              9825\n",
      "Reconnaissance       8537\n",
      "DoS                  2990\n",
      "Backdoor             2329\n",
      "Shellcode            1190\n",
      "Analysis              613\n",
      "Worms                  79\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 591355 linhas\n",
      "Categorias presentes: ['Benign' 'Generic' 'DoS' 'Reconnaissance' 'Exploits' 'Fuzzers' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4268\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            595\n",
      "Analysis             306\n",
      "Worms                 39\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 591359 linhas\n",
      "Categorias presentes: ['Benign' 'Fuzzers' 'Reconnaissance' 'Exploits' 'Generic' 'Analysis'\n",
      " 'Shellcode' 'Backdoor' 'DoS' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4269\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            596\n",
      "Analysis             307\n",
      "Worms                 40\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0516)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-1.4981e-07) tensor(4.5768) tensor(0.0510)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-2.9962e-07) tensor(3.5191) tensor(0.0518)\n"
     ]
    }
   ],
   "source": [
    "df_UNSW = creator.reader(\"NF-UNSW-NB15-v3\")\n",
    "\n",
    "df_train_UNSW, df_test_UNSW, df_val_UNSW = creator.splitter(df_UNSW)\n",
    "\n",
    "train_loader_UNSW, test_loader_UNSW, val_loader_UNSW = creator.balancer(df_train_UNSW, df_test_UNSW, df_val_UNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8fca6f-a878-4ab7-bbef-2d77092947de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 25994 | Teste: 12997 | Validação: 12998\n",
      "Processando a categoria: 'DDoS'\n",
      "  -> Treino: 3575441 | Teste: 1787720 | Validação: 1787721\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 4017095 | Teste: 2008547 | Validação: 2008548\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 847566 | Teste: 423783 | Validação: 423783\n",
      "Processando a categoria: 'Theft'\n",
      "  -> Treino: 807 | Teste: 404 | Validação: 404\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 8466903 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               4017095\n",
      "DDoS              3575441\n",
      "Reconnaissance     847566\n",
      "Benign              25994\n",
      "Theft                 807\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 4233451 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008547\n",
      "DDoS              1787720\n",
      "Reconnaissance     423783\n",
      "Benign              12997\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 4233454 linhas\n",
      "Categorias presentes: ['DoS' 'DDoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008548\n",
      "DDoS              1787721\n",
      "Reconnaissance     423783\n",
      "Benign              12998\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0232)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.1910e-07) tensor(1.4751) tensor(0.0235)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.7865e-07) tensor(5.3125) tensor(0.0232)\n"
     ]
    }
   ],
   "source": [
    "df_BOT= creator.reader(\"NF-BoT-IoT-v3\")\n",
    "\n",
    "df_train_BOT, df_test_BOT, df_val_BOT = creator.splitter(df_BOT)\n",
    "\n",
    "train_loader_BOT, test_loader_BOT, val_loader_BOT = creator.balancer(df_train_BOT, df_test_BOT, df_val_BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bc484e-aa09-40d0-967f-5ffe2b3c7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 8757313 | Teste: 4378656 | Validação: 4378657\n",
      "Processando a categoria: 'FTP-BruteForce'\n",
      "  -> Treino: 193360 | Teste: 96680 | Validação: 96680\n",
      "Processando a categoria: 'SSH-Bruteforce'\n",
      "  -> Treino: 94237 | Teste: 47118 | Validação: 47119\n",
      "Processando a categoria: 'DoS_attacks-GoldenEye'\n",
      "  -> Treino: 30650 | Teste: 15325 | Validação: 15325\n",
      "Processando a categoria: 'DoS_attacks-Slowloris'\n",
      "  -> Treino: 18020 | Teste: 9010 | Validação: 9010\n",
      "Processando a categoria: 'DoS_attacks-SlowHTTPTest'\n",
      "  -> Treino: 52775 | Teste: 26387 | Validação: 26388\n",
      "Processando a categoria: 'DoS_attacks-Hulk'\n",
      "  -> Treino: 50038 | Teste: 25019 | Validação: 25019\n",
      "Processando a categoria: 'DDoS_attacks-LOIC-HTTP'\n",
      "  -> Treino: 144294 | Teste: 72147 | Validação: 72148\n",
      "Processando a categoria: 'DDOS_attack-LOIC-UDP'\n",
      "  -> Treino: 1725 | Teste: 862 | Validação: 863\n",
      "Processando a categoria: 'DDOS_attack-HOIC'\n",
      "  -> Treino: 516155 | Teste: 258078 | Validação: 258078\n",
      "Processando a categoria: 'Brute_Force_-Web'\n",
      "  -> Treino: 809 | Teste: 404 | Validação: 405\n",
      "Processando a categoria: 'Brute_Force_-XSS'\n",
      "  -> Treino: 240 | Teste: 120 | Validação: 120\n",
      "Processando a categoria: 'SQL_Injection'\n",
      "  -> Treino: 220 | Teste: 110 | Validação: 110\n",
      "Processando a categoria: 'Infilteration'\n",
      "  -> Treino: 94076 | Teste: 47038 | Validação: 47038\n",
      "Processando a categoria: 'Bot'\n",
      "  -> Treino: 103851 | Teste: 51926 | Validação: 51926\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 10057763 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'FTP-BruteForce' 'DoS_attacks-Hulk' 'Bot' 'DoS_attacks-GoldenEye'\n",
      " 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Slowloris'\n",
      " 'Brute_Force_-Web' 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      8757313\n",
      "DDOS_attack-HOIC             516155\n",
      "FTP-BruteForce               193360\n",
      "DDoS_attacks-LOIC-HTTP       144294\n",
      "Bot                          103851\n",
      "SSH-Bruteforce                94237\n",
      "Infilteration                 94076\n",
      "DoS_attacks-SlowHTTPTest      52775\n",
      "DoS_attacks-Hulk              50038\n",
      "DoS_attacks-GoldenEye         30650\n",
      "DoS_attacks-Slowloris         18020\n",
      "DDOS_attack-LOIC-UDP           1725\n",
      "Brute_Force_-Web                809\n",
      "Brute_Force_-XSS                240\n",
      "SQL_Injection                   220\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 5028880 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDOS_attack-HOIC' 'FTP-BruteForce'\n",
      " 'SSH-Bruteforce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-LOIC-UDP' 'Bot'\n",
      " 'DoS_attacks-GoldenEye' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'DoS_attacks-Slowloris' 'Brute_Force_-Web' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378656\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72147\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47118\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26387\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            862\n",
      "Brute_Force_-Web                404\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 5028886 linhas\n",
      "Categorias presentes: ['Benign' 'FTP-BruteForce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'Bot' 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'Infilteration' 'DoS_attacks-GoldenEye' 'DoS_attacks-Slowloris'\n",
      " 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS' 'Brute_Force_-Web'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378657\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72148\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47119\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26388\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            863\n",
      "Brute_Force_-Web                405\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0473)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.4776) tensor(0.0477)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(2.7903) tensor(0.0478)\n"
     ]
    }
   ],
   "source": [
    "df_CIC= creator.reader(\"NF-CICIDS2018-v3\")\n",
    "\n",
    "df_train_CIC, df_test_CIC, df_val_CIC = creator.splitter(df_CIC)\n",
    "\n",
    "train_loader_CIC, test_loader_CIC, val_loader_CIC = creator.balancer(df_train_CIC, df_test_CIC, df_val_CIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9457e890-0e00-4842-a0eb-2b9a4bc57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "test_loaders = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "val_loaders = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, num_classes=2):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_dim * 2, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.exit1_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.exit2_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward_exit1(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit1_layers(features)\n",
    "\n",
    "    def forward_exit2(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit2_layers(features)\n",
    "\n",
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "943446ef-8b5f-4e49-82a7-db448f6dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders, val_loaders, epochs, lr, device, normal_threshold, attack_threshold, patience=15):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.001, patience=7)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    metrics = [\n",
    "        'loss1_a', 'loss1_b', 'loss1_c', 'loss_ex1_avg',\n",
    "        'loss2_a', 'loss2_b', 'loss2_c', 'loss_ex2_avg',\n",
    "        'l_joint', 'total_loss', 'comp_cost'\n",
    "    ]\n",
    "\n",
    "    history = {\n",
    "        'train': {k: [] for k in metrics},\n",
    "        'val': {k: [] for k in metrics}\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    max_train_batches = max(len(l) for l in train_loaders) \n",
    "    train_iter_loaders = [itertools.cycle(l) if len(l) < max_train_batches else l for l in train_loaders]\n",
    "    \n",
    "    max_val_batches = max(len(l) for l in val_loaders)\n",
    "    val_iter_loaders = [itertools.cycle(l) if len(l) < max_val_batches else l for l in val_loaders]\n",
    "\n",
    "    def process_batch_logic(inputs, labels, model_ref, class_thresholds, crit, temperature=10):\n",
    "        out1 = model_ref.forward_exit1(inputs)\n",
    "        \n",
    "        probs = F.softmax(out1, dim=1)\n",
    "        conf, preds = torch.max(probs, dim=1)\n",
    "        \n",
    "        current_thresholds = class_thresholds[preds]\n",
    "            \n",
    "        mask_ex1 = conf > current_thresholds\n",
    "        mask_ex2 = ~mask_ex1\n",
    "        \n",
    "        diff = current_thresholds - conf\n",
    "        prob_exit2 = torch.sigmoid(temperature * diff)\n",
    "        \n",
    "        count_ex2 = prob_exit2.sum()\n",
    "        \n",
    "        if mask_ex1.sum() > 0:\n",
    "            loss1 = crit(out1[mask_ex1], labels[mask_ex1])\n",
    "        else:\n",
    "            loss1 = torch.tensor(0.0, device=inputs.device, requires_grad=True)\n",
    "        \n",
    "        loss2 = torch.tensor(0.0, device=inputs.device, requires_grad=True)\n",
    "        \n",
    "        if mask_ex2.sum() > 0:\n",
    "            inputs_ex2 = inputs[mask_ex2]\n",
    "            labels_ex2 = labels[mask_ex2]\n",
    "            \n",
    "            out2 = model_ref.forward_exit2(inputs_ex2)\n",
    "            loss2 = crit(out2, labels_ex2)\n",
    "            \n",
    "        return loss1, loss2, count_ex2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_metrics = {k: 0.0 for k in metrics}\n",
    "        total_steps = 0\n",
    "\n",
    "        loader_iterators = [iter(l) for l in train_iter_loaders]\n",
    "                \n",
    "        for _ in range(max_train_batches):\n",
    "            try:\n",
    "                batches = [next(it) for it in loader_iterators]\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            thresholds = torch.tensor([normal_threshold, attack_threshold], device=device)\n",
    "            \n",
    "            (inputs_a, labels_a) = batches[0]\n",
    "            (inputs_b, labels_b) = batches[1]\n",
    "            (inputs_c, labels_c) = batches[2]\n",
    "            \n",
    "            inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "            inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "            inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "            total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "            \n",
    "            loss1_a, loss2_a, count_a_ex2 = process_batch_logic(inputs_a, labels_a, model, thresholds, criterion)\n",
    "            loss1_b, loss2_b, count_b_ex2 = process_batch_logic(inputs_b, labels_b, model, thresholds, criterion)\n",
    "            loss1_c, loss2_c, count_c_ex2 = process_batch_logic(inputs_c, labels_c, model, thresholds, criterion)\n",
    "\n",
    "            count_exit2 = count_a_ex2 + count_b_ex2 + count_c_ex2\n",
    "                \n",
    "            loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "            loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "            \n",
    "            l_joint = loss_ex1_avg * 1 + loss_ex2_avg * 0\n",
    "\n",
    "            current_cost_ratio = count_exit2 / total_samples\n",
    "\n",
    "            total_loss = l_joint + current_cost_ratio * 0\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_metrics['loss1_a'] += loss1_a.item()\n",
    "            running_metrics['loss1_b'] += loss1_b.item()\n",
    "            running_metrics['loss1_c'] += loss1_c.item()\n",
    "            running_metrics['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "            \n",
    "            running_metrics['loss2_a'] += loss2_a.item()\n",
    "            running_metrics['loss2_b'] += loss2_b.item()\n",
    "            running_metrics['loss2_c'] += loss2_c.item()\n",
    "            running_metrics['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "            \n",
    "            running_metrics['l_joint'] += l_joint.item()\n",
    "            running_metrics['total_loss'] += total_loss.item()\n",
    "            \n",
    "            running_metrics['comp_cost'] += current_cost_ratio.item()\n",
    "            \n",
    "            total_steps += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['train'][key].append(running_metrics[key] / total_steps)\n",
    "        \n",
    "        epoch_train_loss = history['train']['total_loss'][-1]\n",
    "        epoch_train_jloss = history['train']['l_joint'][-1]\n",
    "        epoch_train_loss1 = history['train']['loss_ex1_avg'][-1]\n",
    "        epoch_train_loss2 = history['train']['loss_ex2_avg'][-1]\n",
    "\n",
    "        model.eval()\n",
    "        running_metrics_val = {k: 0.0 for k in metrics}\n",
    "        total_steps_val = 0\n",
    "        \n",
    "        val_loader_iterators = [iter(l) for l in val_iter_loaders]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_val_batches):\n",
    "                try:\n",
    "                    batches = [next(it) for it in val_loader_iterators]\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                thresholds = torch.tensor([normal_threshold, attack_threshold], device=device)\n",
    "\n",
    "                (inputs_a, labels_a) = batches[0]\n",
    "                (inputs_b, labels_b) = batches[1]\n",
    "                (inputs_c, labels_c) = batches[2]\n",
    "   \n",
    "                inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "                inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "                inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "                total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    " \n",
    "                loss1_a, loss2_a, count_a_ex2 = process_batch_logic(inputs_a, labels_a, model, thresholds, criterion)\n",
    "                loss1_b, loss2_b, count_b_ex2 = process_batch_logic(inputs_b, labels_b, model, thresholds, criterion)\n",
    "                loss1_c, loss2_c, count_c_ex2 = process_batch_logic(inputs_c, labels_c, model, thresholds, criterion)\n",
    "    \n",
    "                count_exit2 = count_a_ex2 + count_b_ex2 + count_c_ex2\n",
    "                    \n",
    "                loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "                loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "                \n",
    "                l_joint = loss_ex1_avg * 1 + loss_ex2_avg * 0\n",
    "\n",
    "                current_cost_ratio = count_exit2 / total_samples\n",
    "\n",
    "                total_loss = l_joint + current_cost_ratio * 0\n",
    "                                                                                            \n",
    "                running_metrics_val['loss1_a'] += loss1_a.item()\n",
    "                running_metrics_val['loss1_b'] += loss1_b.item()\n",
    "                running_metrics_val['loss1_c'] += loss1_c.item()\n",
    "                running_metrics_val['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "                \n",
    "                running_metrics_val['loss2_a'] += loss2_a.item()\n",
    "                running_metrics_val['loss2_b'] += loss2_b.item()\n",
    "                running_metrics_val['loss2_c'] += loss2_c.item()\n",
    "                running_metrics_val['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "                \n",
    "                running_metrics_val['l_joint'] += l_joint.item()\n",
    "                running_metrics_val['total_loss'] += total_loss.item()\n",
    "                \n",
    "                running_metrics_val['comp_cost'] += current_cost_ratio.item()\n",
    "\n",
    "                \n",
    "                total_steps_val += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['val'][key].append(running_metrics_val[key] / total_steps_val)\n",
    "\n",
    "        epoch_val_loss = history['val']['total_loss'][-1]\n",
    "        epoch_val_jloss = history['val']['l_joint'][-1]\n",
    "        epoch_val_loss1 = history['val']['loss_ex1_avg'][-1]\n",
    "        epoch_val_loss2 = history['val']['loss_ex2_avg'][-1]\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Train Total: {epoch_train_loss:.4f} | Val Total: {epoch_val_loss:.4f}\\nTrain Joint: {epoch_train_jloss:.4f} | Train Loss Exit 1: {epoch_train_loss1:.4f} | Train Loss Exit 2: {epoch_train_loss2:.4f}\\nVal Joint: {epoch_val_jloss:.4f} | Val Loss Exit 1: {epoch_val_loss1:.4f} | Val Loss Exit 2: {epoch_val_loss2:.4f}\\nAttack Threshold: {attack_threshold:.4f} | Normal Threshold: {normal_threshold:.4f}\\n\\n')        \n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                if best_model_state: model.load_state_dict(best_model_state)\n",
    "                break\n",
    "                \n",
    "        scheduler.step(epoch_val_loss)\n",
    "                \n",
    "    epochs_range = range(1, len(history['train']['l_joint']) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(26, 6))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Exit 1\")\n",
    "    ax.plot(epochs_range, history['train']['loss1_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex1_avg'], label='Tr Avg', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss1_a'], label='Val A', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_b'], label='Val B', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_c'], label='Val C', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex1_avg'], label='Val Avg', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Exit 2\")\n",
    "    ax.plot(epochs_range, history['train']['loss2_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex2_avg'], label='Tr Avg', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss2_a'], label='Val A', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_b'], label='Val B', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_c'], label='Val C', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex2_avg'], label='Val Avg', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Global Optimization\")\n",
    "    ax.plot(epochs_range, history['train']['l_joint'], label='Tr Joint', color='purple')\n",
    "    ax.plot(epochs_range, history['train']['total_loss'], label='Tr Total', color='orange', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['l_joint'], label='Val Joint', color='purple', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['total_loss'], label='Val Total', color='orange', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[3]\n",
    "    ax.set_title(\"Computational Cost (N_Ex2 / N)\")\n",
    "    ax.plot(epochs_range, history['train']['comp_cost'], label='Train Cost', color='brown', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['comp_cost'], label='Val Cost', color='brown', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return attack_threshold, normal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd9acd9be6862e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(loader.dataset)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    exited_early_count = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    attack_threshold = model.get_threshold_attack()\n",
    "    normal_threshold = model.get_threshold_normal()\n",
    "    \n",
    "    thresholds = torch.cat((attack_threshold, normal_threshold), dim=0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            branch_output = model.forward_exit1(samples)\n",
    "            branch_prob = F.softmax(branch_output, dim=1)\n",
    "            trusts, branch_preds = torch.max(branch_prob, 1)\n",
    "\n",
    "            batch_predictions = torch.zeros_like(labels)\n",
    "            \n",
    "            current_thresholds = thresholds[branch_preds]\n",
    "            \n",
    "            early_exit_mask = trusts > current_thresholds\n",
    "            \n",
    "            if early_exit_mask.any():\n",
    "                batch_predictions[early_exit_mask] = branch_preds[early_exit_mask]\n",
    "                exited_early_count += early_exit_mask.sum().item()\n",
    "\n",
    "            main_branch_mask = ~early_exit_mask\n",
    "            if main_branch_mask.any():\n",
    "                samples_to_main = samples[main_branch_mask]\n",
    "                \n",
    "                main_output = model.forward_exit2(samples_to_main)\n",
    "                main_prob = F.softmax(main_output, dim=1)\n",
    "                _, main_preds = torch.max(main_prob, 1)\n",
    "                \n",
    "                batch_predictions[main_branch_mask] = main_preds\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions)\n",
    "    y_data = torch.cat(all_labels)\n",
    "\n",
    "    correct = (final_predictions == y_data).sum().item()\n",
    "    accuracy = 100 * correct / total_samples\n",
    "    exit_rate = 100 * exited_early_count / total_samples\n",
    "    avg_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    print(f\"Limiares Aprendidos -> Attack: {attack_threshold.item():.4f} | Normal: {normal_threshold.item():.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Ataque'],\n",
    "                yticklabels=['Normal', 'Ataque'])\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão (Limiares Dinâmicos)')\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    f1 = f1_score(y_data.numpy(), final_predictions.numpy())\n",
    "    tpr = recall_score(y_data.numpy(), final_predictions.numpy())\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\\n\")\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positive Rate (TPR) / Recall: {tpr:.4f}\")\n",
    "    print(f\"True Negative Rate (TNR) / Specificity: {tnr:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}%\")\n",
    "    print(f\"Early Exit Rate: {exit_rate:.4f}%\")\n",
    "\n",
    "    return {\n",
    "            'accuracy': accuracy,\n",
    "            'exit_rate': exit_rate,\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'exited_early_count': exited_early_count,\n",
    "            'total_samples': total_samples,\n",
    "            'f1': f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bfbf4b5-dcc6-4bd5-96e5-b96988ebd70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arq_v2_saida_1'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'arq_v2_saida_1'\n",
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Train Total: 0.6924 | Val Total: 0.6919\n",
      "Train Joint: 0.6924 | Train Loss Exit 1: 0.6924 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6919 | Val Loss Exit 1: 0.6919 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [2/500] | Train Total: 0.6916 | Val Total: 0.6912\n",
      "Train Joint: 0.6916 | Train Loss Exit 1: 0.6916 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6912 | Val Loss Exit 1: 0.6912 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [3/500] | Train Total: 0.6910 | Val Total: 0.6903\n",
      "Train Joint: 0.6910 | Train Loss Exit 1: 0.6910 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6903 | Val Loss Exit 1: 0.6903 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [4/500] | Train Total: 0.6902 | Val Total: 0.6894\n",
      "Train Joint: 0.6902 | Train Loss Exit 1: 0.6902 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6894 | Val Loss Exit 1: 0.6894 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [5/500] | Train Total: 0.6891 | Val Total: 0.6883\n",
      "Train Joint: 0.6891 | Train Loss Exit 1: 0.6891 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6883 | Val Loss Exit 1: 0.6883 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [6/500] | Train Total: 0.6882 | Val Total: 0.6872\n",
      "Train Joint: 0.6882 | Train Loss Exit 1: 0.6882 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6872 | Val Loss Exit 1: 0.6872 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [7/500] | Train Total: 0.6870 | Val Total: 0.6859\n",
      "Train Joint: 0.6870 | Train Loss Exit 1: 0.6870 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6859 | Val Loss Exit 1: 0.6859 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [8/500] | Train Total: 0.6857 | Val Total: 0.6843\n",
      "Train Joint: 0.6857 | Train Loss Exit 1: 0.6857 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6843 | Val Loss Exit 1: 0.6843 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [9/500] | Train Total: 0.6839 | Val Total: 0.6824\n",
      "Train Joint: 0.6839 | Train Loss Exit 1: 0.6839 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6824 | Val Loss Exit 1: 0.6824 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [10/500] | Train Total: 0.6821 | Val Total: 0.6803\n",
      "Train Joint: 0.6821 | Train Loss Exit 1: 0.6821 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6803 | Val Loss Exit 1: 0.6803 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [11/500] | Train Total: 0.6798 | Val Total: 0.6778\n",
      "Train Joint: 0.6798 | Train Loss Exit 1: 0.6798 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6778 | Val Loss Exit 1: 0.6778 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [12/500] | Train Total: 0.6773 | Val Total: 0.6752\n",
      "Train Joint: 0.6773 | Train Loss Exit 1: 0.6773 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6752 | Val Loss Exit 1: 0.6752 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [13/500] | Train Total: 0.6745 | Val Total: 0.6718\n",
      "Train Joint: 0.6745 | Train Loss Exit 1: 0.6745 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6718 | Val Loss Exit 1: 0.6718 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [14/500] | Train Total: 0.6711 | Val Total: 0.6685\n",
      "Train Joint: 0.6711 | Train Loss Exit 1: 0.6711 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6685 | Val Loss Exit 1: 0.6685 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [15/500] | Train Total: 0.6675 | Val Total: 0.6644\n",
      "Train Joint: 0.6675 | Train Loss Exit 1: 0.6675 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6644 | Val Loss Exit 1: 0.6644 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [16/500] | Train Total: 0.6640 | Val Total: 0.6605\n",
      "Train Joint: 0.6640 | Train Loss Exit 1: 0.6640 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6605 | Val Loss Exit 1: 0.6605 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [17/500] | Train Total: 0.6600 | Val Total: 0.6560\n",
      "Train Joint: 0.6600 | Train Loss Exit 1: 0.6600 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6560 | Val Loss Exit 1: 0.6560 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [18/500] | Train Total: 0.6556 | Val Total: 0.6517\n",
      "Train Joint: 0.6556 | Train Loss Exit 1: 0.6556 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6517 | Val Loss Exit 1: 0.6517 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [19/500] | Train Total: 0.6516 | Val Total: 0.6470\n",
      "Train Joint: 0.6516 | Train Loss Exit 1: 0.6516 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6470 | Val Loss Exit 1: 0.6470 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [20/500] | Train Total: 0.6469 | Val Total: 0.6423\n",
      "Train Joint: 0.6469 | Train Loss Exit 1: 0.6469 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6423 | Val Loss Exit 1: 0.6423 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [21/500] | Train Total: 0.6424 | Val Total: 0.6374\n",
      "Train Joint: 0.6424 | Train Loss Exit 1: 0.6424 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6374 | Val Loss Exit 1: 0.6374 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n",
      "Epoch [22/500] | Train Total: 0.6378 | Val Total: 0.6323\n",
      "Train Joint: 0.6378 | Train Loss Exit 1: 0.6378 | Train Loss Exit 2: 0.0000\n",
      "Val Joint: 0.6323 | Val Loss Exit 1: 0.6323 | Val Loss Exit 2: 0.0000\n",
      "Attack Threshold: 0.0000 | Normal Threshold: 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "limiar = train_model(\n",
    "    model.to(device), \n",
    "    train_loaders, \n",
    "    val_loaders, \n",
    "    epochs,\n",
    "    lr=0.0001,\n",
    "    device=device,\n",
    "    attack_threshold=0,\n",
    "    normal_threshold=0\n",
    ")\n",
    "torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "print(f\"\\nModelo treinado e salvo em 'models/{modelname}.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248ef82-7a24-4504-8cbc-ae2e4e3422fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'models/{modelname}.pth'))\n",
    "print(f\"Modelo 'models/{modelname}.pth' carregado\\n\")\n",
    "\n",
    "print(f\"Base: UNSW\")\n",
    "results = evaluate_model(model, test_loaders[0], device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: BOT\")\n",
    "results = evaluate_model(model, test_loaders[1], device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: CIC\")\n",
    "results = evaluate_model(model, test_loaders[2], device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b99544-909f-49c5-a212-93324562d197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361685d-5c70-46b6-945c-4532ea630054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
