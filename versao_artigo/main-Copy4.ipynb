{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff7fbd-35cd-4bf4-9064-7542fbe66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_creator import CreatorDL\n",
    "creator = CreatorDL(seed=42, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108772d1-330c-4ea7-b4fa-a96a9c5c06ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 1118865 | Teste: 559433 | Validação: 559433\n",
      "Processando a categoria: 'Fuzzers'\n",
      "  -> Treino: 16908 | Teste: 8454 | Validação: 8454\n",
      "Processando a categoria: 'Exploits'\n",
      "  -> Treino: 21374 | Teste: 10687 | Validação: 10687\n",
      "Processando a categoria: 'Backdoor'\n",
      "  -> Treino: 2329 | Teste: 1165 | Validação: 1165\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 8537 | Teste: 4268 | Validação: 4269\n",
      "Processando a categoria: 'Generic'\n",
      "  -> Treino: 9825 | Teste: 4913 | Validação: 4913\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 2990 | Teste: 1495 | Validação: 1495\n",
      "Processando a categoria: 'Shellcode'\n",
      "  -> Treino: 1190 | Teste: 595 | Validação: 596\n",
      "Processando a categoria: 'Analysis'\n",
      "  -> Treino: 613 | Teste: 306 | Validação: 307\n",
      "Processando a categoria: 'Worms'\n",
      "  -> Treino: 79 | Teste: 39 | Validação: 40\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 1182710 linhas\n",
      "Categorias presentes: ['Benign' 'Exploits' 'Reconnaissance' 'Fuzzers' 'DoS' 'Generic' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            1118865\n",
      "Exploits            21374\n",
      "Fuzzers             16908\n",
      "Generic              9825\n",
      "Reconnaissance       8537\n",
      "DoS                  2990\n",
      "Backdoor             2329\n",
      "Shellcode            1190\n",
      "Analysis              613\n",
      "Worms                  79\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 591355 linhas\n",
      "Categorias presentes: ['Benign' 'Generic' 'DoS' 'Reconnaissance' 'Exploits' 'Fuzzers' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4268\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            595\n",
      "Analysis             306\n",
      "Worms                 39\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 591359 linhas\n",
      "Categorias presentes: ['Benign' 'Fuzzers' 'Reconnaissance' 'Exploits' 'Generic' 'Analysis'\n",
      " 'Shellcode' 'Backdoor' 'DoS' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4269\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            596\n",
      "Analysis             307\n",
      "Worms                 40\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0516)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-1.4981e-07) tensor(4.5768) tensor(0.0510)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(-2.9962e-07) tensor(3.5191) tensor(0.0518)\n"
     ]
    }
   ],
   "source": [
    "df_UNSW = creator.reader(\"NF-UNSW-NB15-v3\")\n",
    "\n",
    "df_train_UNSW, df_test_UNSW, df_val_UNSW = creator.splitter(df_UNSW)\n",
    "\n",
    "train_loader_UNSW, test_loader_UNSW, val_loader_UNSW = creator.balancer(df_train_UNSW, df_test_UNSW, df_val_UNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8fca6f-a878-4ab7-bbef-2d77092947de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 25994 | Teste: 12997 | Validação: 12998\n",
      "Processando a categoria: 'DDoS'\n",
      "  -> Treino: 3575441 | Teste: 1787720 | Validação: 1787721\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 4017095 | Teste: 2008547 | Validação: 2008548\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 847566 | Teste: 423783 | Validação: 423783\n",
      "Processando a categoria: 'Theft'\n",
      "  -> Treino: 807 | Teste: 404 | Validação: 404\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 8466903 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               4017095\n",
      "DDoS              3575441\n",
      "Reconnaissance     847566\n",
      "Benign              25994\n",
      "Theft                 807\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 4233451 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008547\n",
      "DDoS              1787720\n",
      "Reconnaissance     423783\n",
      "Benign              12997\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 4233454 linhas\n",
      "Categorias presentes: ['DoS' 'DDoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008548\n",
      "DDoS              1787721\n",
      "Reconnaissance     423783\n",
      "Benign              12998\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0232)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.1910e-07) tensor(1.4751) tensor(0.0235)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(-1.7865e-07) tensor(5.3125) tensor(0.0232)\n"
     ]
    }
   ],
   "source": [
    "df_BOT= creator.reader(\"NF-BoT-IoT-v3\")\n",
    "\n",
    "df_train_BOT, df_test_BOT, df_val_BOT = creator.splitter(df_BOT)\n",
    "\n",
    "train_loader_BOT, test_loader_BOT, val_loader_BOT = creator.balancer(df_train_BOT, df_test_BOT, df_val_BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bc484e-aa09-40d0-967f-5ffe2b3c7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 8757313 | Teste: 4378656 | Validação: 4378657\n",
      "Processando a categoria: 'FTP-BruteForce'\n",
      "  -> Treino: 193360 | Teste: 96680 | Validação: 96680\n",
      "Processando a categoria: 'SSH-Bruteforce'\n",
      "  -> Treino: 94237 | Teste: 47118 | Validação: 47119\n",
      "Processando a categoria: 'DoS_attacks-GoldenEye'\n",
      "  -> Treino: 30650 | Teste: 15325 | Validação: 15325\n",
      "Processando a categoria: 'DoS_attacks-Slowloris'\n",
      "  -> Treino: 18020 | Teste: 9010 | Validação: 9010\n",
      "Processando a categoria: 'DoS_attacks-SlowHTTPTest'\n",
      "  -> Treino: 52775 | Teste: 26387 | Validação: 26388\n",
      "Processando a categoria: 'DoS_attacks-Hulk'\n",
      "  -> Treino: 50038 | Teste: 25019 | Validação: 25019\n",
      "Processando a categoria: 'DDoS_attacks-LOIC-HTTP'\n",
      "  -> Treino: 144294 | Teste: 72147 | Validação: 72148\n",
      "Processando a categoria: 'DDOS_attack-LOIC-UDP'\n",
      "  -> Treino: 1725 | Teste: 862 | Validação: 863\n",
      "Processando a categoria: 'DDOS_attack-HOIC'\n",
      "  -> Treino: 516155 | Teste: 258078 | Validação: 258078\n",
      "Processando a categoria: 'Brute_Force_-Web'\n",
      "  -> Treino: 809 | Teste: 404 | Validação: 405\n",
      "Processando a categoria: 'Brute_Force_-XSS'\n",
      "  -> Treino: 240 | Teste: 120 | Validação: 120\n",
      "Processando a categoria: 'SQL_Injection'\n",
      "  -> Treino: 220 | Teste: 110 | Validação: 110\n",
      "Processando a categoria: 'Infilteration'\n",
      "  -> Treino: 94076 | Teste: 47038 | Validação: 47038\n",
      "Processando a categoria: 'Bot'\n",
      "  -> Treino: 103851 | Teste: 51926 | Validação: 51926\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 10057763 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'FTP-BruteForce' 'DoS_attacks-Hulk' 'Bot' 'DoS_attacks-GoldenEye'\n",
      " 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Slowloris'\n",
      " 'Brute_Force_-Web' 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      8757313\n",
      "DDOS_attack-HOIC             516155\n",
      "FTP-BruteForce               193360\n",
      "DDoS_attacks-LOIC-HTTP       144294\n",
      "Bot                          103851\n",
      "SSH-Bruteforce                94237\n",
      "Infilteration                 94076\n",
      "DoS_attacks-SlowHTTPTest      52775\n",
      "DoS_attacks-Hulk              50038\n",
      "DoS_attacks-GoldenEye         30650\n",
      "DoS_attacks-Slowloris         18020\n",
      "DDOS_attack-LOIC-UDP           1725\n",
      "Brute_Force_-Web                809\n",
      "Brute_Force_-XSS                240\n",
      "SQL_Injection                   220\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 5028880 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDOS_attack-HOIC' 'FTP-BruteForce'\n",
      " 'SSH-Bruteforce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-LOIC-UDP' 'Bot'\n",
      " 'DoS_attacks-GoldenEye' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'DoS_attacks-Slowloris' 'Brute_Force_-Web' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378656\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72147\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47118\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26387\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            862\n",
      "Brute_Force_-Web                404\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 5028886 linhas\n",
      "Categorias presentes: ['Benign' 'FTP-BruteForce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'Bot' 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'Infilteration' 'DoS_attacks-GoldenEye' 'DoS_attacks-Slowloris'\n",
      " 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS' 'Brute_Force_-Web'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378657\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72148\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47119\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26388\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            863\n",
      "Brute_Force_-Web                405\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0473)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.4776) tensor(0.0477)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(2.7903) tensor(0.0478)\n"
     ]
    }
   ],
   "source": [
    "df_CIC= creator.reader(\"NF-CICIDS2018-v3\")\n",
    "\n",
    "df_train_CIC, df_test_CIC, df_val_CIC = creator.splitter(df_CIC)\n",
    "\n",
    "train_loader_CIC, test_loader_CIC, val_loader_CIC = creator.balancer(df_train_CIC, df_test_CIC, df_val_CIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75144f56-00b8-48f0-a649-6400560ce7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras combinadas: 54000\n",
      "Total de batches: 1688\n"
     ]
    }
   ],
   "source": [
    "loaders_para_concatenar_train = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "\n",
    "datasets_train = [loader.dataset for loader in loaders_para_concatenar_train]\n",
    "\n",
    "combined_dataset_train = ConcatDataset(datasets_train)\n",
    "\n",
    "combined_loader_train = DataLoader(combined_dataset_train, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Total de amostras combinadas: {len(combined_dataset_train)}\")\n",
    "print(f\"Total de batches: {len(combined_loader_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5a3dc1-562c-4247-bb4a-e23d677e6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras combinadas: 54000\n",
      "Total de batches: 1688\n"
     ]
    }
   ],
   "source": [
    "loaders_para_concatenar_val = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]\n",
    "\n",
    "datasets_val = [loader.dataset for loader in loaders_para_concatenar_val]\n",
    "\n",
    "combined_dataset_val = ConcatDataset(datasets_val)\n",
    "\n",
    "combined_loader_val = DataLoader(combined_dataset_val, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Total de amostras combinadas: {len(combined_dataset_val)}\")\n",
    "print(f\"Total de batches: {len(combined_loader_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "447280d4-e563-49b6-9b26-e49fd6d8a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de amostras combinadas: 54000\n",
      "Total de batches: 27\n"
     ]
    }
   ],
   "source": [
    "loaders_para_concatenar_test = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "\n",
    "datasets_test = [loader.dataset for loader in loaders_para_concatenar_test]\n",
    "\n",
    "combined_dataset_test = ConcatDataset(datasets_test)\n",
    "\n",
    "combined_loader_test = DataLoader(combined_dataset_test, batch_size=2048, shuffle=False)\n",
    "\n",
    "print(f\"Total de amostras combinadas: {len(combined_dataset_test)}\")\n",
    "print(f\"Total de batches: {len(combined_loader_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9457e890-0e00-4842-a0eb-2b9a4bc57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "test_loaders = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "val_loaders = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, num_classes=2):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_dim * 2, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        self.exit1_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.exit2_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward_exit1(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit1_layers(features)\n",
    "\n",
    "    def forward_exit2(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit2_layers(features)\n",
    "\n",
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "943446ef-8b5f-4e49-82a7-db448f6dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2259e486-8428-444d-b44b-27b7725707eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_soft_count(logits, thresholds, device, sharpness=50.0):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    confidence, pred_class = torch.max(probs, dim=1)\n",
    "    \n",
    "    if not isinstance(thresholds, torch.Tensor):\n",
    "        t_tensor = torch.tensor(thresholds, dtype=torch.float32).to(device)\n",
    "    else:\n",
    "        t_tensor = thresholds.to(device)\n",
    "        \n",
    "    selected_thresholds = t_tensor[pred_class]\n",
    "    \n",
    "    soft_decisions = torch.sigmoid(sharpness * (confidence - selected_thresholds))\n",
    "    \n",
    "    return soft_decisions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders, val_loaders, epochs, lr, attack_threshold, normal_threshold, desired_exit_ex1, device, patience=15):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.001, patience=7)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    metrics = [\n",
    "        'loss1_a', 'loss1_b', 'loss1_c', 'loss_ex1_avg',\n",
    "        'loss2_a', 'loss2_b', 'loss2_c', 'loss_ex2_avg',\n",
    "        'total_loss', 'joint_loss', 'ex1_pct', 'ex1_diff'\n",
    "    ]\n",
    "\n",
    "    history = {\n",
    "        'train': {k: [] for k in metrics},\n",
    "        'val': {k: [] for k in metrics}\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    max_train_batches = max(len(l) for l in train_loaders) \n",
    "    train_iter_loaders = [itertools.cycle(l) if len(l) < max_train_batches else l for l in train_loaders]\n",
    "    \n",
    "    max_val_batches = max(len(l) for l in val_loaders)\n",
    "    val_iter_loaders = [itertools.cycle(l) if len(l) < max_val_batches else l for l in val_loaders]\n",
    "\n",
    "    class_thresholds = [normal_threshold, attack_threshold]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_metrics = {k: 0.0 for k in metrics}\n",
    "        total_steps = 0\n",
    "\n",
    "        loader_iterators = [iter(l) for l in train_iter_loaders]\n",
    "                \n",
    "        for _ in range(max_train_batches):\n",
    "            try:\n",
    "                batches = [next(it) for it in loader_iterators]\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            (inputs_a, labels_a) = batches[0]\n",
    "            (inputs_b, labels_b) = batches[1]\n",
    "            (inputs_c, labels_c) = batches[2]\n",
    "            \n",
    "            inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "            inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "            inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "            total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "            \n",
    "            out_a_1 = model.forward_exit1(inputs_a)\n",
    "            loss_a_1 = criterion(out_a_1, labels_a)\n",
    "            \n",
    "            out_a_2 = model.forward_exit2(inputs_a)\n",
    "            loss_a_2 = criterion(out_a_2, labels_a)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "\n",
    "            out_b_1 = model.forward_exit1(inputs_b)\n",
    "            loss_b_1 = criterion(out_b_1, labels_b)\n",
    "            \n",
    "            out_b_2 = model.forward_exit2(inputs_b)\n",
    "            loss_b_2 = criterion(out_b_2, labels_b)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "\n",
    "            out_c_1 = model.forward_exit1(inputs_c)\n",
    "            loss_c_1 = criterion(out_c_1, labels_c)\n",
    "            \n",
    "            out_c_2 = model.forward_exit2(inputs_c)\n",
    "            loss_c_2 = criterion(out_c_2, labels_c)\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "\n",
    "            num_ex1_a = exit_soft_count(out_a_1, class_thresholds, device)\n",
    "            num_ex1_b = exit_soft_count(out_b_1, class_thresholds, device)\n",
    "            num_ex1_c = exit_soft_count(out_c_1, class_thresholds, device)\n",
    "\n",
    "            count_ex1 = num_ex1_a + num_ex1_b + num_ex1_c\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "\n",
    "            loss_ex1_avg = (loss_a_1 + loss_b_1 + loss_c_1) / 3\n",
    "            loss_ex2_avg = (loss_a_2 + loss_b_2 + loss_c_2) / 3\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "            \n",
    "            joint_loss = loss_ex1_avg * 1 + loss_ex2_avg * 0\n",
    "            \n",
    "            ex1_percentage = count_ex1 / total_samples\n",
    "\n",
    "            ex1_diff = abs(desired_exit_ex1 - ex1_percentage)\n",
    "            \n",
    "            total_loss = joint_loss + ex1_diff * 1\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------\n",
    "                        \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_metrics['loss1_a'] += loss_a_1.item()\n",
    "            running_metrics['loss1_b'] += loss_b_1.item()\n",
    "            running_metrics['loss1_c'] += loss_c_1.item()\n",
    "            running_metrics['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "            \n",
    "            running_metrics['loss2_a'] += loss_a_2.item()\n",
    "            running_metrics['loss2_b'] += loss_b_2.item()\n",
    "            running_metrics['loss2_c'] += loss_c_2.item()\n",
    "            running_metrics['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "            \n",
    "            running_metrics['total_loss'] += total_loss.item()\n",
    "            \n",
    "            running_metrics['joint_loss'] += joint_loss.item()\n",
    "            running_metrics['ex1_pct'] += ex1_percentage if isinstance(ex1_percentage, (float, int)) else ex1_percentage.item()\n",
    "            running_metrics['ex1_diff'] += ex1_diff.item() if isinstance(ex1_diff, torch.Tensor) else ex1_diff\n",
    "                        \n",
    "            total_steps += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['train'][key].append(running_metrics[key] / total_steps)\n",
    "        \n",
    "        epoch_train_loss = history['train']['total_loss'][-1]\n",
    "        epoch_train_loss1 = history['train']['loss_ex1_avg'][-1]\n",
    "        epoch_train_loss2 = history['train']['loss_ex2_avg'][-1]\n",
    "\n",
    "        model.eval()\n",
    "        running_metrics_val = {k: 0.0 for k in metrics}\n",
    "        total_steps_val = 0\n",
    "        \n",
    "        val_loader_iterators = [iter(l) for l in val_iter_loaders]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_val_batches):\n",
    "                try:\n",
    "                    batches = [next(it) for it in val_loader_iterators]\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                (inputs_a, labels_a) = batches[0]\n",
    "                (inputs_b, labels_b) = batches[1]\n",
    "                (inputs_c, labels_c) = batches[2]\n",
    "                \n",
    "                inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "                inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "                inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "                total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "                \n",
    "                out_a_1 = model.forward_exit1(inputs_a)\n",
    "                loss_a_1 = criterion(out_a_1, labels_a)\n",
    "                \n",
    "                out_a_2 = model.forward_exit2(inputs_a)\n",
    "                loss_a_2 = criterion(out_a_2, labels_a)\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "    \n",
    "                out_b_1 = model.forward_exit1(inputs_b)\n",
    "                loss_b_1 = criterion(out_b_1, labels_b)\n",
    "                \n",
    "                out_b_2 = model.forward_exit2(inputs_b)\n",
    "                loss_b_2 = criterion(out_b_2, labels_b)\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "    \n",
    "                out_c_1 = model.forward_exit1(inputs_c)\n",
    "                loss_c_1 = criterion(out_c_1, labels_c)\n",
    "                \n",
    "                out_c_2 = model.forward_exit2(inputs_c)\n",
    "                loss_c_2 = criterion(out_c_2, labels_c)\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "    \n",
    "                num_ex1_a = exit_soft_count(out_a_1, class_thresholds, device)\n",
    "                num_ex1_b = exit_soft_count(out_b_1, class_thresholds, device)\n",
    "                num_ex1_c = exit_soft_count(out_c_1, class_thresholds, device)\n",
    "    \n",
    "                count_ex1 = num_ex1_a + num_ex1_b + num_ex1_c\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "    \n",
    "                loss_ex1_avg = (loss_a_1 + loss_b_1 + loss_c_1) / 3\n",
    "                loss_ex2_avg = (loss_a_2 + loss_b_2 + loss_c_2) / 3\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "                \n",
    "                joint_loss = loss_ex1_avg * 1 + loss_ex2_avg * 0\n",
    "                \n",
    "                ex1_percentage = count_ex1 / total_samples\n",
    "    \n",
    "                ex1_diff = abs(desired_exit_ex1 - ex1_percentage)\n",
    "                \n",
    "                total_loss = joint_loss + ex1_diff * 1\n",
    "    \n",
    "                # ------------------------------------------------------------------------------------------\n",
    "                                                                                                            \n",
    "                running_metrics_val['loss1_a'] += loss_a_1.item()\n",
    "                running_metrics_val['loss1_b'] += loss_b_1.item()\n",
    "                running_metrics_val['loss1_c'] += loss_c_1.item()\n",
    "                running_metrics_val['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "                \n",
    "                running_metrics_val['loss2_a'] += loss_a_2.item()\n",
    "                running_metrics_val['loss2_b'] += loss_b_2.item()\n",
    "                running_metrics_val['loss2_c'] += loss_c_2.item()\n",
    "                running_metrics_val['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "                \n",
    "                running_metrics_val['total_loss'] += total_loss.item()\n",
    "\n",
    "                running_metrics_val['joint_loss'] += joint_loss.item()\n",
    "                running_metrics_val['ex1_pct'] += ex1_percentage if isinstance(ex1_percentage, (float, int)) else ex1_percentage.item()\n",
    "                running_metrics_val['ex1_diff'] += ex1_diff.item() if isinstance(ex1_diff, torch.Tensor) else ex1_diff\n",
    "                \n",
    "                total_steps_val += 1\n",
    "\n",
    "        for key in metrics:\n",
    "            history['val'][key].append(running_metrics_val[key] / total_steps_val)\n",
    "\n",
    "        epoch_val_loss = history['val']['total_loss'][-1]\n",
    "        epoch_val_loss1 = history['val']['loss_ex1_avg'][-1]\n",
    "        epoch_val_loss2 = history['val']['loss_ex2_avg'][-1]\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Train Total Loss: {epoch_train_loss:.4f} | Val Total Loss: {epoch_val_loss:.4f}\\nTrain Loss Exit 1: {epoch_train_loss1:.4f} | Train Loss Exit 2: {epoch_train_loss2:.4f}\\nVal Loss Exit 1: {epoch_val_loss1:.4f} | Val Loss Exit 2: {epoch_val_loss2:.4f}\\n\\n')        \n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                if best_model_state: model.load_state_dict(best_model_state)\n",
    "                break\n",
    "                \n",
    "        scheduler.step(epoch_val_loss)\n",
    "                \n",
    "    epochs_range = range(1, len(history['train']['total_loss']) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 5, figsize=(45, 7))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Exit 1\")\n",
    "    ax.plot(epochs_range, history['train']['loss1_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex1_avg'], label='Tr Avg', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss1_a'], label='Val A', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_b'], label='Val B', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_c'], label='Val C', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex1_avg'], label='Val Avg', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Exit 2\")\n",
    "    ax.plot(epochs_range, history['train']['loss2_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex2_avg'], label='Tr Avg', linewidth=2)\n",
    "    ax.plot(epochs_range, history['val']['loss2_a'], label='Val A', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_b'], label='Val B', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_c'], label='Val C', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex2_avg'], label='Val Avg', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Global Optimization\")\n",
    "    ax.plot(epochs_range, history['train']['total_loss'], label='Tr Total', color='orange')\n",
    "    ax.plot(epochs_range, history['val']['total_loss'], label='Val Total', color='orange', linestyle='--')\n",
    "    ax.plot(epochs_range, history['train']['joint_loss'], label='Tr Joint', color='blue', alpha=0.7)\n",
    "    ax.plot(epochs_range, history['val']['joint_loss'], label='Val Joint', color='blue', linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[3]\n",
    "    ax.set_title(\"Exit 1 Rate (Percentage)\")\n",
    "    ax.plot(epochs_range, history['train']['ex1_pct'], label='Tr Rate', color='green')\n",
    "    ax.plot(epochs_range, history['val']['ex1_pct'], label='Val Rate', color='green', linestyle='--')\n",
    "    ax.axhline(y=desired_exit_ex1, color='red', linestyle=':', label='Desired')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Ratio (0-1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[4]\n",
    "    ax.set_title(\"Exit 1 Rate Error (|Desired - Actual|)\")\n",
    "    ax.plot(epochs_range, history['train']['ex1_diff'], label='Tr Diff', color='purple')\n",
    "    ax.plot(epochs_range, history['val']['ex1_diff'], label='Val Diff', color='purple', linestyle='--')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9946bd-0c91-41de-a009-cf1b3e1d7a4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def precompute_outputs(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs_exit1 = []\n",
    "    all_probs_exit2 = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples = samples.to(device)\n",
    "            \n",
    "            output1 = model.forward_exit1(samples)\n",
    "            probs1 = F.softmax(output1, dim=1)\n",
    "            \n",
    "            output2 = model.forward_exit2(samples)\n",
    "            probs2 = F.softmax(output2, dim=1)\n",
    "\n",
    "            all_probs_exit1.append(probs1.cpu())\n",
    "            all_probs_exit2.append(probs2.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    probs_exit1 = torch.cat(all_probs_exit1)\n",
    "    probs_exit2 = torch.cat(all_probs_exit2)\n",
    "    targets = torch.cat(all_labels)\n",
    "    \n",
    "    return probs_exit1, probs_exit2, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc0ff47-e439-46bd-905c-5e16d9d12054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_decisions(preds1, confs1, preds2, confs2, t_atk1, t_norm1, t_atk2, t_norm2):\n",
    "    thresh_tensor1 = torch.where(preds1 == 1, t_atk1, t_norm1)\n",
    "    mask_exit1 = confs1 > thresh_tensor1\n",
    "    \n",
    "    thresh_tensor2 = torch.where(preds2 == 1, t_atk2, t_norm2)\n",
    "    mask_exit2 = (~mask_exit1) & (confs2 > thresh_tensor2)\n",
    "    \n",
    "    mask_rejected = (~mask_exit1) & (~mask_exit2)\n",
    "    \n",
    "    return mask_exit1, mask_exit2, mask_rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "997ca227-716c-49db-8f72-99352f3eefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(targets, preds1, preds2, mask_exit1, mask_rejected):\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    final_preds = preds2.clone()\n",
    "    final_preds[mask_exit1] = preds1[mask_exit1]\n",
    "    \n",
    "    mask_accepted = ~mask_rejected\n",
    "    \n",
    "    if mask_accepted.sum() == 0:\n",
    "        return 1.0, 0.0, 0.0, 0.0\n",
    "        \n",
    "    y_true_accepted = targets[mask_accepted]\n",
    "    y_pred_accepted = final_preds[mask_accepted]\n",
    "\n",
    "    cm = confusion_matrix(y_true_accepted, y_pred_accepted)\n",
    "    \n",
    "    f1 = f1_score(y_true_accepted.numpy(), y_pred_accepted.numpy(), zero_division=0)\n",
    "    error_rate = 1 - f1\n",
    "    \n",
    "    rate_exit1 = mask_exit1.sum().item() / total_samples\n",
    "    rate_rejection = mask_rejected.sum().item() / total_samples\n",
    "    \n",
    "    return error_rate, f1, rate_exit1, rate_rejection, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b651df4-007c-4867-8e7c-242485161ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholds_no_rejection(model, loader, device, params):\n",
    "    start_inf = time.time()\n",
    "    \n",
    "    t_atk1, t_norm1 = params[0], params[1]\n",
    "    \n",
    "    probs1, probs2, targets = precompute_outputs(model, loader, device)\n",
    "    \n",
    "    confs1, preds1 = torch.max(probs1, dim=1)\n",
    "    confs2, preds2 = torch.max(probs2, dim=1)\n",
    "    \n",
    "    thresh_tensor1 = torch.where(preds1 == 1, t_atk1, t_norm1)\n",
    "    \n",
    "    mask_exit1 = confs1 > thresh_tensor1\n",
    "    \n",
    "    mask_rejected = torch.zeros_like(mask_exit1, dtype=torch.bool)\n",
    "    \n",
    "    error, f1, rate_ex1, rate_rej, cm = calculate_metrics(\n",
    "        targets, preds1, preds2, mask_exit1, mask_rejected\n",
    "    )\n",
    "\n",
    "    end_inf = time.time()\n",
    "    t = end_inf - start_inf\n",
    "    \n",
    "    return {\n",
    "        'F1 Score': f1,\n",
    "        'Error Rate': error,\n",
    "        'Exit 1 Ratio': rate_ex1,\n",
    "        'Rejection Ratio': rate_rej,\n",
    "        't': t,\n",
    "        'Confusion Matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b8398ef-a9f0-481d-8c87-3490b0109fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_rejection(probs1, probs2, targets, max_reject_rate=0.2):\n",
    "    threshold_values = [round(x * 0.05, 2) for x in range(10, 21)]\n",
    "    \n",
    "    confs1, preds1 = torch.max(probs1, dim=1)\n",
    "    confs2, preds2 = torch.max(probs2, dim=1)\n",
    "    \n",
    "    results = {'rejection': [], 'error': [], 'exit1_rate': []}\n",
    "    \n",
    "    best_config = {\n",
    "        'error': float('inf'),\n",
    "        'params': None,\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    print(f\"Iniciando Grid Search ({len(threshold_values)**4} combinações)...\")\n",
    "    \n",
    "    for t_atk1, t_norm1, t_atk2, t_norm2 in itertools.product(threshold_values, repeat=4):\n",
    "\n",
    "        start_inf = time.time()\n",
    "        \n",
    "        mask_exit1, mask_exit2, mask_rejected = compute_decisions(\n",
    "            preds1, confs1, preds2, confs2, t_atk1, t_norm1, t_atk2, t_norm2\n",
    "        )\n",
    "        \n",
    "        cnt_rejected = mask_rejected.sum().item()\n",
    "        rate_rejection_pct = (cnt_rejected / len(targets))\n",
    "        \n",
    "        if rate_rejection_pct > max_reject_rate:\n",
    "            continue\n",
    "            \n",
    "        error, f1, rate_ex1, rate_rej, cm = calculate_metrics(\n",
    "            targets, preds1, preds2, mask_exit1, mask_rejected\n",
    "        )\n",
    "        \n",
    "        results['rejection'].append(rate_rej)\n",
    "        results['error'].append(error)\n",
    "        results['exit1_rate'].append(rate_ex1)\n",
    "\n",
    "        end_inf = time.time()\n",
    "        t = end_inf - start_inf\n",
    "        \n",
    "        if error < best_config['error']:\n",
    "            best_config['error'] = error\n",
    "            best_config['params'] = (t_atk1, t_norm1, t_atk2, t_norm2)\n",
    "            best_config['metrics'] = {\n",
    "                'F1': f1,\n",
    "                'Rejection': rate_rej,\n",
    "                'Exit1': rate_ex1,\n",
    "                't': t,\n",
    "                'Confusion Matrix': cm\n",
    "            }\n",
    "\n",
    "    return results, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc2035cb-374b-488e-a76b-d03ce1ba2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_thresholds_to_dataset(model, loader, device, params):\n",
    "    start_inf = time.time()\n",
    "    \n",
    "    t_atk1, t_norm1, t_atk2, t_norm2 = params\n",
    "    \n",
    "    probs1, probs2, targets = precompute_outputs(model, loader, device)\n",
    "    confs1, preds1 = torch.max(probs1, dim=1)\n",
    "    confs2, preds2 = torch.max(probs2, dim=1)\n",
    "    \n",
    "    mask_exit1, mask_exit2, mask_rejected = compute_decisions(\n",
    "        preds1, confs1, preds2, confs2, t_atk1, t_norm1, t_atk2, t_norm2\n",
    "    )\n",
    "    \n",
    "    error, f1, rate_ex1, rate_rej, cm = calculate_metrics(\n",
    "        targets, preds1, preds2, mask_exit1, mask_rejected\n",
    "    )\n",
    "\n",
    "    end_inf = time.time()\n",
    "\n",
    "    t = end_inf - start_inf\n",
    "    \n",
    "    return {\n",
    "        'F1 Score': f1,\n",
    "        'Error Rate': error,\n",
    "        'Exit 1 Ratio': rate_ex1,\n",
    "        'Rejection Ratio': rate_rej,\n",
    "        't': t,\n",
    "        'Confusion Matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc453e04-6009-41e9-8ad4-3c81241eece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_global_evaluation(model, combined_loader, individual_loaders_dict, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    start_inf = time.time()\n",
    "    probs_comb_1, probs_comb_2, targets_comb = precompute_outputs(model, combined_loader, device)\n",
    "    end_inf = time.time()\n",
    "    t = end_inf - start_inf\n",
    "    \n",
    "    start_time = time.time()\n",
    "    gs_history, best_config = grid_search_rejection(probs_comb_1, probs_comb_2, targets_comb)\n",
    "    print(f\"Grid Search concluído em {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    best_params = best_config['params']\n",
    "    print(f\"\\nMelhores Parâmetros (Concatenado): {best_params}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 7*0.8))\n",
    "\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    \n",
    "    plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    \n",
    "    sc = plt.scatter(gs_history['rejection'], gs_history['error'], \n",
    "                     c=gs_history['exit1_rate'], cmap='viridis', s=15, alpha=0.6)\n",
    "    \n",
    "    plt.colorbar(sc, label='Exit 1')\n",
    "    \n",
    "    plt.xlabel('Rejection Rate')\n",
    "    plt.ylabel('Error Rate (1 - F1)')\n",
    "\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.ylim(0.015, 0.105)\n",
    "    plt.xlim(0.00, 0.205)\n",
    "\n",
    "    plt.savefig('graficos/errorreject.pdf', bbox_inches='tight', pad_inches=0.02)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    results_list.append({\n",
    "        'Dataset': 'COMBINED (Global)',\n",
    "        'F1 Score': best_config['metrics']['F1'],\n",
    "        'Rejection': best_config['metrics']['Rejection'],\n",
    "        'Exit 1': best_config['metrics']['Exit1'],\n",
    "        'Time': best_config['metrics']['t'] + t,\n",
    "        'Confusion Matrix': best_config['metrics']['Confusion Matrix']\n",
    "        \n",
    "    })\n",
    "    \n",
    "    for name, loader in individual_loaders_dict.items():\n",
    "        print(f\"Processando {name}...\")\n",
    "        res = apply_thresholds_to_dataset(model, loader, device, best_params)\n",
    "        \n",
    "        results_list.append({\n",
    "            'Dataset': name,\n",
    "            'F1 Score': res['F1 Score'],\n",
    "            'Rejection': res['Rejection Ratio'],\n",
    "            'Exit 1': res['Exit 1 Ratio'],\n",
    "            'Time': res['t'],\n",
    "            'Confusion Matrix': res['Confusion Matrix']\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    display(df_results)\n",
    "    \n",
    "    return df_results, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bfbf4b5-dcc6-4bd5-96e5-b96988ebd70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final_0.4_v2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'final_0.4_v2'\n",
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "history = train_model(\n",
    "    model=model, \n",
    "    train_loaders=train_loaders, \n",
    "    val_loaders=val_loaders, \n",
    "    epochs=epochs, \n",
    "    lr=0.0001, \n",
    "    attack_threshold=0, \n",
    "    normal_threshold=0, \n",
    "    desired_exit_ex1=1, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "print(f\"\\nModelo treinado e salvo em 'models/{modelname}.pth'\")\n",
    "\n",
    "df_train = pd.DataFrame(history['train'])\n",
    "df_val = pd.DataFrame(history['val'])\n",
    "\n",
    "df_train = df_train.add_prefix('train_')\n",
    "df_val = df_val.add_prefix('val_')\n",
    "\n",
    "df_history = pd.concat([df_train, df_val], axis=1)\n",
    "\n",
    "df_history.insert(0, 'epoch', range(1, len(df_history) + 1))\n",
    "\n",
    "csv_filename = f'logs/{modelname}_history.csv'\n",
    "df_history.to_csv(csv_filename, index=False)\n",
    "print(f\"Curvas de aprendizado salvas em '{csv_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf22fe0-f9d6-4c9b-9d80-16d329fc05bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from: models/final_v2/final_0.4_v2.pth\n",
      "F1 = 0.9754\n",
      "Exit 1 Ratio = 0.5398\n",
      "Rejection Ratio = 0.0\n",
      "Total time = 0.7361 s\n",
      "Confusion Matrix:\n",
      "[[8759  241]\n",
      " [ 203 8797]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/final_v2/{modelname}.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model from: models/final_v2/{modelname}.pth\")\n",
    "\n",
    "metrics_unsw = apply_thresholds_no_rejection(model, test_loader_UNSW, device, [0.85, 0.85])\n",
    "\n",
    "print(f'F1 = {metrics_unsw['F1 Score']:.4}')\n",
    "print(f'Exit 1 Ratio = {metrics_unsw['Exit 1 Ratio']:.4}')\n",
    "print(f'Rejection Ratio = {metrics_unsw['Rejection Ratio']:.4}')\n",
    "print(f'Total time = {metrics_unsw['t']:.4} s')\n",
    "print(f'Confusion Matrix:\\n{metrics_unsw['Confusion Matrix']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57aa920e-09f1-4fd7-a84e-594836d6248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from: models/final_v2/final_0.4_v2.pth\n",
      "F1 = 0.9185\n",
      "Exit 1 Ratio = 0.3704\n",
      "Rejection Ratio = 0.0\n",
      "Total time = 0.3255 s\n",
      "Confusion Matrix:\n",
      "[[3845  155]\n",
      " [ 471 3529]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/final_v2/{modelname}.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model from: models/final_v2/{modelname}.pth\")\n",
    "\n",
    "metrics_bot = apply_thresholds_no_rejection(model, test_loader_BOT, device, [0.85, 0.85])\n",
    "\n",
    "print(f'F1 = {metrics_bot['F1 Score']:.4}')\n",
    "print(f'Exit 1 Ratio = {metrics_bot['Exit 1 Ratio']:.4}')\n",
    "print(f'Rejection Ratio = {metrics_bot['Rejection Ratio']:.4}')\n",
    "print(f'Total time = {metrics_bot['t']:.4} s')\n",
    "print(f'Confusion Matrix:\\n{metrics_bot['Confusion Matrix']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcf39b3d-cd80-4a5b-988b-d90f60ee1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from: models/final_v2/final_0.4_v2.pth\n",
      "F1 = 0.955\n",
      "Exit 1 Ratio = 0.263\n",
      "Rejection Ratio = 0.0\n",
      "Total time = 1.126 s\n",
      "Confusion Matrix:\n",
      "[[13798   202]\n",
      " [ 1020 12980]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/final_v2/{modelname}.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model from: models/final_v2/{modelname}.pth\")\n",
    "\n",
    "metrics_cic = apply_thresholds_no_rejection(model, test_loader_CIC, device, [0.85, 0.85])\n",
    "\n",
    "print(f'F1 = {metrics_cic['F1 Score']:.4}')\n",
    "print(f'Exit 1 Ratio = {metrics_cic['Exit 1 Ratio']:.4}')\n",
    "print(f'Rejection Ratio = {metrics_cic['Rejection Ratio']:.4}')\n",
    "print(f'Total time = {metrics_cic['t']:.4} s')\n",
    "print(f'Confusion Matrix:\\n{metrics_cic['Confusion Matrix']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc0a481e-395d-4160-a8c9-dcae785d32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from: models/final_v2/final_0.4_v2.pth\n",
      "F1 = 0.9567\n",
      "Exit 1 Ratio = 0.3712\n",
      "Rejection Ratio = 0.0\n",
      "Total time = 2.063 s\n",
      "Confusion Matrix:\n",
      "[[26402   598]\n",
      " [ 1694 25306]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/final_v2/{modelname}.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model from: models/final_v2/{modelname}.pth\")\n",
    "\n",
    "metrics_combined = apply_thresholds_no_rejection(model, combined_loader_test, device, [0.85, 0.85])\n",
    "\n",
    "print(f'F1 = {metrics_combined['F1 Score']:.4}')\n",
    "print(f'Exit 1 Ratio = {metrics_combined['Exit 1 Ratio']:.4}')\n",
    "print(f'Rejection Ratio = {metrics_combined['Rejection Ratio']:.4}')\n",
    "print(f'Total time = {metrics_combined['t']:.4} s')\n",
    "print(f'Confusion Matrix:\\n{metrics_combined['Confusion Matrix']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242dd04-c14d-4755-b668-99780b4625f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from: models/final_v2/final_0.4_v2.pth\n",
      "Iniciando Grid Search (14641 combinações)...\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'models/final_v2/{modelname}.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model from: models/final_v2/{modelname}.pth\")\n",
    "\n",
    "loaders_individuais = {\n",
    "    'UNSW': test_loader_UNSW,\n",
    "    'BoT-IoT': test_loader_BOT,\n",
    "    'CIC-IDS': test_loader_CIC\n",
    "}\n",
    "\n",
    "df_final, best_thresholds = run_global_evaluation(model, combined_loader_test, loaders_individuais, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3937e6b-898b-4693-9cbe-fb0f9ce04fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d76ef-ca73-44a7-8078-0a9146378f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5b287-e203-4c29-9a64-b3492c6c49fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
