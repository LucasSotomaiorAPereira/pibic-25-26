{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71fa66b-ed62-456d-b77e-584903b176f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2017b941-a25b-4b2f-9204-bf15ab595e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DROP_COLS = ['IPV4_SRC_ADDR',\n",
    "             'IPV4_DST_ADDR',\n",
    "             'L4_SRC_PORT',\n",
    "             'L4_DST_PORT',\n",
    "             'L7_PROTO',\n",
    "             'TCP_FLAGS',\n",
    "             'CLIENT_TCP_FLAGS',\n",
    "             'SERVER_TCP_FLAGS',\n",
    "             'MIN_TTL', \n",
    "             'MAX_TTL',\n",
    "             'SHORTEST_FLOW_PKT',\n",
    "             'MIN_IP_PKT_LEN', \n",
    "             'TCP_WIN_MAX_IN', \n",
    "             'TCP_WIN_MAX_OUT', \n",
    "             'DNS_QUERY_ID', \n",
    "             'DNS_TTL_ANSWER',\n",
    "             'FTP_COMMAND_RET_CODE',\n",
    "             'SRC_TO_DST_SECOND_BYTES',\n",
    "             'DST_TO_SRC_SECOND_BYTES',\n",
    "             'FLOW_START_MILLISECONDS',\n",
    "             'FLOW_END_MILLISECONDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7443699-9939-458c-a176-7ace528be0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(df, seed):\n",
    "    df_benign = df[df['Attack'] == 'Benign']\n",
    "    df_attacks = df[df['Attack'] != 'Benign']\n",
    "\n",
    "    min_samples = df_attacks['Attack'].value_counts().min()\n",
    "    target_samples = max(1000, min_samples)\n",
    "\n",
    "    df_attacks_balanced = df_attacks.groupby('Attack').sample(n=target_samples, replace=True, random_state=seed)\n",
    "    \n",
    "    total_attacks = len(df_attacks_balanced)\n",
    "    \n",
    "    replace_benign = len(df_benign) < total_attacks\n",
    "    df_benign_sampled = df_benign.sample(n=total_attacks, replace=replace_benign, random_state=seed)\n",
    "    \n",
    "    df_final = pd.concat([df_attacks_balanced, df_benign_sampled])\n",
    "    df_final = shuffle(df_final, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cccbd-2e0f-4f7c-a09b-e32d5dfb08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xy_tensors(X, y):\n",
    "    tensor_x = torch.tensor(X, dtype=torch.float32)\n",
    "    tensor_y = torch.tensor(y, dtype=torch.long)\n",
    "    return tensor_x, tensor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf9087-a36f-440e-969e-c459e5f65f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(filename, seed, batch_size):\n",
    "    print(f\"\\n--- Processando Arquivo: {filename} ---\")\n",
    "    \n",
    "    path = f'../db/{filename}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erro: Arquivo {path} n찾o encontrado.\")\n",
    "        return None\n",
    "\n",
    "    cols_to_drop = [c for c in DROP_COLS if c in df.columns]\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    X = df.drop(['Label', 'Attack'], axis=1)\n",
    "    y_stratify = df['Attack'] \n",
    "    \n",
    "    df_train, df_temp = train_test_split(df, test_size=0.5, random_state=seed, stratify=y_stratify)\n",
    "    \n",
    "\n",
    "    y_stratify_temp = df_temp['Attack']\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=seed, stratify=y_stratify_temp)\n",
    "    \n",
    "    print(f\"Splits iniciais -> Treino: {len(df_train)}, Val: {len(df_val)}, Teste: {len(df_test)}\")\n",
    "\n",
    "    df_train_balanced = balance_data(df_train, seed)\n",
    "    print(f\"Treino ap처s balanceamento: {len(df_train_balanced)}\")\n",
    "\n",
    "    print(df_train_balanced['Attack'].value_counts())\n",
    "\n",
    "    df_test_balanced = balance_data(df_test, seed)\n",
    "    print(f\"Treino ap처s balanceamento: {len(df_test_balanced)}\")\n",
    "\n",
    "    print(df_test_balanced['Attack'].value_counts())\n",
    "\n",
    "    df_val_balanced = balance_data(df_val seed)\n",
    "    print(f\"Treino ap처s balanceamento: {len(df_val_balanced)}\")\n",
    "\n",
    "    print(df_val_balanced['Attack'].value_counts())\n",
    "\n",
    "    def get_numpy_data(dataframe):\n",
    "        x_data = dataframe.drop(['Label', 'Attack'], axis=1).to_numpy()\n",
    "        y_data = dataframe['Label'].to_numpy()\n",
    "        return x_data, y_data\n",
    "\n",
    "    X_train, y_train = get_numpy_data(df_train_balanced)\n",
    "    X_val, y_val = get_numpy_data(df_val_balanced)\n",
    "    X_test, y_test = get_numpy_data(df_test_balanced)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    train_dataset = TensorDataset(*create_xy_tensors(X_train, y_train))\n",
    "    val_dataset = TensorDataset(*create_xy_tensors(X_val, y_val))\n",
    "    test_dataset = TensorDataset(*create_xy_tensors(X_test, y_test))\n",
    "    \n",
    "    return {\n",
    "        'train_loader': DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS),\n",
    "        'val_loader': DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS),\n",
    "        'test_loader': DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS),\n",
    "        'num_features': X_train.shape[1],\n",
    "        'scaler': scaler\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb1c3c-9ee4-4844-8450-693e5aaefe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "Xs = {}\n",
    "ys = {}\n",
    "dataloaders = {}\n",
    "filenames = ['NF-UNSW-NB15-v3', 'NF-BoT-IoT-v3', 'NF-CICIDS2018-v3']\n",
    "\n",
    "for f in filenames:\n",
    "    datasets[f] = dfs_creator(f, SEED)\n",
    "\n",
    "for dataset in datasets:\n",
    "    auxX = {}\n",
    "    auxy = {}\n",
    "    for i in dataset:\n",
    "        x, y = Xy_creator(i, SEED)\n",
    "        auxX[i] = x\n",
    "        auxy[i] = y\n",
    "    Xs[dataset] = auxX\n",
    "    ys[dataset] = auxy\n",
    "\n",
    "for x, y in zip(Xs, ys):\n",
    "    dataloaders[f] = dataloaders_creator(X[f], y[f], BATCH_SIZE, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
