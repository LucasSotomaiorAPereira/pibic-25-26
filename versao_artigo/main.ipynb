{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fff7fbd-35cd-4bf4-9064-7542fbe66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_creator import CreatorDL\n",
    "creator = CreatorDL(seed=42, bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108772d1-330c-4ea7-b4fa-a96a9c5c06ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 1118865 | Teste: 559433 | Validação: 559433\n",
      "Processando a categoria: 'Fuzzers'\n",
      "  -> Treino: 16908 | Teste: 8454 | Validação: 8454\n",
      "Processando a categoria: 'Exploits'\n",
      "  -> Treino: 21374 | Teste: 10687 | Validação: 10687\n",
      "Processando a categoria: 'Backdoor'\n",
      "  -> Treino: 2329 | Teste: 1165 | Validação: 1165\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 8537 | Teste: 4268 | Validação: 4269\n",
      "Processando a categoria: 'Generic'\n",
      "  -> Treino: 9825 | Teste: 4913 | Validação: 4913\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 2990 | Teste: 1495 | Validação: 1495\n",
      "Processando a categoria: 'Shellcode'\n",
      "  -> Treino: 1190 | Teste: 595 | Validação: 596\n",
      "Processando a categoria: 'Analysis'\n",
      "  -> Treino: 613 | Teste: 306 | Validação: 307\n",
      "Processando a categoria: 'Worms'\n",
      "  -> Treino: 79 | Teste: 39 | Validação: 40\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 1182710 linhas\n",
      "Categorias presentes: ['Benign' 'Exploits' 'Reconnaissance' 'Fuzzers' 'DoS' 'Generic' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            1118865\n",
      "Exploits            21374\n",
      "Fuzzers             16908\n",
      "Generic              9825\n",
      "Reconnaissance       8537\n",
      "DoS                  2990\n",
      "Backdoor             2329\n",
      "Shellcode            1190\n",
      "Analysis              613\n",
      "Worms                  79\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 591355 linhas\n",
      "Categorias presentes: ['Benign' 'Generic' 'DoS' 'Reconnaissance' 'Exploits' 'Fuzzers' 'Backdoor'\n",
      " 'Shellcode' 'Analysis' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4268\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            595\n",
      "Analysis             306\n",
      "Worms                 39\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 591359 linhas\n",
      "Categorias presentes: ['Benign' 'Fuzzers' 'Reconnaissance' 'Exploits' 'Generic' 'Analysis'\n",
      " 'Shellcode' 'Backdoor' 'DoS' 'Worms']\n",
      "Attack\n",
      "Benign            559433\n",
      "Exploits           10687\n",
      "Fuzzers             8454\n",
      "Generic             4913\n",
      "Reconnaissance      4269\n",
      "DoS                 1495\n",
      "Backdoor            1165\n",
      "Shellcode            596\n",
      "Analysis             307\n",
      "Worms                 40\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0516)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0502)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    9000\n",
      "0    9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            9000\n",
      "DoS               1000\n",
      "Shellcode         1000\n",
      "Generic           1000\n",
      "Analysis          1000\n",
      "Reconnaissance    1000\n",
      "Fuzzers           1000\n",
      "Worms             1000\n",
      "Exploits          1000\n",
      "Backdoor          1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([18000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([9000, 9000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0506)\n"
     ]
    }
   ],
   "source": [
    "df_UNSW = creator.reader(\"NF-UNSW-NB15-v3\")\n",
    "\n",
    "df_train_UNSW, df_test_UNSW, df_val_UNSW = creator.splitter(df_UNSW)\n",
    "\n",
    "train_loader_UNSW, test_loader_UNSW, val_loader_UNSW = creator.balancer(df_train_UNSW, df_test_UNSW, df_val_UNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8fca6f-a878-4ab7-bbef-2d77092947de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 25994 | Teste: 12997 | Validação: 12998\n",
      "Processando a categoria: 'DDoS'\n",
      "  -> Treino: 3575441 | Teste: 1787720 | Validação: 1787721\n",
      "Processando a categoria: 'DoS'\n",
      "  -> Treino: 4017095 | Teste: 2008547 | Validação: 2008548\n",
      "Processando a categoria: 'Reconnaissance'\n",
      "  -> Treino: 847566 | Teste: 423783 | Validação: 423783\n",
      "Processando a categoria: 'Theft'\n",
      "  -> Treino: 807 | Teste: 404 | Validação: 404\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 8466903 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               4017095\n",
      "DDoS              3575441\n",
      "Reconnaissance     847566\n",
      "Benign              25994\n",
      "Theft                 807\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 4233451 linhas\n",
      "Categorias presentes: ['DDoS' 'DoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008547\n",
      "DDoS              1787720\n",
      "Reconnaissance     423783\n",
      "Benign              12997\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 4233454 linhas\n",
      "Categorias presentes: ['DoS' 'DDoS' 'Reconnaissance' 'Benign' 'Theft']\n",
      "Attack\n",
      "DoS               2008548\n",
      "DDoS              1787721\n",
      "Reconnaissance     423783\n",
      "Benign              12998\n",
      "Theft                 404\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0232)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0203)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "1    4000\n",
      "0    4000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign            4000\n",
      "Reconnaissance    1000\n",
      "DoS               1000\n",
      "Theft             1000\n",
      "DDoS              1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([8000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([4000, 4000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0196)\n"
     ]
    }
   ],
   "source": [
    "df_BOT= creator.reader(\"NF-BoT-IoT-v3\")\n",
    "\n",
    "df_train_BOT, df_test_BOT, df_val_BOT = creator.splitter(df_BOT)\n",
    "\n",
    "train_loader_BOT, test_loader_BOT, val_loader_BOT = creator.balancer(df_train_BOT, df_test_BOT, df_val_BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bc484e-aa09-40d0-967f-5ffe2b3c7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando a categoria: 'Benign'\n",
      "  -> Treino: 8757313 | Teste: 4378656 | Validação: 4378657\n",
      "Processando a categoria: 'FTP-BruteForce'\n",
      "  -> Treino: 193360 | Teste: 96680 | Validação: 96680\n",
      "Processando a categoria: 'SSH-Bruteforce'\n",
      "  -> Treino: 94237 | Teste: 47118 | Validação: 47119\n",
      "Processando a categoria: 'DoS_attacks-GoldenEye'\n",
      "  -> Treino: 30650 | Teste: 15325 | Validação: 15325\n",
      "Processando a categoria: 'DoS_attacks-Slowloris'\n",
      "  -> Treino: 18020 | Teste: 9010 | Validação: 9010\n",
      "Processando a categoria: 'DoS_attacks-SlowHTTPTest'\n",
      "  -> Treino: 52775 | Teste: 26387 | Validação: 26388\n",
      "Processando a categoria: 'DoS_attacks-Hulk'\n",
      "  -> Treino: 50038 | Teste: 25019 | Validação: 25019\n",
      "Processando a categoria: 'DDoS_attacks-LOIC-HTTP'\n",
      "  -> Treino: 144294 | Teste: 72147 | Validação: 72148\n",
      "Processando a categoria: 'DDOS_attack-LOIC-UDP'\n",
      "  -> Treino: 1725 | Teste: 862 | Validação: 863\n",
      "Processando a categoria: 'DDOS_attack-HOIC'\n",
      "  -> Treino: 516155 | Teste: 258078 | Validação: 258078\n",
      "Processando a categoria: 'Brute_Force_-Web'\n",
      "  -> Treino: 809 | Teste: 404 | Validação: 405\n",
      "Processando a categoria: 'Brute_Force_-XSS'\n",
      "  -> Treino: 240 | Teste: 120 | Validação: 120\n",
      "Processando a categoria: 'SQL_Injection'\n",
      "  -> Treino: 220 | Teste: 110 | Validação: 110\n",
      "Processando a categoria: 'Infilteration'\n",
      "  -> Treino: 94076 | Teste: 47038 | Validação: 47038\n",
      "Processando a categoria: 'Bot'\n",
      "  -> Treino: 103851 | Teste: 51926 | Validação: 51926\n",
      "\n",
      "--- Base de Treino ---\n",
      "Tamanho: 10057763 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'FTP-BruteForce' 'DoS_attacks-Hulk' 'Bot' 'DoS_attacks-GoldenEye'\n",
      " 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Slowloris'\n",
      " 'Brute_Force_-Web' 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      8757313\n",
      "DDOS_attack-HOIC             516155\n",
      "FTP-BruteForce               193360\n",
      "DDoS_attacks-LOIC-HTTP       144294\n",
      "Bot                          103851\n",
      "SSH-Bruteforce                94237\n",
      "Infilteration                 94076\n",
      "DoS_attacks-SlowHTTPTest      52775\n",
      "DoS_attacks-Hulk              50038\n",
      "DoS_attacks-GoldenEye         30650\n",
      "DoS_attacks-Slowloris         18020\n",
      "DDOS_attack-LOIC-UDP           1725\n",
      "Brute_Force_-Web                809\n",
      "Brute_Force_-XSS                240\n",
      "SQL_Injection                   220\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Teste ---\n",
      "Tamanho: 5028880 linhas\n",
      "Categorias presentes: ['Benign' 'Infilteration' 'DDOS_attack-HOIC' 'FTP-BruteForce'\n",
      " 'SSH-Bruteforce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-LOIC-UDP' 'Bot'\n",
      " 'DoS_attacks-GoldenEye' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'DoS_attacks-Slowloris' 'Brute_Force_-Web' 'Brute_Force_-XSS'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378656\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72147\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47118\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26387\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            862\n",
      "Brute_Force_-Web                404\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- Base de Validação ---\n",
      "Tamanho: 5028886 linhas\n",
      "Categorias presentes: ['Benign' 'FTP-BruteForce' 'DDoS_attacks-LOIC-HTTP' 'DDOS_attack-HOIC'\n",
      " 'Bot' 'SSH-Bruteforce' 'DoS_attacks-SlowHTTPTest' 'DoS_attacks-Hulk'\n",
      " 'Infilteration' 'DoS_attacks-GoldenEye' 'DoS_attacks-Slowloris'\n",
      " 'DDOS_attack-LOIC-UDP' 'Brute_Force_-XSS' 'Brute_Force_-Web'\n",
      " 'SQL_Injection']\n",
      "Attack\n",
      "Benign                      4378657\n",
      "DDOS_attack-HOIC             258078\n",
      "FTP-BruteForce                96680\n",
      "DDoS_attacks-LOIC-HTTP        72148\n",
      "Bot                           51926\n",
      "SSH-Bruteforce                47119\n",
      "Infilteration                 47038\n",
      "DoS_attacks-SlowHTTPTest      26388\n",
      "DoS_attacks-Hulk              25019\n",
      "DoS_attacks-GoldenEye         15325\n",
      "DoS_attacks-Slowloris          9010\n",
      "DDOS_attack-LOIC-UDP            863\n",
      "Brute_Force_-Web                405\n",
      "Brute_Force_-XSS                120\n",
      "SQL_Injection                   110\n",
      "Name: count, dtype: int64\n",
      "-------------------------\n",
      "\n",
      "--- train ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0473)\n",
      "-------------------------\n",
      "\n",
      "--- test ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0498)\n",
      "-------------------------\n",
      "\n",
      "--- val ---\n",
      "Label\n",
      "0    14000\n",
      "1    14000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Attack\n",
      "Benign                      14000\n",
      "DDoS_attacks-LOIC-HTTP       1000\n",
      "Brute_Force_-Web             1000\n",
      "FTP-BruteForce               1000\n",
      "Infilteration                1000\n",
      "SSH-Bruteforce               1000\n",
      "DoS_attacks-GoldenEye        1000\n",
      "DoS_attacks-SlowHTTPTest     1000\n",
      "DoS_attacks-Slowloris        1000\n",
      "DDOS_attack-LOIC-UDP         1000\n",
      "DoS_attacks-Hulk             1000\n",
      "SQL_Injection                1000\n",
      "Bot                          1000\n",
      "DDOS_attack-HOIC             1000\n",
      "Brute_Force_-XSS             1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "torch.Size([28000, 32])\n",
      "\n",
      "(tensor([0, 1]), tensor([14000, 14000]))\n",
      "tensor(0.) tensor(1.) tensor(0.0445)\n"
     ]
    }
   ],
   "source": [
    "df_CIC= creator.reader(\"NF-CICIDS2018-v3\")\n",
    "\n",
    "df_train_CIC, df_test_CIC, df_val_CIC = creator.splitter(df_CIC)\n",
    "\n",
    "train_loader_CIC, test_loader_CIC, val_loader_CIC = creator.balancer(df_train_CIC, df_test_CIC, df_val_CIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9457e890-0e00-4842-a0eb-2b9a4bc57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "test_loaders = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "val_loaders = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, num_classes=2):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.tensor([1.4])) \n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.exit1_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.exit2_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def get_threshold(self):\n",
    "        return torch.sigmoid(self.alpha)\n",
    "\n",
    "    def forward_exit1(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit1_layers(features)\n",
    "\n",
    "    def forward_exit2(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit2_layers(features)\n",
    "\n",
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943446ef-8b5f-4e49-82a7-db448f6dede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders_dict, val_loader_dict, epochs, lr, device, patience=15):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    sigmoid = nn.Sigmoid()  \n",
    "\n",
    "    metrics = [\n",
    "        'loss1_a', 'loss1_b', 'loss1_c', 'loss_ex1_avg',\n",
    "        'loss2_a', 'loss2_b', 'loss2_c', 'loss_ex2_avg',\n",
    "        'l_joint', 'total_loss'\n",
    "    ]\n",
    "\n",
    "    history = {\n",
    "        'train': {k: [] for k in metrics},\n",
    "        'val': {k: [] for k in metrics}\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_metrics = {k: 0.0 for k in metrics}\n",
    "        total_steps = 0\n",
    "                \n",
    "        for (inputs_a, labels_a), (inputs_b, labels_b), (inputs_c, labels_c) in zip(train_loaders_dict[0], train_loaders_dict[1], train_loaders_dict[2]):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_threshold = model.get_threshold()\n",
    "            \n",
    "            inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "            inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "            inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "            total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "\n",
    "            out1_a = model.forward_exit1(inputs_a)\n",
    "            out1_b = model.forward_exit1(inputs_b)\n",
    "            out1_c = model.forward_exit1(inputs_c)\n",
    "\n",
    "            curr_thresh_val = current_threshold.item()\n",
    "\n",
    "            probs_a = F.softmax(out1_a, dim=1)\n",
    "            conf_a, _ = torch.max(probs_a, dim=1)\n",
    "\n",
    "            probs_b = F.softmax(out1_b, dim=1)\n",
    "            conf_b, _ = torch.max(probs_b, dim=1)\n",
    "\n",
    "            probs_c = F.softmax(out1_c, dim=1)\n",
    "            conf_c, _ = torch.max(probs_c, dim=1)\n",
    "\n",
    "            mask_a_ex1 = conf_a > curr_thresh_val\n",
    "            mask_b_ex1 = conf_b > curr_thresh_val\n",
    "            mask_c_ex1 = conf_c > curr_thresh_val\n",
    "\n",
    "            mask_a_ex2 = conf_a < curr_thresh_val\n",
    "            mask_b_ex2 = conf_b < curr_thresh_val\n",
    "            mask_c_ex2 = conf_c < curr_thresh_val\n",
    "\n",
    "            loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "            loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "            loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "            loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "\n",
    "            k = 10\n",
    "            prob_exit2_a = sigmoid(k * (current_threshold - conf_a))\n",
    "            prob_exit2_b = sigmoid(k * (current_threshold - conf_b))\n",
    "            prob_exit2_c = sigmoid(k * (current_threshold - conf_c))\n",
    "            soft_count_exit2 = torch.sum(prob_exit2_a) + torch.sum(prob_exit2_b) + torch.sum(prob_exit2_c)\n",
    "\n",
    "            loss2_a = torch.tensor(0.0, device=device)\n",
    "            loss2_b = torch.tensor(0.0, device=device)\n",
    "            loss2_c = torch.tensor(0.0, device=device)\n",
    "\n",
    "            if mask_a_ex2.any():\n",
    "                loss2_a = criterion(model.forward_exit2(inputs_a[mask_a_ex2]), labels_a[mask_a_ex2])\n",
    "            if mask_b_ex2.any():\n",
    "                loss2_b = criterion(model.forward_exit2(inputs_b[mask_b_ex2]), labels_b[mask_b_ex2])\n",
    "            if mask_c_ex2.any():\n",
    "                loss2_c = criterion(model.forward_exit2(inputs_c[mask_c_ex2]), labels_c[mask_c_ex2])\n",
    "\n",
    "            loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "\n",
    "            l_joint = loss_ex1_avg + loss_ex2_avg\n",
    "            total_loss = l_joint + (soft_count_exit2 / total_samples)\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_metrics['loss1_a'] += loss1_a.item()\n",
    "            running_metrics['loss1_b'] += loss1_b.item()\n",
    "            running_metrics['loss1_c'] += loss1_c.item()\n",
    "            running_metrics['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "            \n",
    "            running_metrics['loss2_a'] += loss2_a.item()\n",
    "            running_metrics['loss2_b'] += loss2_b.item()\n",
    "            running_metrics['loss2_c'] += loss2_c.item()\n",
    "            running_metrics['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "            \n",
    "            running_metrics['l_joint'] += l_joint.item()\n",
    "            running_metrics['total_loss'] += total_loss.item()\n",
    "            \n",
    "            total_steps += 1\n",
    "\n",
    "        for k in metrics:\n",
    "            history['train'][k].append(running_metrics[k] / total_steps)\n",
    "        \n",
    "        epoch_train_loss = history['train']['total_loss'][-1]\n",
    "\n",
    "        model.eval()\n",
    "        running_metrics_val = {k: 0.0 for k in metrics}\n",
    "        total_steps_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current_threshold = model.get_threshold()\n",
    "            curr_thresh_val = current_threshold.item()\n",
    "\n",
    "            for (inputs_a, labels_a), (inputs_b, labels_b), (inputs_c, labels_c) in zip(val_loader_dict[0], val_loader_dict[1], val_loader_dict[2]):\n",
    "                inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "                inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "                inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "                total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "    \n",
    "                out1_a = model.forward_exit1(inputs_a)\n",
    "                out1_b = model.forward_exit1(inputs_b)\n",
    "                out1_c = model.forward_exit1(inputs_c)\n",
    "    \n",
    "                curr_thresh_val = current_threshold.item()\n",
    "\n",
    "                probs_a = F.softmax(out1_a, dim=1)\n",
    "                conf_a, _ = torch.max(probs_a, dim=1)\n",
    "    \n",
    "                probs_b = F.softmax(out1_b, dim=1)\n",
    "                conf_b, _ = torch.max(probs_b, dim=1)\n",
    "    \n",
    "                probs_c = F.softmax(out1_c, dim=1)\n",
    "                conf_c, _ = torch.max(probs_c, dim=1)\n",
    "    \n",
    "                mask_a_ex1 = conf_a > curr_thresh_val\n",
    "                mask_b_ex1 = conf_b > curr_thresh_val\n",
    "                mask_c_ex1 = conf_c > curr_thresh_val\n",
    "    \n",
    "                mask_a_ex2 = conf_a < curr_thresh_val\n",
    "                mask_b_ex2 = conf_b < curr_thresh_val\n",
    "                mask_c_ex2 = conf_c < curr_thresh_val\n",
    "    \n",
    "                loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "                loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "                loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "                loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "                \n",
    "                real_count_exit2 = mask_a_ex2.sum() + mask_b_ex2.sum() + mask_c_ex2.sum()\n",
    "    \n",
    "                loss2_a = torch.tensor(0.0, device=device)\n",
    "                loss2_b = torch.tensor(0.0, device=device)\n",
    "                loss2_c = torch.tensor(0.0, device=device)\n",
    "    \n",
    "                if mask_a_ex2.any(): loss2_a = criterion(model.forward_exit2(inputs_a[mask_a_ex2]), labels_a[mask_a_ex2])\n",
    "                if mask_b_ex2.any(): loss2_b = criterion(model.forward_exit2(inputs_b[mask_b_ex2]), labels_b[mask_b_ex2])\n",
    "                if mask_c_ex2.any(): loss2_c = criterion(model.forward_exit2(inputs_c[mask_c_ex2]), labels_c[mask_c_ex2])\n",
    "    \n",
    "                loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "    \n",
    "                total_loss = l_joint + (real_count_exit2 / total_samples)\n",
    "                \n",
    "                running_metrics_val['loss1_a'] += loss1_a.item()\n",
    "                running_metrics_val['loss1_b'] += loss1_b.item()\n",
    "                running_metrics_val['loss1_c'] += loss1_c.item()\n",
    "                running_metrics_val['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "                \n",
    "                running_metrics_val['loss2_a'] += loss2_a.item()\n",
    "                running_metrics_val['loss2_b'] += loss2_b.item()\n",
    "                running_metrics_val['loss2_c'] += loss2_c.item()\n",
    "                running_metrics_val['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "                \n",
    "                running_metrics_val['l_joint'] += l_joint.item()\n",
    "                running_metrics_val['total_loss'] += total_loss.item()\n",
    "                \n",
    "                total_steps_val += 1\n",
    "\n",
    "        for k in metrics:\n",
    "            history['val'][k].append(running_metrics_val[k] / total_steps_val)\n",
    "\n",
    "        epoch_val_loss = history['val']['total_loss'][-1]\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Train Total: {epoch_train_loss:.4f} | Val Total: {epoch_val_loss:.4f} | Alpha: {curr_thresh_val}')\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                if best_model_state: model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    epochs_range = range(1, len(history['train']['total_loss']) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Exit 1\")\n",
    "    ax.plot(epochs_range, history['train']['loss1_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex1_avg'], label='Tr Avg', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['loss1_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex1_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Exit 2\")\n",
    "    ax.plot(epochs_range, history['train']['loss2_a'], label='Tr A', color='blue', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_b'], label='Tr B', color='green', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_c'], label='Tr C', color='red', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex2_avg'], label='Tr Avg', color='black', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['loss2_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex2_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Global Optimization\")\n",
    "    ax.plot(epochs_range, history['train']['l_joint'], label='Tr Joint (Ex1 + Ex2)', color='purple')\n",
    "    ax.plot(epochs_range, history['train']['total_loss'], label='Tr Total (Joint + (NEx2 / N))', color='orange', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['l_joint'], label='Val Joint', color='purple', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['total_loss'], label='Val Total', color='orange', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return curr_thresh_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd9acd9be6862e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, confidence_threshold, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(loader.dataset)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    exited_early_count = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            branch_output = model.forward_exit1(samples)\n",
    "            \n",
    "            branch_prob = F.softmax(branch_output, dim=1)\n",
    "            trusts, branch_preds = torch.max(branch_prob, 1)\n",
    "\n",
    "            batch_predictions = torch.zeros_like(labels)\n",
    "            \n",
    "            early_exit_mask = trusts > confidence_threshold\n",
    "            \n",
    "            if early_exit_mask.any():\n",
    "                batch_predictions[early_exit_mask] = branch_preds[early_exit_mask]\n",
    "                exited_early_count += early_exit_mask.sum().item()\n",
    "\n",
    "            main_branch_mask = ~early_exit_mask\n",
    "            if main_branch_mask.any():\n",
    "                \n",
    "                samples_to_main = samples[main_branch_mask]\n",
    "                \n",
    "                main_output = model.forward_exit2(samples_to_main)\n",
    "                \n",
    "                main_prob = F.softmax(main_output, dim=1)\n",
    "                _, main_preds = torch.max(main_prob, 1)\n",
    "                \n",
    "                batch_predictions[main_branch_mask] = main_preds\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions)\n",
    "    y_data = torch.cat(all_labels)\n",
    "\n",
    "    correct = (final_predictions == y_data).sum().item()\n",
    "    accuracy = 100 * correct / total_samples\n",
    "    exit_rate = 100 * exited_early_count / total_samples\n",
    "    avg_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Ataque'],\n",
    "                yticklabels=['Normal', 'Ataque'])\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão (Limiar de Confiança = {confidence_threshold})')\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    f1 = f1_score(y_data.numpy(), final_predictions.numpy())\n",
    "    \n",
    "    tpr = recall_score(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\\n\")\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positive Rate (TPR) / Recall: {tpr:.4f}\")\n",
    "    print(f\"True Negative Rate (TNR) / Specificity: {tnr:.4f}\")\n",
    "\n",
    "    return {\n",
    "            'accuracy': accuracy,\n",
    "            'exit_rate': exit_rate,\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'exited_early_count': exited_early_count,\n",
    "            'total_samples': total_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bfbf4b5-dcc6-4bd5-96e5-b96988ebd70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teste6'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'teste6'\n",
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] | Train Total: nan | Val Total: nan | Alpha: 0.8021775484085083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x766a5e417ba0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m epochs = \u001b[32m1000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m limiar = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.00001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m torch.save(model.state_dict(), \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mModelo treinado e salvo em \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loaders_dict, val_loader_dict, epochs, lr, device, patience)\u001b[39m\n\u001b[32m    117\u001b[39m current_threshold = model.get_threshold()\n\u001b[32m    118\u001b[39m curr_thresh_val = current_threshold.item()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_c\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_loader_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_a\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_a\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_b\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_b\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/pibic-25-26/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/pibic-25-26/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/pibic-25-26/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/pibic-25-26/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "limiar = train_model(model, train_loaders, val_loaders, epochs, lr=0.00001, device=device)\n",
    "\n",
    "torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "print(f\"\\nModelo treinado e salvo em 'models/{modelname}.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248ef82-7a24-4504-8cbc-ae2e4e3422fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'models/{modelname}.pth'))\n",
    "print(f\"Modelo 'models/{modelname}.pth' carregado\\n\")\n",
    "\n",
    "print(f\"Base: UNSW\")\n",
    "results = evaluate_model(model, test_loaders[0], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: BOT\")\n",
    "results = evaluate_model(model, test_loaders[1], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: CIC\")\n",
    "results = evaluate_model(model, test_loaders[2], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d1f61-6752-4720-bddb-8f1ab58f5141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6767f08-a37d-4e18-83ba-cc200572d205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
