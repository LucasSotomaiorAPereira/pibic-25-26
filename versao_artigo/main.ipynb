{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66eec202eab352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T13:51:59.291523Z",
     "start_time": "2025-09-16T13:51:59.286456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff7fbd-35cd-4bf4-9064-7542fbe66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_creator import CreatorDL\n",
    "creator = CreatorDL(seed=42, bs=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108772d1-330c-4ea7-b4fa-a96a9c5c06ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_UNSW = creator.reader(\"NF-UNSW-NB15-v3\")\n",
    "\n",
    "df_train_UNSW, df_test_UNSW, df_val_UNSW = creator.splitter(df_UNSW)\n",
    "\n",
    "train_loader_UNSW, test_loader_UNSW, val_loader_UNSW = creator.balancer(df_train_UNSW, df_test_UNSW, df_val_UNSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fca6f-a878-4ab7-bbef-2d77092947de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_BOT= creator.reader(\"NF-BoT-IoT-v3\")\n",
    "\n",
    "df_train_BOT, df_test_BOT, df_val_BOT = creator.splitter(df_BOT)\n",
    "\n",
    "train_loader_BOT, test_loader_BOT, val_loader_BOT = creator.balancer(df_train_BOT, df_test_BOT, df_val_BOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc484e-aa09-40d0-967f-5ffe2b3c7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_CIC= creator.reader(\"NF-CICIDS2018-v3\")\n",
    "\n",
    "df_train_CIC, df_test_CIC, df_val_CIC = creator.splitter(df_CIC)\n",
    "\n",
    "train_loader_CIC, test_loader_CIC, val_loader_CIC = creator.balancer(df_train_CIC, df_test_CIC, df_val_CIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457e890-0e00-4842-a0eb-2b9a4bc57e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [train_loader_UNSW, train_loader_BOT, train_loader_CIC]\n",
    "test_loaders = [test_loader_UNSW, test_loader_BOT, test_loader_CIC]\n",
    "val_loaders = [val_loader_UNSW, val_loader_BOT, val_loader_CIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdd023e2b43ce8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:18.608856Z",
     "start_time": "2025-09-16T14:23:18.591073Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 32\n",
    "\n",
    "class IDSBranchyNet(nn.Module):\n",
    "    def __init__(self, input_dim=INPUT_DIM, num_classes=2):\n",
    "        super(IDSBranchyNet, self).__init__()\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.tensor([0.4])) \n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.exit1_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.exit2_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def get_threshold(self):\n",
    "        return torch.sigmoid(self.alpha)\n",
    "\n",
    "    def forward_exit1(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit1_layers(features)\n",
    "\n",
    "    def forward_exit2(self, x):\n",
    "        features = self.shared_layers(x)\n",
    "        return self.exit2_layers(features)\n",
    "\n",
    "model = IDSBranchyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943446ef-8b5f-4e49-82a7-db448f6dede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdbb5806d45549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.555965Z",
     "start_time": "2025-09-16T14:23:19.548893Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders, val_loaders, epochs, lr, device, patience=15):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.001, patience=7)\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    sigmoid = nn.Sigmoid()  \n",
    "\n",
    "    metrics = [\n",
    "        'loss1_a', 'loss1_b', 'loss1_c', 'loss_ex1_avg',\n",
    "        'loss2_a', 'loss2_b', 'loss2_c', 'loss_ex2_avg',\n",
    "        'l_joint', 'total_loss'\n",
    "    ]\n",
    "\n",
    "    history = {\n",
    "        'train': {k: [] for k in metrics},\n",
    "        'val': {k: [] for k in metrics}\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    max_train_batches = max(len(l) for l in train_loaders) \n",
    "    train_iter_loaders = [itertools.cycle(l) if len(l) < max_train_batches else l for l in train_loaders]\n",
    "    \n",
    "    max_val_batches = max(len(l) for l in val_loaders)\n",
    "    val_iter_loaders = [itertools.cycle(l) if len(l) < max_val_batches else l for l in val_loaders]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_metrics = {k: 0.0 for k in metrics}\n",
    "        total_steps = 0\n",
    "\n",
    "        loader_iterators = [iter(l) for l in train_iter_loaders]\n",
    "                \n",
    "        for _ in range(max_train_batches):\n",
    "            try:\n",
    "                batches = [next(it) for it in loader_iterators]\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_threshold = model.get_threshold()\n",
    "\n",
    "            (inputs_a, labels_a) = batches[0]\n",
    "            (inputs_b, labels_b) = batches[1]\n",
    "            (inputs_c, labels_c) = batches[2]\n",
    "            \n",
    "            inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "            inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "            inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "            total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "\n",
    "            out1_a = model.forward_exit1(inputs_a)\n",
    "            out1_b = model.forward_exit1(inputs_b)\n",
    "            out1_c = model.forward_exit1(inputs_c)\n",
    "\n",
    "            curr_thresh_val = current_threshold.item()\n",
    "\n",
    "            probs_a = F.softmax(out1_a, dim=1)\n",
    "            conf_a, _ = torch.max(probs_a, dim=1)\n",
    "\n",
    "            probs_b = F.softmax(out1_b, dim=1)\n",
    "            conf_b, _ = torch.max(probs_b, dim=1)\n",
    "\n",
    "            probs_c = F.softmax(out1_c, dim=1)\n",
    "            conf_c, _ = torch.max(probs_c, dim=1)\n",
    "\n",
    "            mask_a_ex1 = conf_a > curr_thresh_val\n",
    "            mask_b_ex1 = conf_b > curr_thresh_val\n",
    "            mask_c_ex1 = conf_c > curr_thresh_val\n",
    "\n",
    "            mask_a_ex2 = conf_a < curr_thresh_val\n",
    "            mask_b_ex2 = conf_b < curr_thresh_val\n",
    "            mask_c_ex2 = conf_c < curr_thresh_val\n",
    "\n",
    "            k = 10\n",
    "\n",
    "            prob_exit1_a = sigmoid(k * (conf_a - current_threshold))\n",
    "            prob_exit1_b = sigmoid(k * (conf_b - current_threshold))\n",
    "            prob_exit1_c = sigmoid(k * (conf_c - current_threshold))\n",
    "            soft_count_exit1 = torch.sum(prob_exit1_a) + torch.sum(prob_exit1_b) + torch.sum(prob_exit1_c)\n",
    "        \n",
    "            prob_exit2_a = sigmoid(k * (current_threshold - conf_a))\n",
    "            prob_exit2_b = sigmoid(k * (current_threshold - conf_b))\n",
    "            prob_exit2_c = sigmoid(k * (current_threshold - conf_c))\n",
    "            soft_count_exit2 = torch.sum(prob_exit2_a) + torch.sum(prob_exit2_b) + torch.sum(prob_exit2_c)\n",
    "\n",
    "            loss1_a = torch.tensor(0.0, device=device)\n",
    "            loss1_b = torch.tensor(0.0, device=device)\n",
    "            loss1_c = torch.tensor(0.0, device=device)\n",
    "\n",
    "            if mask_a_ex1.any():\n",
    "                loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "            if mask_b_ex1.any():\n",
    "                loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "            if mask_c_ex1.any():\n",
    "                loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "\n",
    "            # loss1_a = criterion(out1_a, labels_a)\n",
    "            # loss1_b = criterion(out1_b, labels_b)\n",
    "            # loss1_c = criterion(out1_c, labels_c)\n",
    "                \n",
    "            loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "\n",
    "            loss2_a = torch.tensor(0.0, device=device)\n",
    "            loss2_b = torch.tensor(0.0, device=device)\n",
    "            loss2_c = torch.tensor(0.0, device=device)\n",
    "\n",
    "            if mask_a_ex2.any():\n",
    "                loss2_a = criterion(model.forward_exit2(inputs_a[mask_a_ex2]), labels_a[mask_a_ex2])\n",
    "            if mask_b_ex2.any():\n",
    "                loss2_b = criterion(model.forward_exit2(inputs_b[mask_b_ex2]), labels_b[mask_b_ex2])\n",
    "            if mask_c_ex2.any():\n",
    "                loss2_c = criterion(model.forward_exit2(inputs_c[mask_c_ex2]), labels_c[mask_c_ex2])\n",
    "\n",
    "            # loss2_a = criterion(model.forward_exit2(inputs_a), labels_a)\n",
    "            # loss2_b = criterion(model.forward_exit2(inputs_b), labels_b)\n",
    "            # loss2_c = criterion(model.forward_exit2(inputs_c), labels_c)\n",
    "\n",
    "            loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "\n",
    "            l_joint = loss_ex1_avg + loss_ex2_avg\n",
    "                        \n",
    "            total_loss = l_joint + (soft_count_exit2 / total_samples)\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_metrics['loss1_a'] += loss1_a.item()\n",
    "            running_metrics['loss1_b'] += loss1_b.item()\n",
    "            running_metrics['loss1_c'] += loss1_c.item()\n",
    "            running_metrics['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "            \n",
    "            running_metrics['loss2_a'] += loss2_a.item()\n",
    "            running_metrics['loss2_b'] += loss2_b.item()\n",
    "            running_metrics['loss2_c'] += loss2_c.item()\n",
    "            running_metrics['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "            \n",
    "            running_metrics['l_joint'] += l_joint.item()\n",
    "            running_metrics['total_loss'] += total_loss.item()\n",
    "            \n",
    "            total_steps += 1\n",
    "\n",
    "        for k in metrics:\n",
    "            history['train'][k].append(running_metrics[k] / total_steps)\n",
    "        \n",
    "        epoch_train_loss = history['train']['total_loss'][-1]\n",
    "\n",
    "        model.eval()\n",
    "        running_metrics_val = {k: 0.0 for k in metrics}\n",
    "        total_steps_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current_threshold = model.get_threshold()\n",
    "            curr_thresh_val = current_threshold.item()\n",
    "\n",
    "            for _ in range(max_val_batches):\n",
    "                try:\n",
    "                    batches = [next(it) for it in val_loader_iterators]\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                (inputs_a, labels_a) = batches[0]\n",
    "                (inputs_b, labels_b) = batches[1]\n",
    "                (inputs_c, labels_c) = batches[2]\n",
    "                    \n",
    "                inputs_a, labels_a = inputs_a.to(device), labels_a.to(device)\n",
    "                inputs_b, labels_b = inputs_b.to(device), labels_b.to(device)\n",
    "                inputs_c, labels_c = inputs_c.to(device), labels_c.to(device)\n",
    "                total_samples = inputs_a.size(0) + inputs_b.size(0) + inputs_c.size(0)\n",
    "    \n",
    "                out1_a = model.forward_exit1(inputs_a)\n",
    "                out1_b = model.forward_exit1(inputs_b)\n",
    "                out1_c = model.forward_exit1(inputs_c)\n",
    "    \n",
    "                curr_thresh_val = current_threshold.item()\n",
    "\n",
    "                probs_a = F.softmax(out1_a, dim=1)\n",
    "                conf_a, _ = torch.max(probs_a, dim=1)\n",
    "    \n",
    "                probs_b = F.softmax(out1_b, dim=1)\n",
    "                conf_b, _ = torch.max(probs_b, dim=1)\n",
    "    \n",
    "                probs_c = F.softmax(out1_c, dim=1)\n",
    "                conf_c, _ = torch.max(probs_c, dim=1)\n",
    "    \n",
    "                mask_a_ex1 = conf_a > curr_thresh_val\n",
    "                mask_b_ex1 = conf_b > curr_thresh_val\n",
    "                mask_c_ex1 = conf_c > curr_thresh_val\n",
    "    \n",
    "                mask_a_ex2 = conf_a < curr_thresh_val\n",
    "                mask_b_ex2 = conf_b < curr_thresh_val\n",
    "                mask_c_ex2 = conf_c < curr_thresh_val\n",
    "    \n",
    "                loss1_a = torch.tensor(0.0, device=device)\n",
    "                loss1_b = torch.tensor(0.0, device=device)\n",
    "                loss1_c = torch.tensor(0.0, device=device)\n",
    "    \n",
    "                if mask_a_ex1.any():\n",
    "                    loss1_a = criterion(out1_a[mask_a_ex1], labels_a[mask_a_ex1])\n",
    "                \n",
    "                if mask_b_ex1.any():\n",
    "                    loss1_b = criterion(out1_b[mask_b_ex1], labels_b[mask_b_ex1])\n",
    "    \n",
    "                if mask_c_ex1.any():\n",
    "                    loss1_c = criterion(out1_c[mask_c_ex1], labels_c[mask_c_ex1])\n",
    "\n",
    "                # loss1_a = criterion(out1_a, labels_a)\n",
    "                # loss1_b = criterion(out1_b, labels_b)\n",
    "                # loss1_c = criterion(out1_c, labels_c)\n",
    "    \n",
    "                loss_ex1_avg = (loss1_a + loss1_b + loss1_c) / 3\n",
    "                \n",
    "                real_count_exit2 = mask_a_ex2.sum() + mask_b_ex2.sum() + mask_c_ex2.sum()\n",
    "    \n",
    "                loss2_a = torch.tensor(0.0, device=device)\n",
    "                loss2_b = torch.tensor(0.0, device=device)\n",
    "                loss2_c = torch.tensor(0.0, device=device)\n",
    "    \n",
    "                if mask_a_ex2.any(): loss2_a = criterion(model.forward_exit2(inputs_a[mask_a_ex2]), labels_a[mask_a_ex2])\n",
    "                if mask_b_ex2.any(): loss2_b = criterion(model.forward_exit2(inputs_b[mask_b_ex2]), labels_b[mask_b_ex2])\n",
    "                if mask_c_ex2.any(): loss2_c = criterion(model.forward_exit2(inputs_c[mask_c_ex2]), labels_c[mask_c_ex2])\n",
    "    \n",
    "                # loss2_a = criterion(model.forward_exit2(inputs_a), labels_a)\n",
    "                # loss2_b = criterion(model.forward_exit2(inputs_b), labels_b)\n",
    "                # loss2_c = criterion(model.forward_exit2(inputs_c), labels_c)\n",
    "    \n",
    "                loss_ex2_avg = (loss2_a + loss2_b + loss2_c) / 3\n",
    "    \n",
    "                l_joint = loss_ex1_avg + loss_ex2_avg\n",
    "                                                    \n",
    "                total_loss = l_joint + (real_count_exit2 / total_samples)\n",
    "                \n",
    "                running_metrics_val['loss1_a'] += loss1_a.item()\n",
    "                running_metrics_val['loss1_b'] += loss1_b.item()\n",
    "                running_metrics_val['loss1_c'] += loss1_c.item()\n",
    "                running_metrics_val['loss_ex1_avg'] += loss_ex1_avg.item()\n",
    "                \n",
    "                running_metrics_val['loss2_a'] += loss2_a.item()\n",
    "                running_metrics_val['loss2_b'] += loss2_b.item()\n",
    "                running_metrics_val['loss2_c'] += loss2_c.item()\n",
    "                running_metrics_val['loss_ex2_avg'] += loss_ex2_avg.item()\n",
    "                \n",
    "                running_metrics_val['l_joint'] += l_joint.item()\n",
    "                running_metrics_val['total_loss'] += total_loss.item()\n",
    "                \n",
    "                total_steps_val += 1\n",
    "\n",
    "        for k in metrics:\n",
    "            history['val'][k].append(running_metrics_val[k] / total_steps_val)\n",
    "\n",
    "        epoch_val_loss = history['val']['total_loss'][-1]\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Train Total: {epoch_train_loss:.4f} | Val Total: {epoch_val_loss:.4f} | Alpha: {curr_thresh_val}')\n",
    "        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                if best_model_state: model.load_state_dict(best_model_state)\n",
    "                break\n",
    "                \n",
    "        scheduler.step(epoch_val_loss) \n",
    "                \n",
    "    epochs_range = range(1, len(history['train']['total_loss']) + 1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Exit 1\")\n",
    "    ax.plot(epochs_range, history['train']['loss1_a'], label='Tr A', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_b'], label='Tr B', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss1_c'], label='Tr C', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex1_avg'], label='Tr Avg', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['loss1_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss1_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex1_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Exit 2\")\n",
    "    ax.plot(epochs_range, history['train']['loss2_a'], label='Tr A', color='blue', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_b'], label='Tr B', color='green', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss2_c'], label='Tr C', color='red', alpha=0.6)\n",
    "    ax.plot(epochs_range, history['train']['loss_ex2_avg'], label='Tr Avg', color='black', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['loss2_a'], label='Val A', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_b'], label='Val B', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss2_c'], label='Val C', color='black', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['loss_ex2_avg'], label='Val Avg', color='black', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Global Optimization\")\n",
    "    ax.plot(epochs_range, history['train']['l_joint'], label='Tr Joint (Ex1 + Ex2)', color='purple')\n",
    "    ax.plot(epochs_range, history['train']['total_loss'], label='Tr Total (Joint + (NEx2 / N))', color='orange', linewidth=2)\n",
    "    \n",
    "    ax.plot(epochs_range, history['val']['l_joint'], label='Val Joint', color='purple', linestyle='--')\n",
    "    ax.plot(epochs_range, history['val']['total_loss'], label='Val Total', color='orange', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return curr_thresh_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9acd9be6862e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:23:19.574727Z",
     "start_time": "2025-09-16T14:23:19.563129Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, confidence_threshold, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_samples = len(loader.dataset)\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    exited_early_count = 0\n",
    "    total_inference_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples, labels = samples.to(device), labels.to(device)\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            branch_output = model.forward_exit1(samples)\n",
    "            \n",
    "            branch_prob = F.softmax(branch_output, dim=1)\n",
    "            trusts, branch_preds = torch.max(branch_prob, 1)\n",
    "\n",
    "            batch_predictions = torch.zeros_like(labels)\n",
    "            \n",
    "            early_exit_mask = trusts > confidence_threshold\n",
    "            \n",
    "            if early_exit_mask.any():\n",
    "                batch_predictions[early_exit_mask] = branch_preds[early_exit_mask]\n",
    "                exited_early_count += early_exit_mask.sum().item()\n",
    "\n",
    "            main_branch_mask = ~early_exit_mask\n",
    "            if main_branch_mask.any():\n",
    "                \n",
    "                samples_to_main = samples[main_branch_mask]\n",
    "                \n",
    "                main_output = model.forward_exit2(samples_to_main)\n",
    "                \n",
    "                main_prob = F.softmax(main_output, dim=1)\n",
    "                _, main_preds = torch.max(main_prob, 1)\n",
    "                \n",
    "                batch_predictions[main_branch_mask] = main_preds\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            total_inference_time += (end_time - start_time)\n",
    "\n",
    "            all_predictions.append(batch_predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    final_predictions = torch.cat(all_predictions)\n",
    "    y_data = torch.cat(all_labels)\n",
    "\n",
    "    correct = (final_predictions == y_data).sum().item()\n",
    "    accuracy = 100 * correct / total_samples\n",
    "    exit_rate = 100 * exited_early_count / total_samples\n",
    "    avg_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    cm = confusion_matrix(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Normal', 'Ataque'],\n",
    "                yticklabels=['Normal', 'Ataque'])\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão (Limiar de Confiança = {confidence_threshold})')\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    f1 = f1_score(y_data.numpy(), final_predictions.numpy())\n",
    "    \n",
    "    tpr = recall_score(y_data.numpy(), final_predictions.numpy())\n",
    "\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\\n\")\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positive Rate (TPR) / Recall: {tpr:.4f}\")\n",
    "    print(f\"True Negative Rate (TNR) / Specificity: {tnr:.4f}\")\n",
    "\n",
    "    return {\n",
    "            'accuracy': accuracy,\n",
    "            'exit_rate': exit_rate,\n",
    "            'avg_inference_time_ms': avg_time_ms,\n",
    "            'exited_early_count': exited_early_count,\n",
    "            'total_samples': total_samples,\n",
    "            'f1': f1\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbf4b5-dcc6-4bd5-96e5-b96988ebd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'teste_ljoint6'\n",
    "modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef852c-867f-440f-bb30-d0adec7af30a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "limiar = train_model(\n",
    "    model, \n",
    "    train_loaders, \n",
    "    val_loaders, \n",
    "    epochs, \n",
    "    lr=0.01,\n",
    "    device=device\n",
    ")\n",
    "torch.save(model.state_dict(), f'models/{modelname}.pth')\n",
    "print(f\"\\nModelo treinado e salvo em 'models/{modelname}.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248ef82-7a24-4504-8cbc-ae2e4e3422fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T14:28:08.281274Z",
     "start_time": "2025-09-16T14:23:19.576754Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'models/{modelname}.pth'))\n",
    "print(f\"Modelo 'models/{modelname}.pth' carregado\\n\")\n",
    "\n",
    "print(f\"Base: UNSW\")\n",
    "results = evaluate_model(model, test_loaders[0], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: BOT\")\n",
    "results = evaluate_model(model, test_loaders[1], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(f\"\\nBase: CIC\")\n",
    "results = evaluate_model(model, test_loaders[2], limiar, device=device)\n",
    "print(\"-\" * 20)\n",
    "print(f\"  Accuracy: {results['accuracy']:.4f}%\")\n",
    "print(f\"  Avg. Inference Time: {results['avg_inference_time_ms']:.4f} ms\")\n",
    "print(f\"  Early Exit Rate: {results['exit_rate']:.4f}% ({results['exited_early_count']}/{results['total_samples']})\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b99544-909f-49c5-a212-93324562d197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361685d-5c70-46b6-945c-4532ea630054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
